{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d199a342",
   "metadata": {
    "id": "d199a342"
   },
   "source": [
    "# Data preprocessing and tree-based models\n",
    "## Introduction\n",
    "This practical will address the following topics:\n",
    "1. Typical **data preprocessing** steps (handling missing values, normalization, dealing with categorical variables).\n",
    "2. A **vanilla implementation** of the **K-Nearest Neighbors (KNN)** algorithm for regression.\n",
    "3. Regression with tree-based models: **regression trees** and **Random forests**\n",
    "4. **Feature importances** in tree-based models.\n",
    "5. **Scikit-learn pipelines** to combine preprocessing and modeling into a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d01de",
   "metadata": {
    "id": "4d7d01de"
   },
   "source": [
    "### Data preprocessing overview\n",
    "Data preprocessing is the foundation of any successful machine learning project. Many Machine Learning models see their performances degraded when trained on poorly preprocessed data. Key steps include:\n",
    "\n",
    "- **Handling Missing Values**: Missing data can bias results if not addressed. This problem affects many models—such as linear models, support vector machines (SVMs), and neural networks—but it is particularly critical for distance-based algorithms like KNN where missing feature values can disrupt the calculation of distances, leading to inaccurate outcomes. Common strategies for handling missing values include:\n",
    "  \n",
    "  - *Dropping* rows/columns with missing data.\n",
    "  - *Imputing* missing data using simple statistics (mean, median) or more sophisticated methods like **`KNNImputer`**.<br><br>\n",
    "- **Normalization / Scaling**: Many algorithms assume features are on similar scales. Without proper scaling, features with larger numerical ranges may dominate those with smaller ranges, potentially biasing the model's results. Common scaling strategies include:\n",
    "  - *Standardization*: Transform features to have zero mean and unit variance.\n",
    "  - *Min-Max Scaling*: Scale features to a fixed $[0,1]$ range.<br><br>\n",
    "- **Dealing with Categorical Variables**: Categorical variables must be converted to a numeric representation that preserves meaningful differences. Common strategies for dealing with categorical variables include:\n",
    "  - *Label Encoding*: Assign an integer to each category (useful if there is an **ordinal** relationship).\n",
    "  - *One-Hot Encoding (Dummy Encoding)*: Create a new binary column for each category (common for **nominal** features). This approach can cause a significant increase in input dimensionality (see the curse of dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ffe73",
   "metadata": {
    "id": "636ffe73"
   },
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "The *$K$-Nearest Neighbors* (KNN) algorithm is a **local modeling** approach that infers its prediction from the data points closest (in some metric) to a query. Suppose we have a training set of inputs $\\{x_i\\}$ with associated labels or targets $\\{y_i\\}$. For a new query point $q$:\n",
    "\n",
    "1. **Distance computation**: Compute the distance (e.g., Euclidean) between $q$ and each training example $x_i$.\n",
    "2. **Ranking**: Sort or rank the training points by increasing distance to $q$.\n",
    "3. **Neighborhood selection**: Identify the subset of the $K$ nearest neighbors $\\{x_{[1]}, \\dots, x_{[K]}\\}$ where $y_{[i]}$ denotes the label of neighbor $x_{[i]}$.\n",
    "4. **Regression or classification**: KNN can be viewed as a local estimator of the conditional expectation $\\mathbb{E}[y|x=q]$.<br>For **regression** tasks, the simplest approach is to average the targets of the $K$ nearest neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y}(q) = \\frac{1}{K}\\sum_{i=1}^{K} y_{[i]}.\n",
    "$$\n",
    "\n",
    "Alternatively, a local linear model may be used:\n",
    "\n",
    "$$\n",
    "\\hat{y}(q) = \\hat{a}^T q + \\hat{b},\n",
    "$$\n",
    "\n",
    "where $\\hat{a}$ and $\\hat{b}$ are estimated (locally) by least-squares on the neighbors.\n",
    "<br><br>For **classification**, one can estimate the conditional probability of belonging to a specific class (e.g., class \"1\") by the proportion of neighbors labeled \"1\":\n",
    "\n",
    "$$\n",
    "\\hat{p}_1(q) = \\frac{1}{K}\\sum_{i=1}^{K} y_{[i]}.\n",
    "$$\n",
    "\n",
    "A threshold (e.g., 0.5) can then be used to decide the class label.\n",
    "\n",
    "A key hyperparameter here is $K$. A *smaller* $K$ reduces bias but increases variance, while a *larger* $K$ produces a smoother model at the risk of higher bias. KNN’s performance also heavily depends on the distance metric, especially when features have different scales or include categorical variables (which may require specialized encodings or distance definitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49540275",
   "metadata": {
    "id": "49540275"
   },
   "source": [
    "### Regression trees\n",
    "\n",
    "Regression trees rely on a tree-based structure of *internal nodes* (where decisions are made) and *terminal nodes* (leaves), which partition the input space into **mutually exclusive** regions, each associated with a simple (often constant) local model. Its construction begins with a **tree growing** phase: starting from the root node that contains all data, we recursively choose a split that best reduces the empirical error. Specifically, for a node $t$ containing $N(t)$ samples, we define\n",
    "\n",
    "$$\n",
    "R_{\\mathrm{emp}}(t) \\;=\\; \\min_{\\alpha_t} \\sum_{i=1}^{N(t)} L\\bigl(y_i,\\;h_t(x_i,\\;\\alpha_t)\\bigr),\n",
    "$$\n",
    "\n",
    "where $L$ is typically the squared error $(y - \\hat{y})^2$ for regression. Given a potential split $s$ dividing $t$ into children $t_l$ and $t_r$, we consider the decrease in empirical error:\n",
    "\n",
    "$$\n",
    "\\Delta E(s,\\;t) \\;=\\; R_{\\mathrm{emp}}(t)\\;-\\;\\bigl(R_{\\mathrm{emp}}(t_l)\\;+\\;R_{\\mathrm{emp}}(t_r)\\bigr).\n",
    "$$\n",
    "\n",
    "We choose the split $s^*$ maximizing $\\Delta E$, partition the dataset accordingly, and repeat the procedure recursively until no further improvement is found or a stopping criterion is met. This exhaustive splitting often yields a **very large** tree that overfits the data.\n",
    "\n",
    "To address overfitting, a **cost-complexity pruning** procedure is commonly used. Introducing a complexity parameter $\\lambda \\ge 0$ that penalizes the number of leaf nodes, we define the cost-complexity measure:\n",
    "\n",
    "$$\n",
    "R_{\\lambda}(T) = R_{\\mathrm{emp}}(T) + \\lambda \\, |T|,\n",
    "$$\n",
    "\n",
    "where $|T|$ is the number of terminal nodes (leaves) in tree $T$.\n",
    "\n",
    "- By gradually increasing $\\lambda$, we obtain a *sequence* of subtrees:\n",
    "\n",
    "$$\n",
    "T_{\\max} \\supset T_{L-1} \\supset \\dots \\supset T_2 \\supset T_1.\n",
    "$$\n",
    "\n",
    "Each has fewer leaves. We consider all admissible subtrees $T_t \\subset T$ of the large tree and compute the smallest\n",
    "\n",
    "$$\n",
    "\\lambda_t = \\frac{R_{\\mathrm{emp}}(T) - R_{\\mathrm{emp}}(T_t)}{|T| - |T_t|}\n",
    "$$\n",
    "\n",
    "that yields a lower cost. We select the subtree that **minimizes** $R_{\\lambda}(T)$ (the best balances between empirical error and model complexity).\n",
    "- The final tree structure is typically chosen via cross-validation or held-out validation to find the best $\\lambda$.\n",
    "\n",
    "Below is a short snippet demonstrating how to initialize a **DecisionTreeRegressor** in `sklearn` with default parameters (such as `max_depth`, `min_samples_split`, etc.):\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg_example = DecisionTreeRegressor(\n",
    "    criterion='squared_error',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,             \n",
    "    min_samples_leaf=1,              \n",
    "    random_state=42                  \n",
    ")\n",
    "```\n",
    "This model is then grown with the standard CART algorithm described above.\n",
    "\n",
    "For a more visual representation of tree-based models, an animated version of decision trees (classification) can be found [here](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21a9e4",
   "metadata": {
    "id": "3b21a9e4"
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "A **Random Forest** (RF) is an ensemble learning technique designed to reduce the variance of decision trees by combining two main ideas:\n",
    "\n",
    "1. **Bootstrap Sampling** (Bagging): We generate $B$ *bootstrap* datasets, each by sampling (with replacement) from the original training data.\n",
    "2. **Random Feature Selection** (Feature Bagging): At each split, a *random subset* of $n' < n$ features is chosen, and the best split is found *only among those $n'$ features*.\n",
    "\n",
    "Hence, each tree $h_b(\\mathbf{x}, \\alpha_b)$ is built from a *different* resampled dataset **and** uses only a random subset of features at each split. As a result, the trees are more **decorrelated** and often are *not pruned* heavily.\n",
    "\n",
    "Formally, for a regression task:\n",
    "\n",
    "$$\n",
    "  h_{\\mathrm{rf}}(\\mathbf{x}) = \\frac{1}{B} \\sum_{b=1}^{B} h_b(\\mathbf{x}, \\alpha_b),\n",
    "$$\n",
    "\n",
    "i.e., the **average** of all tree predictions. (In classification, we take the majority vote.)\n",
    "\n",
    "#### Why does this reduce variance?\n",
    "\n",
    "Suppose each tree $h_b$ has variance $\\sigma^2$ and they have pairwise correlation $\\rho$. The variance of the RF predictor $h_{\\mathrm{rf}}$ can be approximated as:\n",
    "\n",
    "$$\n",
    "  \\mathrm{Var}[h_{\\mathrm{rf}}] \\approx \\frac{(1 - \\rho)\\,\\sigma^2}{B} + \\rho\\,\\sigma^2,\n",
    "$$\n",
    "\n",
    "which shows that **increasing** $B$ (the number of trees) reduces the first term, while **lowering** the correlation $\\rho$ among trees reduces overall variance. Random feature selection helps reduce $\\rho$, because each tree sees a different subset of features.\n",
    "\n",
    "#### Main Hyperparameters\n",
    "\n",
    "1. $B$: the number of trees in the forest.  \n",
    "2. $n'$: the number of randomly selected features at each split (often $\\sqrt{n}$ by default in classification).  \n",
    "3. **Tree parameters**: e.g., maximum depth, minimum samples per leaf, etc.\n",
    "\n",
    "In practice, random forests typically provide strong performance out of the box and are less sensitive to hyperparameter tuning compared to single trees or more complex models. They also allow computing a useful estimate of **feature importance** by measuring, for each variable, how much it contributes to the cost function improvement across all splits in all trees.\n",
    "\n",
    "Below is a short snippet demonstrating how to **initialize a RandomForestRegressor** in `sklearn` with default values:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg_example = RandomForestRegressor(\n",
    "    n_estimators=100,    # number of trees\n",
    "    criterion='squared_error',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='auto',\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "All of these parameters correspond to the concepts introduced in the sections above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a6cb55",
   "metadata": {
    "id": "d9a6cb55"
   },
   "source": [
    "### Feature importances\n",
    "\n",
    "Most scikit-learn models allows you to estimate how each feature contribues to the final prediction. For example, for tree-based models, both **DecisionTreeRegressor** and **RandomForestRegressor** in scikit-learn store a `feature_importances_` attribute after fitting. This gives an estimate of how much each feature contributed to reducing the split criterion (e.g., MSE). Here’s a quick example using a fitted model:\n",
    "```python\n",
    "# Suppose we have a fitted random forest model: forest_reg_example.fit(X, y)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_names = [\"Feature1\", \"Feature2\", \"Feature3\"]  # adapt to your data\n",
    "importances = forest_reg_example.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "The resulting bar plot helps visualize which features were most important for the splits (on average) across the forest. A single decision tree also offers a similar `feature_importances_` vector but typically is less robust than the ensemble measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1fe59",
   "metadata": {
    "id": "d4b1fe59"
   },
   "source": [
    "### Scikit-learn Pipelines\n",
    "\n",
    "Pipelines in scikit-learn let us combine **data preprocessing** (e.g., imputation, scaling) and **model training** (e.g., a random forest) into a single workflow object. Below is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e09f3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56e09f3e",
    "outputId": "8a24bea6-1d5f-45c9-cb13-6b7425f02750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration of how a pipeline is defined. No fitting done here.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example pipeline: impute missing values with mean, then scale, then fit a linear model\n",
    "example_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "print(\"Demonstration of how a pipeline is defined. No fitting done here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1788a2d9",
   "metadata": {
    "id": "1788a2d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class AddConstantTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer that adds a constant value to all features.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant=0.0):\n",
    "        self.constant = constant\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X + self.constant\n",
    "\n",
    "custom_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"custom_add_constant\", AddConstantTransformer(constant=1.0)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", RandomForestRegressor(n_estimators=10, random_state=42))\n",
    "])\n",
    "\n",
    "# custom_pipeline.fit(X_train, y_train)\n",
    "# y_pred = custom_pipeline.predict(X_test)\n",
    "# print(\"MSE with custom transformer in pipeline:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ndcY-UPPULGq",
   "metadata": {
    "id": "ndcY-UPPULGq"
   },
   "source": [
    "### Important precision about the evaluation metric used in this practical:\n",
    "In this practical, we will use the *Mean Squared Error* (MSE) as a convenient shorthand for the estimated *Mean Integrated Squared Error* $(\\widehat{\\mathrm{MISE}})$. Indeed, in practice, we rarely know the true MISE and instead rely on estimates obtained via techniques like $K$-fold cross-validation or leave-one-out (LOO) cross-validation. So keep in mind that when we refer to the MSE in these contexts, what we are really computing is an estimation of $\\widehat{\\mathrm{MISE}}$ from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438ab8f",
   "metadata": {
    "id": "5438ab8f"
   },
   "source": [
    "## Exercise 1: Data Loading, Exploration, and Preprocessing\n",
    "1. Load the well-known **California Housing** regression dataset using the following function:\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "```\n",
    "2. Explore the dataset (distribution, summary statistics).\n",
    "3. Check for missing values. If the dataset does not contain missing values, artificially introduce some for the purpose of this exercice. Then, handle missing values by:\n",
    "   - Dropping them (simple approach).\n",
    "   - Imputing them (mean or median).\n",
    "   - Using a more sophisticated method (e.g., `KNNImputer`).\n",
    "4. Normalize the features (using e.g., `StandardScaler`). Remember that, as all processing steps, **scaling is fit on the training set** (not on the entire dataset) to avoid biasing the test set!\n",
    "5. Preserve original vs. processed versions of the data for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adDENNLOUiSP",
   "metadata": {
    "id": "adDENNLOUiSP"
   },
   "source": [
    "### Dataset and exploration\n",
    "For this exercise, we will use the **California Housing** dataset from `sklearn.datasets.fetch_california_housing`. It contains features related to California housing data, and the target is the average house value.\n",
    "\n",
    "Alternatively, if you would like test your solution on other datasets, scikit-learn provides other built-in datasets, such as:\n",
    "- **Diabetes** (a small dataset with 10 features for a regression task on disease progression).\n",
    "- **Linnerud** (exercise/physiological data, a small multi-output regression problem).\n",
    "If you want a dataset with even more features or complexity, you could consider **Ames Housing** or **Kaggle House Price**, both of which have many categorical and numeric features but may require additional steps to download or preprocess. You can also explore the **UCI Machine Learning Repository** for a wide range of tabular datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31f1ecd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "b31f1ecd",
    "outputId": "24c47f04-1c53-4acf-ab50-3fb5b0ddf900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix: (20640, 8)\n",
      "Shape of the target vector: (20640,)\n",
      "\n",
      "--- Basic Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   MedInc      20640 non-null  float64\n",
      " 1   HouseAge    20640 non-null  float64\n",
      " 2   AveRooms    20640 non-null  float64\n",
      " 3   AveBedrms   20640 non-null  float64\n",
      " 4   Population  20640 non-null  float64\n",
      " 5   AveOccup    20640 non-null  float64\n",
      " 6   Latitude    20640 non-null  float64\n",
      " 7   Longitude   20640 non-null  float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 1.3 MB\n",
      "\n",
      "--- Data Statistics (Original) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>-119.569704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.899822</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.563400</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.534800</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.743250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
       "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
       "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
       "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
       "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
       "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude  \n",
       "count  20640.000000  20640.000000  20640.000000  \n",
       "mean       3.070655     35.631861   -119.569704  \n",
       "std       10.386050      2.135952      2.003532  \n",
       "min        0.692308     32.540000   -124.350000  \n",
       "25%        2.429741     33.930000   -121.800000  \n",
       "50%        2.818116     34.260000   -118.490000  \n",
       "75%        3.282261     37.710000   -118.010000  \n",
       "max     1243.333333     41.950000   -114.310000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data exploration done. Now let's handle missing values.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "cal_housing = fetch_california_housing()\n",
    "X_original = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "y_original = pd.Series(cal_housing.target, name='Target')\n",
    "\n",
    "print(\"Shape of the feature matrix:\", X_original.shape)\n",
    "print(\"Shape of the target vector:\", y_original.shape)\n",
    "\n",
    "print(\"\\n--- Basic Info ---\")\n",
    "X_original.info()\n",
    "\n",
    "print(\"\\n--- Data Statistics (Original) ---\")\n",
    "display(X_original.describe())\n",
    "\n",
    "print(\"\\nData exploration done. Now let's handle missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38be104",
   "metadata": {
    "id": "c38be104"
   },
   "source": [
    "### Handling Missing Values\n",
    "Although the California Housing dataset typically does not contain missing values, let’s **artificially introduce some** missing values for demonstration. We will then show three approaches:\n",
    "\n",
    "1. **Dropping** rows with missing values.\n",
    "2. **Imputing** with mean or median.\n",
    "3. **KNNImputer** or another advanced method.\n",
    "\n",
    "#### `KNNImputer` Explanation\n",
    "`KNNImputer` uses a K-Nearest Neighbors approach to fill missing entries: for each row that has missing features, it finds the nearest *k* rows (in feature space) that do not have missing values in those columns, and uses their values (e.g., average) to impute the missing ones. This can often be more accurate than a simple mean or median if the data has local structure—rows that are similar in some features are likely also similar in the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9f5ad1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b9f5ad1",
    "outputId": "9fe274eb-4488-4660-9a50-e9e1bfa4a13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Number of missing values (artificial) ---\n",
      "MedInc        212\n",
      "HouseAge      190\n",
      "AveRooms      200\n",
      "AveBedrms     203\n",
      "Population    230\n",
      "AveOccup      167\n",
      "Latitude      193\n",
      "Longitude     221\n",
      "dtype: int64\n",
      "\n",
      "--- After Dropping Missing Values ---\n",
      "Training set size: (15246, 8)\n",
      "Test set size: (3833, 8)\n",
      "\n",
      "Imputation done. Now we have multiple versions of the dataset.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Artificially introduce missing values\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.rand(*X_original.shape) < 0.01  # ~1% missing\n",
    "X_missing = X_original.copy()\n",
    "X_missing[missing_mask] = np.nan\n",
    "\n",
    "print(\"\\n--- Number of missing values (artificial) ---\")\n",
    "print(X_missing.isnull().sum())\n",
    "\n",
    "# Let's first do a simple train-test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_missing, y_original, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2.3.1 Dropping rows with missing values\n",
    "X_train_drop = X_train_full.dropna(axis=0)\n",
    "y_train_drop = y_train_full.loc[X_train_drop.index]\n",
    "X_test_drop = X_test_full.dropna(axis=0)\n",
    "y_test_drop = y_test_full.loc[X_test_drop.index]\n",
    "\n",
    "print(\"\\n--- After Dropping Missing Values ---\")\n",
    "print(\"Training set size:\", X_train_drop.shape)\n",
    "print(\"Test set size:\", X_test_drop.shape)\n",
    "\n",
    "# 2.3.2 Imputing with mean or median\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "X_train_mean = mean_imputer.fit_transform(X_train_full)\n",
    "X_test_mean = mean_imputer.transform(X_test_full)\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "X_train_median = median_imputer.fit_transform(X_train_full)\n",
    "X_test_median = median_imputer.transform(X_test_full)\n",
    "\n",
    "# 2.3.3 A more sophisticated method: KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "X_train_knn = knn_imputer.fit_transform(X_train_full)\n",
    "X_test_knn = knn_imputer.transform(X_test_full)\n",
    "\n",
    "print(\"\\nImputation done. Now we have multiple versions of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8b056",
   "metadata": {
    "id": "e3f8b056"
   },
   "source": [
    "### Dealing with Categorical Values\n",
    "\n",
    "In this dataset, we have only numerical features. However, real-world datasets often include **categorical** variables. Two popular encoding strategies are:\n",
    "\n",
    "1. **Label Encoding**: Each category is assigned an integer.\n",
    "2. **One-Hot Encoding (Dummy Encoding)**: Create a new binary column for each category.\n",
    "\n",
    "Below is a small example using a synthetic dataframe containing a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f01095",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "38f01095",
    "outputId": "29d20ed8-232d-476d-e709-24c4b5206a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Label Encoding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Size_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Small</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Size  Weight  Size_label\n",
       "0   Small     1.0           2\n",
       "1  Medium     2.3           1\n",
       "2   Large     5.5           0\n",
       "3  Medium     2.0           1\n",
       "4   Small     1.1           2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with One-Hot Encoding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Size_label</th>\n",
       "      <th>Size_Large</th>\n",
       "      <th>Size_Medium</th>\n",
       "      <th>Size_Small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weight  Size_label  Size_Large  Size_Medium  Size_Small\n",
       "0     1.0           2       False        False        True\n",
       "1     2.3           1       False         True       False\n",
       "2     5.5           0        True        False       False\n",
       "3     2.0           1       False         True       False\n",
       "4     1.1           2       False        False        True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_df = pd.DataFrame({\n",
    "    \"Size\": [\"Small\", \"Medium\", \"Large\", \"Medium\", \"Small\"],\n",
    "    \"Weight\": [1.0, 2.3, 5.5, 2.0, 1.1]\n",
    "})\n",
    "\n",
    "# 2.4.1 Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "example_df[\"Size_label\"] = label_encoder.fit_transform(example_df[\"Size\"])\n",
    "\n",
    "# 2.4.2 One-Hot Encoding\n",
    "one_hot_df = pd.get_dummies(example_df, columns=[\"Size\"], prefix=\"Size\")\n",
    "\n",
    "print(\"Original DataFrame with Label Encoding:\")\n",
    "display(example_df)\n",
    "print(\"\\nDataFrame with One-Hot Encoding:\")\n",
    "display(one_hot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4303b7-7261-42ab-b6a1-2d1198d957ea",
   "metadata": {},
   "source": [
    "We'll stick to numeric data for the main practical, but this is how you'd handle categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a99a42",
   "metadata": {
    "id": "80a99a42"
   },
   "source": [
    "### Normalization/Scaling\n",
    "\n",
    "Scaling is critical for distance-based methods. We will use **`StandardScaler`**, which transforms each feature to have zero mean and unit variance.\n",
    "\n",
    "> **Remember**: *Always fit the scaler on the **training set**, then apply the same transformation to the test set. This prevents data leakage and preserves the integrity of the test data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0057ab2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0057ab2e",
    "outputId": "9155e951-1e12-40f3-d8d8-3a3e45f6dffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data is now scaled (fit on training set) using mean-imputed values.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We'll choose the mean-imputed dataset for demonstration\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_mean)  # fit on training\n",
    "X_test_scaled = scaler.transform(X_test_mean)       # transform test with the same scaler\n",
    "\n",
    "# Another popular method: Min-Max Scaling (commented out)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# X_train_scaled_minmax = minmax_scaler.fit_transform(X_train_mean)\n",
    "# X_test_scaled_minmax = minmax_scaler.transform(X_test_mean)\n",
    "\n",
    "# Renaming final sets\n",
    "X_train_final = X_train_scaled\n",
    "X_test_final = X_test_scaled\n",
    "y_train_final = y_train_full\n",
    "y_test_final = y_test_full\n",
    "\n",
    "print(\"\\nData is now scaled (fit on training set) using mean-imputed values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054cafe",
   "metadata": {
    "id": "c054cafe"
   },
   "source": [
    "## Exercise 2: Vanilla KNN Implementation\n",
    "1. Split your data into training and test sets (we should have already done that in the previous exercise).\n",
    "2. Implement a vanilla KNN regressor from scratch (no scikit-learn for this part).\n",
    "3. Choose a value of $k$ (e.g., 3, 5, 7).\n",
    "4. Compare predictions with the true values using Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c80f1",
   "metadata": {
    "id": "4d0c80f1"
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e710d9ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cb01dc7d0da740489b7280b92b5b5b9c",
      "fa43245755b54d92a0c4b8623322c6c8",
      "70a0a20b9dc1444aadad81ce47cfa94c",
      "f320747cea5147468d1696bc0018778b",
      "8267a7c4e87e403ab8027ce17d3a75bc",
      "926510fb3a4b44b58070b909f8d48164",
      "481dab20bc9742cd8877006322b7fbdc",
      "e05cf20018804cdfb7512ecb5e079594",
      "3681a1a4cf674ac99ab9c4b913efd586",
      "d8df9af2d3ba410392f1591a7326008f",
      "ecda0e22ab7140b7a89ec0e0968369ae"
     ]
    },
    "id": "e710d9ce",
    "outputId": "a331336b-2e52-4b5d-f4da-37e78b437a09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a169b8bab2459594d8944f121ce448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(predictions)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m]:\n\u001b[0;32m---> 24\u001b[0m   y_pred_knn \u001b[38;5;241m=\u001b[39m \u001b[43mknn_regressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m   mse_knn \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test_final, y_pred_knn)\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNN Regressor (k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_knn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mknn_regressor\u001b[0;34m(X_train, y_train, X_test, k)\u001b[0m\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_point \u001b[38;5;129;01min\u001b[39;00m tqdm(X_test):\n\u001b[0;32m---> 17\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m     k_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances)[:k]\n\u001b[1;32m     19\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_train[k_indices])\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_point \u001b[38;5;129;01min\u001b[39;00m tqdm(X_test):\n\u001b[0;32m---> 17\u001b[0m     distances \u001b[38;5;241m=\u001b[39m [\u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_train \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[1;32m     18\u001b[0m     k_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances)[:k]\n\u001b[1;32m     19\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_train[k_indices])\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36meuclidean_distance\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meuclidean_distance\u001b[39m(x1, x2):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:72\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 72\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_NoValue\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:73\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     72\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m---> 73\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def knn_regressor(X_train, y_train, X_test, k=3):\n",
    "    \"\"\"\n",
    "    A simple KNN regressor (manual implementation).\n",
    "    - X_train: training features (numpy array)\n",
    "    - y_train: training targets (numpy array)\n",
    "    - X_test: test features (numpy array)\n",
    "    - k: number of neighbors to consider\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for test_point in tqdm(X_test):\n",
    "        distances = [euclidean_distance(test_point, x_train) for x_train in X_train]\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        prediction = np.mean(y_train[k_indices])\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "for k in [3, 5, 7]:\n",
    "  y_pred_knn = knn_regressor(X_train_final, y_train_final.values, X_test_final, k=k)\n",
    "  mse_knn = mean_squared_error(y_test_final, y_pred_knn)\n",
    "  print(f\"KNN Regressor (k={k}) MSE: {mse_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9fc02b",
   "metadata": {
    "id": "ad9fc02b"
   },
   "source": [
    "## Exercise 3: Regression Trees and Random Forests\n",
    "1. Train a regression tree on the training set.\n",
    "2. Train a random forest on the training set.\n",
    "3. Evaluate both models using MSE.\n",
    "4. Compare their performance with your KNN model.\n",
    "5. Determine how modifying the `max_depth` (for the regression trees) and `n_estimators` (for the random forests) impacts the performances of the model. Plot these results graphically.\n",
    "6. Plot the features importance of the best model with the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d141d",
   "metadata": {
    "id": "0e4d141d"
   },
   "source": [
    "### Implementation with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc41849",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddc41849",
    "outputId": "b0230fa6-532c-4397-b68a-42632ad9b89f"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 4.2.1 Regression Tree\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train_final, y_train_final)\n",
    "y_pred_tree = tree_reg.predict(X_test_final)\n",
    "mse_tree = mean_squared_error(y_test_final, y_pred_tree)\n",
    "print(f\"Regression Tree MSE (default parameters): {mse_tree:.4f}\")\n",
    "\n",
    "# 4.2.2 Random Forest\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(X_train_final, y_train_final)\n",
    "y_pred_forest = forest_reg.predict(X_test_final)\n",
    "mse_forest = mean_squared_error(y_test_final, y_pred_forest)\n",
    "print(f\"Random Forest MSE (default parameters): {mse_forest:.4f}\")\n",
    "\n",
    "print(\"\\nComparison of MSE:\")\n",
    "print(f\"KNN (k={k}): {mse_knn:.4f}\")\n",
    "print(f\"Decision Tree: {mse_tree:.4f}\")\n",
    "print(f\"Random Forest: {mse_forest:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38172039",
   "metadata": {
    "id": "38172039"
   },
   "source": [
    "### Exploring Key Parameters\n",
    "\n",
    "#### Decision Tree: `max_depth`\n",
    "We can examine how **max_depth** (the maximum number of splits from the root to the leaf) affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed74b08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "6ed74b08",
    "outputId": "404b5965-fec3-43b4-bc12-ea1634cd5b16"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['seaborn-v0_8-muted', 'practicals.mplstyle'])\n",
    "\n",
    "depths = [2, 5, 10, 20, None]\n",
    "mse_values_tree = []\n",
    "\n",
    "for d in depths:\n",
    "    dt = DecisionTreeRegressor(max_depth=d, random_state=42)\n",
    "    dt.fit(X_train_final, y_train_final)\n",
    "    y_pred_dt = dt.predict(X_test_final)\n",
    "    mse_values_tree.append(mean_squared_error(y_test_final, y_pred_dt))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([str(d) for d in depths], mse_values_tree, marker='o')\n",
    "plt.title(\"MSE vs max_depth (Decision Tree)\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de719d",
   "metadata": {
    "id": "c2de719d"
   },
   "source": [
    "#### Random Forest: `n_estimators`\n",
    "Similarly, we can see how **n_estimators** (the number of trees in the forest) impacts the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e778aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "67e778aa",
    "outputId": "001fd45a-4dec-4eba-ba43-f9ea06a7b0bf"
   },
   "outputs": [],
   "source": [
    "estimators_range = [10, 50, 100, 200]\n",
    "mse_values_rf = []\n",
    "\n",
    "for n in estimators_range:\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train_final, y_train_final)\n",
    "    y_pred_rf = rf.predict(X_test_final)\n",
    "    mse_values_rf.append(mean_squared_error(y_test_final, y_pred_rf))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(estimators_range, mse_values_rf, marker='o', color='green')\n",
    "plt.title(\"MSE vs n_estimators (Random Forest)\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e24ec",
   "metadata": {
    "id": "a25e24ec"
   },
   "source": [
    "### Feature importance with the best models\n",
    "Based on the experiments above, let’s define **user-chosen** parameters for the *best* (or at least improved) **Decision Tree** and **Random Forest**, then **compute and display** their feature importances.\n",
    "\n",
    "Feel free to update `max_depth` and `n_estimators` according to the results you obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf58d4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "ccf58d4d",
    "outputId": "3dd3eb54-6eee-4a0c-cbd4-4a6fa7bc5f85"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# User-chosen parameters (edit based on the results of section 4.3)\n",
    "max_depth = 5     # for the Decision Tree\n",
    "n_estimators = 50 # for the Random Forest\n",
    "\n",
    "# Instantiate and fit the best Decision Tree\n",
    "best_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "best_tree.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Instantiate and fit the best Random Forest\n",
    "best_forest = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "best_forest.fit(X_train_final, y_train_final)\n",
    "\n",
    "feature_names = X_original.columns  # Using original column names\n",
    "\n",
    "tree_importances = best_tree.feature_importances_\n",
    "forest_importances = best_forest.feature_importances_\n",
    "\n",
    "# Sort features by importance for each model\n",
    "tree_sorted_idx = np.argsort(tree_importances)[::-1]\n",
    "forest_sorted_idx = np.argsort(forest_importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Decision Tree Feature Importances\")\n",
    "plt.bar(range(len(tree_importances)), tree_importances[tree_sorted_idx], align='center')\n",
    "plt.xticks(range(len(tree_importances)), feature_names[tree_sorted_idx], rotation=45)\n",
    "plt.grid(\"on\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.bar(range(len(forest_importances)), forest_importances[forest_sorted_idx], align='center', color='green')\n",
    "plt.xticks(range(len(forest_importances)), feature_names[forest_sorted_idx], rotation=45)\n",
    "plt.grid(\"on\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1cab8",
   "metadata": {
    "id": "ccf1cab8"
   },
   "source": [
    "## Exercise 4: Building a Pipeline\n",
    "1. **Define** a pipeline that applies the preprocessing steps (e.g., imputation, scaling) and the best-performing model (from Exercise 3).\n",
    "2. **Fit** this pipeline on the training data and evaluate on the test data.\n",
    "3. **Observe** how scikit-learn handles transformations during `fit` and `predict`.\n",
    "\n",
    "### Pipeline Implementation\n",
    "Below, we define a pipeline that:\n",
    "1. Imputes missing values with mean.\n",
    "2. Scales the features.\n",
    "3. Trains a random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a47c8d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a47c8d5",
    "outputId": "be7aa9b0-7bf8-48be-9ef0-4ca2a89614a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (Random Forest) MSE: 0.2672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the entire training set with missing values (X_train_full)\n",
    "pipeline_rf.fit(X_train_full, y_train_full)\n",
    "y_pred_pipe = pipeline_rf.predict(X_test_full)\n",
    "mse_pipeline_rf = mean_squared_error(y_test_full, y_pred_pipe)\n",
    "print(f\"Pipeline (Random Forest) MSE: {mse_pipeline_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YsGHuCK-Mma7",
   "metadata": {
    "id": "YsGHuCK-Mma7"
   },
   "source": [
    "## Additional exercise: bias–variance trade-off in regression with decision trees\n",
    "\n",
    "In this exercise, you will investigate the bias–variance trade-off for decision tree regressors of varying complexity. Specifically, you will perform simulations to estimate how the depth of a decision tree influences its performance.\n",
    "\n",
    "**Your tasks are as follows:**\n",
    "\n",
    "1. Select a nonlinear, moderately complex target function e.g.:\n",
    "   $f(x) = \\sin(4\\pi x) + w$ or $f(x) = \\sin(2\\pi x_1 x_2 x_3) + w$\n",
    "\n",
    "2. Evaluate three decision tree regressors, each with a different complexity level:\n",
    "   - **Shallow tree**: depth = 2\n",
    "   - **Medium-depth tree**: depth = 6\n",
    "   - **Deep tree**: depth = 20\n",
    "\n",
    "3. Generate training datasets composed of $n_{\\text{train}} = 400$ random samples:\n",
    "\n",
    "   \n",
    "   $$\n",
    "   X_{\\text{train}} \\sim U(0,1), \\quad y_{\\text{train}} = f(X_{\\text{train}}) + w\n",
    "   $$\n",
    "\n",
    "   \n",
    "   Here, noise $w$ follows a Gaussian distribution with mean 0 and standard deviation $0.3$:\n",
    "\n",
    "   \n",
    "   $$\n",
    "   w \\sim \\mathcal{N}(0, 0.3^2)\n",
    "   $$\n",
    "\n",
    "5. Repeat the entire training and evaluation process $n_{\\text{runs}} = 100$ times. For each run, compute and store the mean squared error (MSE) using 10-fold cross-validation on the training set.\n",
    "\n",
    "6. Evaluate your models on a test set $X_{\\text{test}}$ evenly spaced in the interval $[0,1]$ (e.g., 400 points).\n",
    "   - For each test point, compute predictions across all 100 simulation runs.\n",
    "   - Using these predictions, compute\n",
    "     - **MSE (CV)**: The average cross-validation mean squared error (which we will call $\\widehat{\\text{MISE}}_{\\text{kfold}}$) obtained across the 100 runs.\n",
    "     - **Bias\\(^2\\)**: Based on how far the *average prediction* per sample is from the sample's true label.\n",
    "     - **Variance**: Based on the variability of predictions per sample around that sample’s mean prediction.\n",
    "     - **MSE (true)**: the approximation $\n",
    "\\text{MSE} \\approx \\text{Bias}^2 + \\text{Variance}.$\n",
    "\n",
    "7. **Discussion**  \n",
    "   Analyze your results to illustrate how model complexity (tree depth) affects the balance between bias and variance:\n",
    "   - Identify which depth leads to underfitting (high bias, low variance).\n",
    "   - Identify which depth leads to overfitting (low bias, high variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pBTLvUAVB0Qa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBTLvUAVB0Qa",
    "outputId": "9942c30c-6ee8-449d-c45e-2581a079e891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Bias^2  Variance  True MSE  CV MSE\n",
      "Tree Depth                                    \n",
      "2           0.1875    0.0996    0.3770  0.3779\n",
      "6           0.0011    0.0316    0.1227  0.1243\n",
      "20          0.0009    0.0886    0.1795  0.1784\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "n_train = 400        # number of training samples per run\n",
    "n_runs = 100        # number of simulation runs for averaging\n",
    "noise_std = 0.3     # standard deviation of noise\n",
    "depths = [2, 6, 20] # tree depths to evaluate\n",
    "K = 10               # number of folds for K-fold cross-validation\n",
    "\n",
    "# Define the target function f(x) = sin(4πx)\n",
    "def f_func(x):\n",
    "    return np.sin(4 * np.pi * x).ravel()\n",
    "\n",
    "# Prepare a grid of test points to evaluate predictions (for bias/variance calculations)\n",
    "X_test = np.linspace(0, 1, 400)[:, None]\n",
    "y_true = f_func(X_test)\n",
    "\n",
    "# Storage for predictions and CV results\n",
    "predictions = {depth: np.zeros((n_runs, X_test.shape[0])) for depth in depths}\n",
    "cv_mse_values = {depth: [] for depth in depths}\n",
    "\n",
    "# Simulation over multiple runs\n",
    "for i in range(n_runs):\n",
    "\n",
    "    # Generate random training data for this run\n",
    "    X_train = np.random.rand(n_train, 1)\n",
    "    y_train = f_func(X_train) + np.random.normal(0, noise_std, size=n_train)\n",
    "\n",
    "    # Train and evaluate each tree depth\n",
    "    for depth in depths:\n",
    "\n",
    "        # Train decision tree regressor\n",
    "        model = DecisionTreeRegressor(max_depth=depth)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store predictions\n",
    "        predictions[depth][i, :] = model.predict(X_test)\n",
    "\n",
    "        # Compute K-fold CV MSE on the training set (negative MSE is returned by cross_val_score)\n",
    "        cv_scores = cross_val_score(\n",
    "            DecisionTreeRegressor(max_depth=depth),\n",
    "            X_train, y_train,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=KFold(n_splits=K, shuffle=True, random_state=1)\n",
    "        )\n",
    "        cv_mse = -cv_scores.mean()\n",
    "        cv_mse_values[depth].append(cv_mse)\n",
    "\n",
    "\n",
    "# Known noise variance\n",
    "noise_var = noise_std**2\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "metrics = {}\n",
    "\n",
    "for depth in depths:\n",
    "    # Mean prediction over all runs at each test point\n",
    "    mean_pred = np.mean(predictions[depth], axis=0)\n",
    "\n",
    "    # Bias^2 (averaged over test points)\n",
    "    bias_sq = np.mean((mean_pred - y_true)**2)\n",
    "\n",
    "    # Variance (averaged over test points)\n",
    "    variance = np.mean(np.var(predictions[depth], axis=0))\n",
    "\n",
    "    # True MSE = Bias^2 + Variance + noise variance\n",
    "    true_mse = bias_sq + variance + noise_var\n",
    "\n",
    "    # Average CV MSE across runs\n",
    "    cv_mse_mean = np.mean(cv_mse_values[depth])\n",
    "\n",
    "    metrics[depth] = {\n",
    "        \"Bias^2\": bias_sq,\n",
    "        \"Variance\": variance,\n",
    "        \"True MSE\": true_mse,\n",
    "        \"CV MSE\": cv_mse_mean\n",
    "    }\n",
    "\n",
    "# Display the metrics in a table for each depth\n",
    "metrics_df = pd.DataFrame(metrics).T  # transpose so that depth is the index\n",
    "metrics_df.index.name = \"Tree Depth\"\n",
    "# Round the values for cleaner display\n",
    "metrics_df = metrics_df.round(4)\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3681a1a4cf674ac99ab9c4b913efd586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "481dab20bc9742cd8877006322b7fbdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70a0a20b9dc1444aadad81ce47cfa94c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e05cf20018804cdfb7512ecb5e079594",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3681a1a4cf674ac99ab9c4b913efd586",
      "value": 200
     }
    },
    "8267a7c4e87e403ab8027ce17d3a75bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "926510fb3a4b44b58070b909f8d48164": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb01dc7d0da740489b7280b92b5b5b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa43245755b54d92a0c4b8623322c6c8",
       "IPY_MODEL_70a0a20b9dc1444aadad81ce47cfa94c",
       "IPY_MODEL_f320747cea5147468d1696bc0018778b"
      ],
      "layout": "IPY_MODEL_8267a7c4e87e403ab8027ce17d3a75bc"
     }
    },
    "d8df9af2d3ba410392f1591a7326008f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e05cf20018804cdfb7512ecb5e079594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecda0e22ab7140b7a89ec0e0968369ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f320747cea5147468d1696bc0018778b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8df9af2d3ba410392f1591a7326008f",
      "placeholder": "​",
      "style": "IPY_MODEL_ecda0e22ab7140b7a89ec0e0968369ae",
      "value": " 200/200 [00:26&lt;00:00,  9.25it/s]"
     }
    },
    "fa43245755b54d92a0c4b8623322c6c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_926510fb3a4b44b58070b909f8d48164",
      "placeholder": "​",
      "style": "IPY_MODEL_481dab20bc9742cd8877006322b7fbdc",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
