{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdf396d",
   "metadata": {},
   "source": [
    "# INFO-F-422 - Statistical Foundations of Machine Learning\n",
    "\n",
    "**Teaching assistants:**  \n",
    "Gian Marco Paldino - gian.marco.paldino@ulb.be  \n",
    "Cédric Simar - cedric.simar@ulb.be  \n",
    "Pascal Tribel - pascal.tribel@ulb.be\n",
    "\n",
    "**Reference**: *\"Statistical Foundations of Machine Learning: the handbook\"*  \n",
    "by **Gianluca Bontempi**, Machine Learning Group, Computer Science Department, ULB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238e73e",
   "metadata": {},
   "source": [
    "# Practical 4: Neural Networks for Regression\n",
    "\n",
    "## 1. Introduction to Neural Networks\n",
    "\n",
    "This part of the introduction summarizes the key theoretical concepts for **Neural Networks for Regression**, building on Chapter 8 of the handbook (pp. 189–199).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac9545",
   "metadata": {},
   "source": [
    "### 1.1. Data Preprocessing for Neural Networks\n",
    "\n",
    "#### 1.1.1 Normalization and Standardization\n",
    "\n",
    "**Motivation**:\n",
    "Gradient-based optimization of neural networks often becomes unstable if different features vary widely in scale. Hence, **Sec. 8.1** of the handbook (and earlier practicals) reiterate that scaling inputs to zero mean and unit variance—or to a small bounded range—is essential. This ensures the gradients have more balanced magnitudes across parameters, which helps the optimizer converge more reliably.\n",
    "\n",
    "**Typical Steps**:\n",
    "1. Compute the empirical mean and standard deviation of each input dimension:\n",
    "   $$\n",
    "   \\mu_j = \\frac{1}{N} \\sum_{i=1}^N x_{ij}, \\quad\n",
    "   \\sigma_j = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (x_{ij}-\\mu_j)^2}.\n",
    "   $$\n",
    "2. Transform (standardise) the data as\n",
    "   $$\n",
    "   x_{ij}^{(std)} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}.\n",
    "   $$\n",
    "3. Use the same $\\mu_j$ and $\\sigma_j$ for the validation and test sets to maintain consistency.\n",
    "\n",
    "#### 1.1.2 Input Data Shaping\n",
    "- **For MLPs**: Usually arrange training data into a matrix of shape $(\\text{batch}, n)$, where $n$ is the number of input features.\n",
    "- **For CNNs**: Reshape data into a 4D tensor of shape $(\\text{batch}, \\text{channels}, \\text{height}, \\text{width})$. Convolutional layers exploit the spatial dimensions for images or other grid-like inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca28ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example in Python-like pseudocode:\n",
    "import torch\n",
    "\n",
    "# Suppose X_train is shape (N, n) for an MLP:\n",
    "N, n = 1000, 10\n",
    "X_train = torch.randn(N, n)\n",
    "X_val = torch.randn(N//5, n)\n",
    "X_test = torch.randn(N//5, n)\n",
    "\n",
    "mean = X_train.mean(dim=0)\n",
    "std  = X_train.std(dim=0)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_val   = (X_val - mean) / std\n",
    "X_test  = (X_test - mean) / std\n",
    "\n",
    "# For CNN input, if needed:\n",
    "H, W = 28, 28  # e.g., image dimension\n",
    "# Reshape to (N, 1, H, W) for single-channel images:\n",
    "X_train_cnn = X_train.view(N, 1, H, W)\n",
    "X_val_cnn   = X_val.view(N//5, 1, H, W)\n",
    "X_test_cnn  = X_test.view(N//5, 1, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41208dac",
   "metadata": {},
   "source": [
    "### 1.2 Feed-Forward Neural Networks (MLPs)\n",
    "\n",
    "#### 1.2.1 Architecture and Notation\n",
    "\n",
    "An MLP (or feed-forward network) with $L$ layers (including hidden and output layers) maps an input vector $\\mathbf{x}\\in\\mathbb{R}^n$ to an output (for regression, typically $\\hat{y}\\in\\mathbb{R}$ or $\\mathbb{R}^m$) through successive affine transformations and nonlinear activations. In the notation of **Eqs. $(8.1.2)$–$(8.1.5)$**:\n",
    "\n",
    "1. **Layer 1**:\n",
    "   $$\n",
    "   A^{(1)} = Z^{(0)} W^{(1)} + b^{(1)},\\quad Z^{(1)} = g^{(1)}(A^{(1)}),\n",
    "  $$\n",
    "   where $Z^{(0)} = \\mathbf{x}$ is the input and $W^{(1)}$ is an $(n \\times h_1)$ weight matrix if the first hidden layer has $h_1$ units.\n",
    "\n",
    "2. **Layer 2** (or subsequent hidden layers):\n",
    "   $$\n",
    "   A^{(2)} = Z^{(1)} W^{(2)} + b^{(2)},\\quad Z^{(2)} = g^{(2)}(A^{(2)}),\n",
    "  $$\n",
    "   continuing similarly up to layer $L-1$.\n",
    "\n",
    "3. **Output Layer** ($l=L$):\n",
    "   For regression with a linear output,\n",
    "   $$\n",
    "   A^{(L)} = Z^{(L-1)} W^{(L)} + b^{(L)},\\quad Z^{(L)} = A^{(L)}.\n",
    "  $$\n",
    "   If the output dimension is 1, $W^{(L)}$ is $(h_{L-1} \\times 1)$. For vector outputs, adjust accordingly.\n",
    "\n",
    "#### 1.2.2 Activation Functions\n",
    "\n",
    "Nonlinearities $g^{(l)}(\\cdot)$ are critical. Common examples in the course:\n",
    "- **Sigmoid**: $\\sigma(z)=1/(1+e^{-z})$, which saturates for large $|z|$.\n",
    "- **Tanh**: $\\tanh(z)\\in(-1,1)$, similar saturation behavior.\n",
    "- **ReLU**: $\\max(0,z)$, which alleviates vanishing gradients on the positive side and is widely used in deep networks.\n",
    "\n",
    "Without a nonlinear $g^{(l)}$, the stacked linear transformations collapse to an effective single linear mapping. Nonlinearity ensures the network can approximate complex functions (Universal Approximation property)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e506f0",
   "metadata": {},
   "source": [
    "### 1.3 Cost function and matrix-based Back-Propagation\n",
    "\n",
    "#### 1.3.1 Sum of Squared Errors (SSE)\n",
    "\n",
    "For regression tasks, the *empirical sum of squared errors*:\n",
    "$$\n",
    "\\mathrm{SSE}_{\\mathrm{emp}}(\\alpha_N) = \\sum_{i=1}^N (y_i - \\hat{y}(x_i))^2,\n",
    "\\]\n",
    "where $\\hat{y}(x_i)$ is the network’s prediction for input $x_i$, and $y_i$ is the true target. The parameter vector $\\alpha_N$ includes $\\{W^{(l)}, b^{(l)}\\}$ for $l=1,\\ldots,L$.\n",
    "\n",
    "#### 1.3.2 Gradient Descent Update\n",
    "\n",
    "Using the notation from **Algorithm 2 (p. 194)**, we iteratively update each parameter in the direction that reduces the SSE cost:\n",
    "$$\n",
    "\\alpha_N(k+1) = \\alpha_N(k) - \\eta\\, \\frac{\\partial\\,\\mathrm{SSE}_{\\mathrm{emp}}}{\\partial\\,\\alpha_N}(k),\n",
    "\\]\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "#### 1.3.3 Back-Propagation (Algorithm 2)\n",
    "\n",
    "Back-propagation is the application of the chain rule to compute $\\nabla \\mathrm{SSE}_{\\mathrm{emp}}$ with respect to each weight matrix $W^{(l)}$ and bias $b^{(l)}$.\n",
    "\n",
    "1. **Output-Layer Delta**:\n",
    "   $$\n",
    "   \\delta^{(L)} = \\frac{\\partial\\,\\mathrm{SSE}_{\\mathrm{emp}}}{\\partial\\,A^{(L)}} \\odot g'^{(L)}(A^{(L)}).\n",
    "  $$\n",
    "   - In linear output regression, $g'^{(L)}(\\cdot) = 1$, and the cost derivative is $\\delta^{(L)} = (\\hat{y}-y).$\n",
    "\n",
    "2. **Backward Recursion**:\n",
    "   $$\n",
    "   \\delta^{(l)} = \\bigl(W^{(l+1)} \\delta^{(l+1)}\\bigr) \\odot g'^{(l)}(A^{(l)}), \\quad l = L-1,\\ldots,1.\n",
    "  $$\n",
    "   This “error” $\\delta^{(l)}$ is used to compute partial derivatives for $W^{(l)}$ and $b^{(l)}$.\n",
    "\n",
    "3. **Gradient w.r.t. Weights**:\n",
    "   $$\n",
    "   \\frac{\\partial\\,\\mathrm{SSE}_{\\mathrm{emp}}}{\\partial\\,W^{(l)}} = (Z^{(l-1)})^\\top \\,\\delta^{(l)},\\quad\n",
    "   \\frac{\\partial\\,\\mathrm{SSE}_{\\mathrm{emp}}}{\\partial\\,b^{(l)}} = \\sum_{i=1}^{N} \\delta^{(l)}_i.\n",
    "  $$\n",
    "   using the matrix-based form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ac4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration in Python (symbolic style)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Simple2LayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)  # ReLU activation\n",
    "        x = self.fc2(x)    # linear output for regression\n",
    "        return x\n",
    "\n",
    "# Instantiate the network and define SSE-like loss\n",
    "model = Simple2LayerMLP(input_dim=10, hidden_dim=15, output_dim=1)\n",
    "criterion = nn.MSELoss(reduction='sum')  # SSE (sum of squared errors)\n",
    "eta = 1e-2\n",
    "\n",
    "# Dummy data\n",
    "X_sample = torch.randn(5, 10)\n",
    "y_sample = torch.randn(5, 1)\n",
    "\n",
    "y_pred = model(X_sample)\n",
    "loss = criterion(y_pred, y_sample)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()\n",
    "\n",
    "# Gradient descent step\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= eta * param.grad\n",
    "        param.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85fa46",
   "metadata": {},
   "source": [
    "### 1.4 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Though the practical focuses on regression, CNNs (p. 198–199) illustrate deep architectures that handle grid-structured data (e.g., images). Main ideas:\n",
    "1. **Local Connectivity**: Each neuron in a convolutional layer only connects to a local patch of the input.\n",
    "2. **Shared Weights**: A small kernel (filter) is “convolved” across the entire image or feature map.\n",
    "3. **Translation Equivariance**: The same filter is applied at all spatial locations.\n",
    "\n",
    "#### 1.4.1 Convolution Operation\n",
    "A single 2D filter:\n",
    "$$\n",
    "y_{ij} = \\sum_{m=1}^{k_h} \\sum_{n=1}^{k_w} W_{mn} x_{(i + m - 1, j + n - 1)} + b,\n",
    "$$\n",
    "where $(k_h, k_w)$ is the kernel size. ReLU or another activation is then applied.\n",
    "\n",
    "#### 1.4.2 Pooling Layers\n",
    "Often, a **pooling** layer follows one or more convolutional layers to reduce the spatial resolution. For instance, **max pooling** with a $2\\times2$ window and stride 2 halves the height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4e29c",
   "metadata": {},
   "source": [
    "### 1.5 Overfitting and Cross-Validation\n",
    "\n",
    "#### 1.5.1 Overfitting Phenomenon\n",
    "\n",
    "In the handbook’s example on **p. 197**, a network with 15 hidden units achieves extremely low training SSE but high test SSE, illustrating **overfitting**. Overfitting arises when a model fits training data “too well,” capturing noise rather than the underlying trend.\n",
    "\n",
    "#### 1.5.2 Detection and Mitigation\n",
    "- **Validation Set**: Split data into training and validation. Monitor the validation error (e.g., MSE). If it increases while training error decreases, the model is overfitting.\n",
    "- **Cross-Validation**: Alternatively, use $k$-fold cross-validation.\n",
    "- **Regularization**:\n",
    "  - **Dropout**: Temporarily “drop” a fraction of hidden units during training.\n",
    "  - **Weight Decay (L2)**: Penalizes large weights.\n",
    "  - **Early Stopping**: Halt training when validation performance stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549c899",
   "metadata": {},
   "source": [
    "### 1.6 Practical Aspects: Model Saving and Loading in PyTorch\n",
    "\n",
    "For safe and reproducible experiments, one typically needs to **save**:\n",
    "1. **Model Weights (state_dict)**: The learned parameters $\\{W^{(l)},b^{(l)}\\}$.\n",
    "2. **Optimizer State (if resuming)**: Internal states like momentum buffers in gradient-based optimizers.\n",
    "\n",
    "A typical code pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b53079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model = Simple2LayerMLP(10, 15, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training... then save checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': 10\n",
    "}, \"checkpoint.pth\")\n",
    "\n",
    "# Later, to resume\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "# For inference, only model_state_dict is strictly required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de37b8",
   "metadata": {},
   "source": [
    "### 1.7 Complete example of a CNN for Regression\n",
    "\n",
    "Below is a more detailed illustration of an implementation of a CNN in PyTorch for regression highlighting many of the points presented above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# This CNN will perform a regression task (e.g., predicting a real-valued output from an image).\n",
    "# We'll use two convolution layers (with 2x2 max pooling after each), then two fully-connected layers,\n",
    "# include dropout with probability=0.25, and weight decay in the optimizer for regularization.\n",
    "\n",
    "class SimpleCNNRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        \n",
    "        # After two 2x2 pools on a 28x28 input, the spatial dimension goes 28->14->7\n",
    "        # => final feature map is 16 x 7 x 7 = 16*7*7 = 784\n",
    "        self.fc1   = nn.Linear(16 * 7 * 7, 64)  \n",
    "        self.fc2   = nn.Linear(64, 1)          # single output for regression\n",
    "        \n",
    "        # Dropout layer to help regularize the fully connected part\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        \n",
    "        # MaxPool layer (2x2) used after each convolution\n",
    "        self.pool   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv: conv1 -> ReLU -> pool => shape: (N, 8, 14, 14)\n",
    "        x = self.conv1(x)        \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second conv: conv2 -> ReLU -> pool => shape: (N, 16, 7, 7)\n",
    "        x = self.conv2(x)        \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten => shape: (N, 16*7*7 = 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # First fully connected => ReLU => Dropout => shape: (N, 64)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final fully connected => shape: (N, 1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# We'll create some dummy data for regression:\n",
    "# - Shape of X: (N, 1, 28, 28)\n",
    "# - Shape of y: (N, 1) containing continuous values\n",
    "N_train = 1000\n",
    "N_val   = 200\n",
    "X_train = torch.randn(N_train, 1, 28, 28)\n",
    "y_train = torch.randn(N_train, 1)  # random continuous targets\n",
    "X_val   = torch.randn(N_val, 1, 28, 28)\n",
    "y_val   = torch.randn(N_val, 1)\n",
    "\n",
    "# We'll wrap the data in TensorDatasets and DataLoaders for batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset   = TensorDataset(X_val, y_val)\n",
    "batch_size    = 50\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate our CNN model\n",
    "model = SimpleCNNRegression()\n",
    "\n",
    "# We'll use MSELoss for this regression task\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# We'll use Adam optimizer with weight decay for L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Setup for training with early stopping\n",
    "max_epochs         = 20\n",
    "patience           = 3  # number of epochs with no improvement allowed\n",
    "best_val_loss      = float('inf')\n",
    "epochs_no_improve  = 0\n",
    "best_model_path    = \"best_model.pth\"\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train mode: enables dropout, etc.\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # Fetch mini-batches from the training loader\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass -> compute predictions\n",
    "        preds = model(X_batch)\n",
    "        \n",
    "        # Compute MSE loss for regression\n",
    "        loss = criterion(preds, y_batch)\n",
    "        \n",
    "        # Backward pass -> compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Compute average training loss for this epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase (disable dropout, etc.)\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for Xv_batch, yv_batch in val_loader:\n",
    "            # Forward pass on validation set\n",
    "            val_preds = model(Xv_batch)\n",
    "            val_loss = criterion(val_preds, yv_batch)\n",
    "            running_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{max_epochs}]  \"\n",
    "          f\"Train MSE: {avg_train_loss:.4f}  \"\n",
    "          f\"Val MSE: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        # If validation improves, update best_val_loss and reset patience counter\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save current model as best\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"  Best model saved (improved validation MSE).\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        # If no improvement for 'patience' epochs, stop\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered (no improvement).\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining complete. Loading best model for final checks.\")\n",
    "best_model = SimpleCNNRegression()\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "\n",
    "# We can now evaluate 'best_model' on the validation set (or test set if we had one).\n",
    "# For instance, a quick check of final validation MSE:\n",
    "final_val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for Xv_batch, yv_batch in val_loader:\n",
    "        val_preds = best_model(Xv_batch)\n",
    "        val_loss = criterion(val_preds, yv_batch)\n",
    "        final_val_loss += val_loss.item()\n",
    "print(f\"Final validation MSE (best model): {final_val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4b128",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 Introduction to PyTorch tensors\n",
    "\n",
    "PyTorch is a popular open-source deep learning framework, and **tensors** are its core data structure. In PyTorch, tensors are the central data abstraction – a specialized data structure very similar to arrays or matrices, used to represent data (inputs, outputs, model parameters) in deep learning models. Tensors are conceptually like NumPy’s multi-dimensional arrays, but with two key advantages: they can run on GPUs (for accelerated computing), and they support automatic differentiation (through PyTorch’s autograd engine) for computing gradients. These features make tensors fundamental for building and training neural networks.\n",
    "\n",
    "In this introduction, we’ll explore PyTorch tensors from the ground up. We assume you are comfortable with Python and NumPy, but completely new to PyTorch. We’ll cover what tensors are and how they compare to NumPy arrays, how to create and manipulate them, basic operations, how PyTorch’s autograd works for automatic differentiation, and some additional important tensor functionalities like broadcasting, using GPUs, and in-place operations. Along the way, you’ll find hands-on code examples you can run and modify to solidify your understanding.\n",
    "\n",
    "Let's get started by importing PyTorch (the `torch` library) and seeing how to create tensors in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbdd63",
   "metadata": {},
   "source": [
    "PyTorch (`torch`) provides the `Tensor` class, which, as mentioned, is similar to NumPy’s `ndarray`s but with GPU support and gradient computation. If you’re familiar with NumPy arrays, you’ll find the tensor API quite intuitive. In fact, many operations (creation, indexing, math, etc.) have a very similar syntax.\n",
    "\n",
    "One important difference is that PyTorch operations can be automatically differentiated (if required), which is essential for tasks like training neural networks. Before diving in, let’s import the PyTorch library. (Make sure you have PyTorch installed in your environment; if not, you can install it via `pip` or `conda`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np  # we'll use NumPy to compare with PyTorch\n",
    "\n",
    "# Now we’re ready to explore tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20877a9a",
   "metadata": {},
   "source": [
    "### 2.1 Creating Tensors\n",
    "\n",
    "There are many ways to create a tensor in PyTorch. Here are some of the most common methods:\n",
    "\n",
    "1. **From Python data**: You can create a tensor directly from a Python list or sequence of numbers using `torch.tensor()`. PyTorch will automatically infer the data type (dtype) if not specified.\n",
    "2. **From a NumPy array**: PyTorch can convert NumPy `ndarray`s into tensors with `torch.from_numpy()`, and vice versa using the Tensor’s `.numpy()` method. This makes it easy to move data between NumPy and PyTorch. (Notably, if the NumPy array is on CPU, the tensor and the array will share the same memory buffer, so changing one will also change the other.)\n",
    "3. **Using factory functions**: PyTorch provides factory methods to create tensors with specific values or distributions. For example, `torch.zeros(shape)` creates a tensor filled with zeros, `torch.ones(shape)` creates one filled with ones, and `torch.rand(shape)` creates one with random values uniformly sampled between 0 and 1. You can also create tensors based on an existing tensor’s properties with functions like `torch.ones_like(existing_tensor)` or `torch.rand_like(existing_tensor)`.\n",
    "4. **Uninitialized tensors**: If you need an uninitialized tensor (just allocating memory without setting values), you can use `torch.empty(shape)`.\n",
    "5. **Specifying data types and devices**: All the above functions accept a `dtype` parameter to specify the tensor’s data type (e.g. `torch.float32`, `torch.int64`, etc.), and a `device` parameter to specify whether the tensor should reside on the CPU or GPU. By default, PyTorch tensors are created with `dtype=torch.float32` (for floating point numbers) and on the CPU.\n",
    "\n",
    "Let’s go through some examples of creating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor from a Python list\n",
    "data_list = [[1, 2, 3], [4, 5, 6]]\n",
    "x_data = torch.tensor(data_list)\n",
    "print(\"Tensor from list:\")\n",
    "print(x_data)\n",
    "\n",
    "# Creating a tensor from a NumPy array\n",
    "np_array = np.array(data_list)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(\"\\nTensor from NumPy array:\")\n",
    "print(x_np)\n",
    "\n",
    "# Verify that modifying the NumPy array changes the tensor (shared memory)\n",
    "np_array[0, 0] = 99\n",
    "print(\"\\nAfter modifying numpy array, tensor becomes:\")\n",
    "print(x_np)\n",
    "\n",
    "# Creating tensors using factory functions\n",
    "shape = (2, 3)\n",
    "x_zeros = torch.zeros(shape)\n",
    "x_ones = torch.ones(shape)\n",
    "x_rand = torch.rand(shape)\n",
    "print(\"\\nZero Tensor:\\n\", x_zeros)\n",
    "print(\"Ones Tensor:\\n\", x_ones)\n",
    "print(\"Random Tensor:\\n\", x_rand)\n",
    "\n",
    "# Creating tensors based on existing ones\n",
    "x_like = torch.ones_like(x_rand)        # tensor of ones with same shape as x_rand\n",
    "x_like_float = torch.rand_like(x_data, dtype=torch.float)  # random tensor with same shape as x_data, but forced float type\n",
    "print(\"\\nOnes_like Tensor:\\n\", x_like)\n",
    "print(\"Rand_like Tensor with float dtype:\\n\", x_like_float)\n",
    "\n",
    "# Checking tensor attributes\n",
    "print(\"\\nTensor shape:\", x_data.shape)\n",
    "print(\"Tensor data type:\", x_data.dtype)\n",
    "print(\"Tensor device:\", x_data.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91227d1",
   "metadata": {},
   "source": [
    "### 2.2 Basic Tensor Operations\n",
    "\n",
    "Once you have tensors, PyTorch provides a rich set of operations to manipulate them. In fact, there are over 100 tensor operations available, including arithmetic, linear algebra, slicing, reshaping, and more. Many of these mirror NumPy operations in syntax and function. Let’s go through some fundamental operations.\n",
    "\n",
    "#### Arithmetic operations\n",
    "\n",
    "Arithmetic on tensors is typically element-wise (just like with NumPy arrays). You can use Python arithmetic operators (`+`, `-`, `*`, `/`) or the equivalent `torch` functions (`torch.add`, `torch.sub`, etc.). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise arithmetic operations\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"a + b =\", a + b)        # element-wise addition\n",
    "print(\"a * b =\", a * b)        # element-wise multiplication\n",
    "print(\"a - b =\", a - b)        # element-wise subtraction\n",
    "print(\"a / b =\", a / b)        # element-wise division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e08f05",
   "metadata": {},
   "source": [
    "The results should be as expected:\n",
    "\n",
    "- `a + b` gives `[5.0, 7.0, 9.0]`\n",
    "- `a * b` gives `[4.0, 10.0, 18.0]`\n",
    "- etc.\n",
    "\n",
    "If you want to do **matrix multiplication** (dot products, etc.), you should use the `@` operator or `torch.matmul`. For example, `A @ B` does matrix multiply if `A` and `B` are 2-D matrices (or higher-dim, following broadcast batch rules). For 1-D vectors, `a @ b` will give a scalar dot product. Let’s see a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication vs elementwise\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "B = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "print(\"Element-wise multiply:\\n\", A * B)   # same shape -> elementwise product\n",
    "print(\"Matrix multiply:\\n\", A @ B)         # 2x2 @ 2x2 -> 2x2 matrix result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79aad1",
   "metadata": {},
   "source": [
    "Here, `A * B` will do element-wise multiplication (resulting in `[[5, 12], [21, 32]]`), whereas `A @ B` will compute the matrix product (result `[[19, 22], [43, 50]]`).\n",
    "\n",
    "#### Indexing and slicing\n",
    "\n",
    "You can access and modify parts of tensors using indexing and slicing, just like with NumPy. PyTorch uses 0-based indexing. You can use indices, ranges, colons, and even Boolean masks or index tensors (though advanced indexing is outside our scope here). Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2-D tensor (matrix) for demonstration\n",
    "x = torch.tensor([[10, 11, 12],\n",
    "                  [20, 21, 22],\n",
    "                  [30, 31, 32]])\n",
    "print(\"Original tensor:\\n\", x)\n",
    "\n",
    "# Indexing a single element\n",
    "print(\"\\nElement at row 0, col 1:\", x[0, 1].item())   # .item() to get Python scalar\n",
    "\n",
    "# Slicing rows and columns\n",
    "print(\"\\nFirst row:\", x[0])            # equivalent to x[0, :]\n",
    "print(\"Last column:\", x[:, -1])       # all rows, last column\n",
    "\n",
    "# Slice a submatrix (e.g., top-left 2x2 block)\n",
    "sub_x = x[0:2, 0:2]   # rows 0-1 and cols 0-1\n",
    "print(\"\\nTop-left 2x2 sub-tensor:\\n\", sub_x)\n",
    "\n",
    "# Modify part of the tensor\n",
    "x[1:, 1:] = torch.tensor([[ -1, -2],\n",
    "                          [ -3, -4]])   # set bottom-right 2x2 block to new values\n",
    "print(\"\\nTensor after modification:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbe358",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "- We created a 3×3 tensor `x` with some distinct values.\n",
    "- We accessed a single element `x[0, 1]` (which should be `11` in the original tensor). Using `.item()` converts the 0-dim tensor to a Python number.\n",
    "- We took the first row `x[0]` (which yields `[10, 11, 12]`) and the last column `x[:, -1]` (which yields `[12, 22, 32]`).\n",
    "- We sliced a 2×2 sub-tensor (the upper-left corner).\n",
    "- We then modified a sub-section of the tensor (`x[1:, 1:]`) by assigning a new tensor of the same shape. The original tensor `x` is changed in place.\n",
    "\n",
    "Note: Slicing a tensor (e.g., `y = x[0:2, 0:2]`) will result in a **view** of the original tensor whenever possible (i.e., it doesn’t copy the data). So modifying `y` would also modify `x`. If you need to explicitly copy a tensor or sub-tensor, you can use `.clone()`.\n",
    "\n",
    "#### Reshaping and concatenation\n",
    "\n",
    "Often you'll want to change the shape or dimensionality of a tensor without changing its data. This is done with operations like `tensor.view()` or `tensor.reshape()`. Both do similar things (though `.view()` only works on contiguous tensors). For simplicity, we'll use `reshape` here.\n",
    "\n",
    "You can also join tensors together (concatenate) along a given dimension using `torch.cat` or stack them with `torch.stack` (which adds a new dimension). Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d05b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape (view) a tensor\n",
    "t = torch.arange(16)        # 1-D tensor [0, 1, 2, ..., 15]\n",
    "print(\"Original tensor t shape:\", t.shape)\n",
    "\n",
    "t_matrix = t.reshape(4, 4)  # reshape to 4x4 matrix\n",
    "print(\"t reshaped to 4x4:\\n\", t_matrix)\n",
    "\n",
    "# Transpose the matrix (swap dimensions)\n",
    "print(\"t_matrix transposed:\\n\", t_matrix.T)\n",
    "\n",
    "# Concatenate two tensors\n",
    "t1 = torch.tensor([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9],\n",
    "                   [10, 11, 12]])\n",
    "t_cat0 = torch.cat([t1, t2], dim=0)  # concatenate along rows (dim=0)\n",
    "t_cat1 = torch.cat([t1, t2], dim=1)  # concatenate along columns (dim=1)\n",
    "\n",
    "print(\"\\nTensor t1:\\n\", t1)\n",
    "print(\"Tensor t2:\\n\", t2)\n",
    "print(\"t1 and t2 concatenated along dim=0 (shape {}):\\n\".format(t_cat0.shape), t_cat0)\n",
    "print(\"t1 and t2 concatenated along dim=1 (shape {}):\\n\".format(t_cat1.shape), t_cat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9198e",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- We created a 1-D tensor `t` with 16 sequential elements. We reshaped it into a 4×4 tensor `t_matrix`.\n",
    "- We transposed `t_matrix` with `.T`.\n",
    "- We took two 2×3 tensors `t1` and `t2` and concatenated them. Along `dim=0` (rows), we got a 4×3 tensor by stacking `t2` below `t1`. Along `dim=1` (columns), we got a 2×6 tensor by stacking `t2` to the right of `t1`.\n",
    "\n",
    "These basic operations (arithmetic, indexing, reshaping, concatenation, etc.) allow you to manipulate tensor data in flexible ways. PyTorch’s API is designed to be highly compatible with NumPy’s where it makes sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15194e",
   "metadata": {},
   "source": [
    "### 2.3 Autograd and Gradients\n",
    "\n",
    "One of the most powerful features of PyTorch is its **autograd** system, which allows automatic computation of gradients for tensor operations. This is what enables training of neural networks using gradient descent: you define a forward computation, and PyTorch can compute the backward gradients for you.\n",
    "\n",
    "In PyTorch, the `torch.autograd` engine tracks all operations on tensors that have `requires_grad=True`. By default, tensors do **not** track gradients. You need to specify which tensors require gradient computation (usually the learnable parameters of your model, or any input for which you want to compute a derivative) by setting `requires_grad=True` when creating the tensor or by calling `.requires_grad_()` on an existing tensor.\n",
    "\n",
    "When you perform operations on such tensors, PyTorch builds a computational graph behind the scenes: nodes are tensors, and edges are functions that produce output tensors from input tensors. Once you have a result (typically a scalar loss value), you can call `.backward()` on it, and PyTorch will automatically traverse the graph to compute the gradient of that result with respect to every tensor that has `requires_grad=True` (using the chain rule of calculus). Let’s see a simple example to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d75cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple autograd example: y = f(x) where f(x) = x^2 + 2x + 1\n",
    "x = torch.tensor(3.0, requires_grad=True)   # define a tensor x with gradient tracking\n",
    "print(\"x:\", x)\n",
    "\n",
    "# Define a function of x\n",
    "y = x**2 + 2*x + 1\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Compute the gradient dy/dx by calling backward on y\n",
    "y.backward()    # computes gradient of y w.r.t. x\n",
    "print(\"dy/dx at x=3:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54ea48",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- We created a tensor `x` with value `3.0` and `requires_grad=True`. This means PyTorch will track operations on `x`.\n",
    "- We computed `y = x**2 + 2*x + 1`. Because `x` requires grad, `y` will also by default require grad and have a grad function associated.\n",
    "- We called `y.backward()`. Since `y` is a scalar (single value), we can call `backward` directly. After this call, the gradient of `y` with respect to `x` is computed and stored in `x.grad`.\n",
    "- We printed `x.grad`. The derivative $ y = x^2 + 2x + 1 $ is $ dy/dx = 2x + 2 $. At $ x = 3 $, that is `8`.\n",
    "\n",
    "**Important notes about autograd**:\n",
    "\n",
    "1. You can disable gradient tracking by wrapping code in `with torch.no_grad():` or by using `.detach()` on a tensor to get a new tensor that shares the same data but with no grad tracking.\n",
    "2. If `y` is not a scalar, you need to specify a gradient argument in `y.backward(gradient=...)` which is the tensor of same shape as `y` containing the gradient of the final result w.r.t. each element of `y`.\n",
    "3. By default, gradients are **accumulated** into the `.grad` property. So typically you want to zero out gradients (e.g., `x.grad.zero_()`) between independent backward passes.\n",
    "\n",
    "Autograd is what powers training routines. If you had a multi-parameter function (like a neural network with many weights), you would set `requires_grad=True` for all those parameters. After computing a loss (scalar), you’d call `loss.backward()`, and each parameter tensor’s `.grad` will be filled with its gradient.\n",
    "\n",
    "To reinforce understanding, let’s do a slightly more complex autograd example with two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd with two inputs: z = x * y + y^2\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(-3.0, requires_grad=True)\n",
    "\n",
    "z = x * y + y**2   # z is a function of both x and y\n",
    "print(\"z:\", z)\n",
    "\n",
    "z.backward()  # Compute gradients dz/dx and dz/dy\n",
    "print(\"dz/dx:\", x.grad)   # should be y\n",
    "print(\"dz/dy:\", y.grad)   # should be x + 2y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2adcb75",
   "metadata": {},
   "source": [
    "Here $ z = x \\times y + y^2 $. We expect $ dz/dx = y $ and $ dz/dy = x + 2y $. With $ x = 2.0 $ and $ y = -3.0 $, those values are $ dz/dx = -3 $ and $ dz/dy = 2 + 2(-3) = -4 $. The autograd should produce the same results in `x.grad` and `y.grad`.\n",
    "\n",
    "### 2.4 Advanced Tensor Functions\n",
    "\n",
    "#### 2.4.1 Broadcasting\n",
    "\n",
    "Like NumPy, PyTorch supports **broadcasting** – a mechanism that allows arithmetic operations on tensors of different shapes by automatically expanding one of them to match the shape of the other. Broadcasting rules in PyTorch are the same as NumPy’s. In short, two tensors are compatible for an elementwise operation if their shapes are equal or align in such a way that one of them can be expanded (a dimension of size 1 can be expanded to match the other’s size, and missing dimensions are treated as size 1).\n",
    "\n",
    "For example, if you have a tensor of shape (4, 3) and another of shape (3,), you can add them because the second tensor can be treated as if it were (1, 3) and then expanded to (4, 3) to match the first tensor’s shape.\n",
    "\n",
    "In code, we can demonstrate broadcasting with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting example: add a vector to each row of a matrix\n",
    "mat = torch.tensor([[0, 0, 0],\n",
    "                    [10, 10, 10],\n",
    "                    [20, 20, 20],\n",
    "                    [30, 30, 30]])   # shape (4,3)\n",
    "vec = torch.tensor([1, 2, 3])        # shape (3,)\n",
    "\n",
    "result = mat + vec  # vec is broadcast to shape (4,3)\n",
    "print(\"Matrix:\\n\", mat)\n",
    "print(\"Vector:\\n\", vec)\n",
    "print(\"Result of mat + vec:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72704814",
   "metadata": {},
   "source": [
    "Here, `vec` is added to each row of `mat`. The expected result would be each row of `mat` incremented by `[1, 2, 3]`. Broadcasting is a powerful feature that makes your code more concise and often more efficient.\n",
    "\n",
    "#### 2.4.2 Device Management (CPU vs GPU)\n",
    "\n",
    "One of PyTorch’s strengths is the ability to perform computations on a **GPU** for speed. Tensors can reside on either the CPU or the GPU. By default, tensors are created on the CPU.\n",
    "You can check where a tensor is located by looking at its `.device` attribute. To leverage a GPU (if you have one and PyTorch is installed with CUDA support), you need to explicitly move tensors to the GPU. There are a few ways to do this:\n",
    "\n",
    "1. Using the `.to(device)` method on a tensor, where `device` is something like `torch.device(\"cuda\")` or the shorthand string `\"cuda\"`.\n",
    "2. Using `.cuda()` method on the tensor.\n",
    "3. Creating the tensor directly on the GPU by specifying `device=torch.device(\"cuda\")` (or `device=\"cuda\"`) in the creation function.\n",
    "\n",
    "It’s common to write code that is device-agnostic (runs on CPU if no GPU is available, otherwise uses the GPU). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device-agnostic code: choose CPU or CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create a tensor on the chosen device\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "print(\"x device:\", x.device)\n",
    "\n",
    "# Alternatively, move an existing tensor to the device\n",
    "y = torch.ones(3)            # defaults to CPU\n",
    "y = y.to(device)             # move to GPU if available\n",
    "print(\"y device:\", y.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bf998",
   "metadata": {},
   "source": [
    "If you actually run this on a system with a GPU and CUDA installed, you should see device printed as `cuda:0` and the tensor devices as such. On a CPU-only system, it will say `cpu`.\n",
    "\n",
    "A quick verification of CPU vs GPU performance can be done by timing a large matrix multiplication. (Note: if you actually run this, avoid printing the giant result; just measure the time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235caa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Large tensor operation on CPU vs GPU (if available)\n",
    "large_cpu = torch.randn(10000, 10000)  # 10000x10000 matrix on CPU\n",
    "start = time.time()\n",
    "res_cpu = large_cpu @ large_cpu       # matrix multiply on CPU\n",
    "end = time.time()\n",
    "print(\"CPU computation time:\", end - start, \"seconds\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    large_gpu = large_cpu.to(\"cuda\")\n",
    "    torch.cuda.synchronize()  # ensure data is transferred\n",
    "    start = time.time()\n",
    "    res_gpu = large_gpu @ large_gpu   # matrix multiply on GPU\n",
    "    torch.cuda.synchronize()  # wait for GPU to finish computation\n",
    "    end = time.time()\n",
    "    print(\"GPU computation time:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efec80",
   "metadata": {},
   "source": [
    "This snippet will likely show that the GPU computation is significantly faster for large matrix operations (depending on hardware). Remember to move your **model parameters** and **data** to the same device before doing operations.\n",
    "\n",
    "For example, the code below moves a tensor to GPU if available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c367f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3)\n",
    "print(\"Before moving, device:\", tensor.device)\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(\"After moving, device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69600368",
   "metadata": {},
   "source": [
    "Running this will print:\n",
    "```\n",
    "Before moving, device: cpu\n",
    "After moving, device: cuda:0\n",
    "```\n",
    "if you have a GPU.\n",
    "\n",
    "#### 2.4.3 In-place Operations\n",
    "\n",
    "PyTorch differentiates between operations that create new tensors and those that modify an existing tensor **in-place**. In-place operations are identified by an underscore suffix in the method name. For example, `tensor.add_(5)` will add 5 to every element of the tensor **in-place** (mutating the tensor), whereas `tensor.add(5)` returns a new tensor with the result and leaves the original unchanged. All functions ending in `_` modify the tensor in-place.\n",
    "\n",
    "Why use in-place ops? They can save memory and sometimes a bit of compute, because you don’t allocate a new tensor for the result. However, be careful: in-place operations can sometimes interfere with autograd (if you modify values required to compute gradients). As a rule of thumb, avoid using in-place ops on tensors that require grad unless you know what you’re doing.\n",
    "\n",
    "Let’s demonstrate the difference between an in-place operation and its out-of-place counterpart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aab55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(\"a initially:\", a)\n",
    "\n",
    "# Out-of-place addition\n",
    "b = a.add(5)\n",
    "print(\"\\nAfter b = a.add(5):\")\n",
    "print(\"a =\", a)  # a should remain unchanged\n",
    "print(\"b =\", b)  # b is the new tensor\n",
    "\n",
    "# In-place addition\n",
    "a.add_(5)\n",
    "print(\"\\nAfter a.add_(5):\")\n",
    "print(\"a =\", a)  # a is now changed in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5c1af",
   "metadata": {},
   "source": [
    "Output should show:\n",
    "\n",
    "1. Initially, `a` is `[1, 1, 1, 1, 1]`.\n",
    "2. After `b = a.add(5)`, `a` stays `[1, 1, 1, 1, 1]` but `b` is `[6, 6, 6, 6, 6]`.\n",
    "3. After `a.add_(5)`, now `a` has become `[6, 6, 6, 6, 6]` in-place.\n",
    "\n",
    "Another example with multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da66cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3)\n",
    "y = x.mul_(2)   # in-place multiply x by 2\n",
    "print(\"x after in-place mul_:\", x)\n",
    "print(\"y (result of x.mul_(2)):\", y)\n",
    "\n",
    "# Now x is [2,2,2], let's do out-of-place multiply\n",
    "z = x.mul(3)\n",
    "print(\"x after out-of-place mul(3):\", x)  # x remains [2,2,2]\n",
    "print(\"z (result of x.mul(3)):\", z)       # z is [6,6,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d965cbe",
   "metadata": {},
   "source": [
    "This shows that after the in-place multiply, `x` (and `y`) have doubled their values, while the out-of-place multiply by 3 produces a new tensor `z` and leaves `x` the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c661e07",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aecb33-751e-49ac-b677-baaf1322ad55",
   "metadata": {},
   "source": [
    "### 3.1 Comparison of symbolic and automatic differentiation in a simple feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd872e64",
   "metadata": {},
   "source": [
    "In this exercise, we will illustrate how **automatic differentiation** (as implemented in PyTorch) matches with the result of **symbolic differentiation** for a simple two-hidden-neuron feedforward neural network. You should:\n",
    "\n",
    "1. **Define** a simple feedforward neural network (FNN) with two hidden neurons $z_1$ and $z_2$ using the values and notation from the figure below.\n",
    "2. **Compute** the network output and the loss function (a squared error).\n",
    "3. **Derive** the gradient of the loss function with respect to each weight **symbolically**.\n",
    "4. **Use** PyTorch's **automatic differentiation** to compute the same gradients **numerically**.\n",
    "5. **Compare** the two results to verify they match.\n",
    "\n",
    "The feedforward neural network architecture that we will use in this exercise is the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a07cad-e5f6-46e3-b790-c2a9de39489d",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" style=\"width: 50%; height: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05fef2-c375-4b99-8770-f28aa30d0f6f",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2: Facial keypoints detection with PyTorch FCN and CNN architectures\n",
    "\n",
    "#### 3.2.1 Introduction\n",
    "Facial keypoint detection is the task of identifying important landmarks on a human face (e.g., corners of the eyes, nose tip, mouth corners) in an image. These keypoints are crucial for many applications in computer vision:\n",
    "\n",
    "- Aligning faces for recognition or verification.\n",
    "- Tracking facial expressions for emotion detection.\n",
    "- Applying augmented reality filters or effects accurately on a face.\n",
    "\n",
    "In this exercise, we will build fully connected neural networks and convolutional neural networks to detect **15 keypoints** on **96x96 grayscale face images**. The network will take an image as input and output the $ (x,y) $ coordinates of the **15 facial keypoints**.\n",
    "\n",
    "Source: [danielnouri.org](https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d707b8-ddb5-4e3a-8fb5-2d3568b3848a",
   "metadata": {},
   "source": [
    "### 3.2.2 Dataset overview\n",
    "\n",
    "We will use the **Kaggle Facial Keypoints Detection** dataset ([Kaggle challenge webpage](https://www.kaggle.com/competitions/facial-keypoints-detection)). This dataset consists of a training set of **7,049** face images (96x96 pixels, grayscale). Each image has up to **15 keypoint coordinates** labeled (30 values: $ x $ and $ y $ for each of 15 facial landmarks). For example, the keypoints include the centers of the eyes, the tip of the nose, and the corners of the mouth.\n",
    "\n",
    "#### Data format\n",
    "The training data is provided as a CSV file (`training.csv`). Each row contains:\n",
    "\n",
    "- **30 columns** of keypoint coordinates: `left_eye_center_x`, `left_eye_center_y`, `right_eye_center_x`, ... etc. (15 keypoints $\\times$ 2). If a keypoint was not labeled for a given image, its value is missing.\n",
    "- An **Image** column containing 96x96 pixel intensity values (9216 numbers) in a single string, separated by spaces.\n",
    "\n",
    "If you downloaded this exercise from the Université Virtuelle, you should have a `training.csv` file in the `data/` directory. Now, let's load the training data using pandas and examine its structure.\n",
    "\n",
    "#### Missing values\n",
    "Not all images have all 15 keypoints annotated. In fact, only about 30% of the images have all 15 keypoints; the rest have some missing keypoint labels. Nearly all images have at least a few keypoints (e.g., eyes, nose, mouth) labeled, but many did not have the more peripheral points. In this tutorial, for simplicity, we will use only images with **complete keypoint data** (all 15 points). This means we'll discard images with any missing keypoint values. (In practice, one could use data augmentation or multi-stage training to leverage partially labeled images, but we will not cover that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a00209-7de9-44d7-80fe-947f8c9273f2",
   "metadata": {},
   "source": [
    "### 3.2.3 Exercise instructions\n",
    "\n",
    "In this exercise, you should:\n",
    "1. Explore the dataset\n",
    "2. Preprocess the dataset to use it with neural networks including:\n",
    "      - handling missing values\n",
    "      - converting the image data from strings to numeric arrays\n",
    "      - normalizing the pixel values\n",
    "      - reshaping the images\n",
    "      - preparing the keypoint coordinates labels\n",
    "3. Implement a fully connected neural network and compute the validated MSE\n",
    "4. Implement a convolutional neural network and compute the validated MSE\n",
    "5. Visualize the predictions of both models with respect to the true keypoints coordinates on images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "name": "Facial Keypoints Detection with PyTorch"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
