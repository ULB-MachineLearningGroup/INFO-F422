
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Linear Models &#8212; Statistical Foundations of Machine Learning - Practicals handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '02';</script>
    <link rel="icon" href="_static/sfml.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Data Preprocessing, Local Models and Tree-Based Models" href="03.html" />
    <link rel="prev" title="2. Introduction to Probabilistic Methods and Monte Carlo Simulations" href="01.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontpage.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/sfml.png" class="logo__image only-light" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>
    <script>document.write(`<img src="_static/sfml.png" class="logo__image only-dark" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontpage.html">
                    Home
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="01.html">2. Introduction to Probabilistic Methods and Monte Carlo Simulations</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.html">4. Data Preprocessing, Local Models and Tree-Based Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.html">5. Neural Networks for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.html">6. Ensembles of models and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="06.html">7. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion.html">8. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">9. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/02.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-synthetic-multivariate-data">3.2. Generating Synthetic Multivariate Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.2.1. Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">3.2.1.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-data-inspection">3.2.2. Quick Data Inspection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols-manual-computation-using-matrix-algebra">3.3. Ordinary Least Squares (OLS) – Manual Computation Using Matrix Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-statsmodels">3.3.1. Comparison with statsmodels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observations">3.3.2. Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-and-tests">3.4. Residual Analysis and Tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residuals-vs-fitted-plot">3.4.1. Residuals vs. Fitted Plot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-q-q-plot-quantile-quantile-plot">3.4.2. Normal Q-Q Plot (Quantile-Quantile Plot)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">3.4.2.1. Comments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-transformations-polynomial-features">3.5. Input Transformations - Polynomial Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-still-a-linear-model">3.5.1. Example: Still a Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-would-not-be-a-linear-model">3.5.2. What Would NOT Be a Linear Model?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-polynomial-features-in-linear-regression">3.6. Why Use Polynomial Features in Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-feature-expansion">3.6.1. Polynomial Feature Expansion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">3.6.2. Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-implement-polynomial-features-in-python">3.6.3. How to Implement Polynomial Features in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-and-qq-plot">3.6.3.1. Residual analysis and QQ-plot</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rbf-expansion">3.7. RBF Expansion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-rbf-expansion">3.7.1. What is RBF Expansion?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-rbf-instead-of-polynomial-expansion">3.7.1.1. Why Use RBF Instead of Polynomial Expansion?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-rbf-expansion-with-random-fourier-features">3.7.2. Implementing RBF Expansion with Random Fourier Features</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.7.2.1. Comments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">3.8. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-closed-form">3.8.1. Ridge Closed-form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-for-ridge">3.8.2. Residual analysis for Ridge</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">3.9. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-models">
<h1><span class="section-number">3. </span>Linear Models<a class="headerlink" href="#linear-models" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Linear models form one of the foundational pillars of statistical learning. Their simplicity, interpretability, and strong theoretical underpinnings make them an essential first step when analyzing data and building predictive models. Despite their apparent simplicity, linear models offer a surprising level of flexibility, especially when combined with feature transformations such as polynomials or radial basis functions.</p>
<p>In this chapter, we explore the principles and practical implementation of linear models. We begin with the generation of synthetic multivariate data to simulate a controlled environment for experimentation. We then dive into the manual computation of the Ordinary Least Squares (OLS) solution using matrix algebra, before comparing our results to those produced by well-established Python libraries such as <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<p>The chapter also highlights the importance of residual analysis for model diagnostics, offering tools to detect violations of assumptions like linearity, homoscedasticity, and normality. We further extend the linear model framework through input transformations, allowing us to capture non-linear patterns while preserving the model’s linear structure in parameters.</p>
<p>Finally, we introduce <strong>Ridge Regression</strong>, a regularized variant of linear regression, and examine its role in mitigating overfitting—particularly in scenarios involving multicollinearity or high-dimensional input spaces.</p>
<p>We now assume basic familiarity with matrix algebra and Python packages such as <code class="docutils literal notranslate"><span class="pre">NumPy</span></code>, <code class="docutils literal notranslate"><span class="pre">Pandas</span></code>, and <code class="docutils literal notranslate"><span class="pre">Statsmodels</span></code>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Don’t forget: if some of those libraries are not installed on your computer, you can always install them using <a class="reference external" href="http://pypi.org">Pip</a>.</p>
</aside>
<p>We define the necessary imports as well as the <code class="docutils literal notranslate"><span class="pre">pprint</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">probplot</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpyarray_to_latex.jupyter</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_ltx</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn-v0_8-muted&#39;</span><span class="p">,</span> <span class="s1">&#39;practicals.mplstyle&#39;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pprint</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="n">to_ltx</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">brackets</span><span class="o">=</span><span class="s1">&#39;[]&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="n">i</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Math</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="generating-synthetic-multivariate-data">
<h2><span class="section-number">3.2. </span>Generating Synthetic Multivariate Data<a class="headerlink" href="#generating-synthetic-multivariate-data" title="Link to this heading">#</a></h2>
<p>We’ll create a dataset where <code class="docutils literal notranslate"><span class="pre">Y</span></code> depends on three features: <code class="docutils literal notranslate"><span class="pre">X1,</span> <span class="pre">X2,</span> <span class="pre">X3</span></code>.
The true data-generating process (DGP) is:</p>
<div class="math notranslate nohighlight">
\[
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<section id="exercise">
<h3><span class="section-number">3.2.1. </span>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Create <code class="docutils literal notranslate"><span class="pre">N</span></code> observations of <code class="docutils literal notranslate"><span class="pre">(X1,</span> <span class="pre">X2,</span> <span class="pre">X3)</span></code>.</p></li>
<li><p>Define true coefficients <code class="docutils literal notranslate"><span class="pre">beta_true</span></code>.</p></li>
<li><p>Generate noise and produce <code class="docutils literal notranslate"><span class="pre">Y</span></code>.</p></li>
<li><p>Store everything in a DataFrame for convenience.</p></li>
</ol>
<section id="solution">
<h4><span class="section-number">3.2.1.1. </span>Solution<a class="headerlink" href="#solution" title="Link to this heading">#</a></h4>
<p>The following code generates a synthetic dataset for a linear regression problem with three explanatory variables (<span class="math notranslate nohighlight">\(\mathbf X_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf X_2\)</span>, <span class="math notranslate nohighlight">\(\mathbf X_3\)</span>) and one target variable (<span class="math notranslate nohighlight">\(\mathbf Y\)</span>). It sets a random seed for reproducibility and defines true regression coefficients (<code class="docutils literal notranslate"><span class="pre">beta_true</span></code>) and a standard deviation for the noise (<code class="docutils literal notranslate"><span class="pre">sigma_true</span></code>). The explanatory variables are drawn from different distributions: uniform for <span class="math notranslate nohighlight">\(\mathbf X_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf X_2\)</span>, and normal for <span class="math notranslate nohighlight">\(\mathbf X_3\)</span>. The true response <code class="docutils literal notranslate"><span class="pre">Y_true</span></code> is computed as a linear combination of the variables with the defined coefficients, and Gaussian noise is added to simulate observed values of <span class="math notranslate nohighlight">\(\mathbf Y\)</span>. The resulting dataset, stored in a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> DataFrame, includes the predictors and the noisy target variable, and the first few rows are displayed using <code class="docutils literal notranslate"><span class="pre">df.head()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X1&#39;</span><span class="p">:</span><span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">:</span><span class="n">X2</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">:</span><span class="n">X3</span><span class="p">})</span>

<span class="n">Y_true</span> <span class="o">=</span> <span class="p">(</span><span class="n">beta_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
          <span class="o">+</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">]</span>
          <span class="o">+</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X2&#39;</span><span class="p">]</span>
          <span class="o">+</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X3&#39;</span><span class="p">])</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_true</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>X3</th>
      <th>Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.964692</td>
      <td>0.852719</td>
      <td>-3.510804</td>
      <td>5.687964</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.138607</td>
      <td>-8.664511</td>
      <td>-0.697215</td>
      <td>16.509223</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-2.731485</td>
      <td>3.067297</td>
      <td>-0.385230</td>
      <td>-4.050829</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.513148</td>
      <td>9.921727</td>
      <td>0.898271</td>
      <td>-12.453060</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.194690</td>
      <td>5.387947</td>
      <td>-0.290727</td>
      <td>-1.697682</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="quick-data-inspection">
<h3><span class="section-number">3.2.2. </span>Quick Data Inspection<a class="headerlink" href="#quick-data-inspection" title="Link to this heading">#</a></h3>
<p>The following code performs exploratory data analysis on a DataFrame <code class="docutils literal notranslate"><span class="pre">df</span></code>. It first prints summary statistics of the DataFrame using <code class="docutils literal notranslate"><span class="pre">df.describe()</span></code>, which includes metrics like mean, standard deviation, min, and max for each numerical column. Then, it creates a pairplot using Seaborn with regression lines (<code class="docutils literal notranslate"><span class="pre">kind='reg'</span></code>) to visualize pairwise relationships between all numerical features in the DataFrame. The regression lines are colored red, as specified in the <code class="docutils literal notranslate"><span class="pre">plot_kws</span></code>. A title is added slightly above the plot using <code class="docutils literal notranslate"><span class="pre">plt.suptitle()</span></code>, and finally, the complete visualization is displayed with <code class="docutils literal notranslate"><span class="pre">plt.show()</span></code>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Doing a first data inspection is extremely important. In real-life cases, you could discover patterns in the data, discard particular features, or even, sometimes, solve the problem you are tackling just by extracting visual information by the data. This step also helps understanding the problem you are dealing with. The more visualization of the data you make, the more expert you become on the dataset. Never underestimate the importance of the base knowledge of the problem: <em>data is more than just numbers</em>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;reg&#39;</span><span class="p">,</span> <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;line_kws&#39;</span><span class="p">:{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s1">&#39;green&#39;</span><span class="p">}})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Pairplot of the Generated Data&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>               X1          X2          X3           Y
count  200.000000  200.000000  200.000000  200.000000
mean     0.032708   -0.193044   -0.112239    5.572700
std      2.663947    5.907073    1.963533   12.473036
min     -4.973119   -9.929356   -5.262876  -20.342999
25%     -1.953937   -5.254828   -1.286318   -4.258475
50%      0.207716   -0.714124   -0.101728    5.417761
75%      2.181854    5.168943    1.206797   14.861563
max      4.953585    9.921727    4.501352   30.849943
</pre></div>
</div>
<img alt="_images/95f646eef33f40059375211048f6ab1c34d3812bc267383150ad89f9c9933813.png" src="_images/95f646eef33f40059375211048f6ab1c34d3812bc267383150ad89f9c9933813.png" />
</div>
</div>
</section>
</section>
<section id="ordinary-least-squares-ols-manual-computation-using-matrix-algebra">
<h2><span class="section-number">3.3. </span>Ordinary Least Squares (OLS) – Manual Computation Using Matrix Algebra<a class="headerlink" href="#ordinary-least-squares-ols-manual-computation-using-matrix-algebra" title="Link to this heading">#</a></h2>
<p>We begin by specifying the standard linear regression model in matrix form:</p>
<div class="math notranslate nohighlight">
\[
Y = X \boldsymbol{\beta} + \boldsymbol{\epsilon},
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is an <span class="math notranslate nohighlight">\(N \times 1\)</span> vector of observed responses,</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is an <span class="math notranslate nohighlight">\(N \times p\)</span> <em>design matrix</em>, which includes a column of ones to account for the intercept,</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector of regression coefficients (including the intercept),</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is an <span class="math notranslate nohighlight">\(N \times 1\)</span> vector of errors or residuals.</p></li>
</ul>
<p>The goal of Ordinary Least Squares is to find the vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes the sum of squared residuals. There is a well-known closed-form solution for this optimization problem, given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} = (X^\top X)^{-1} X^\top Y.
\]</div>
<p>This formula provides the best linear unbiased estimator (BLUE) of the coefficients under standard assumptions (linearity, independence, homoscedasticity, and normally distributed errors).</p>
<p>In the following steps, we will:</p>
<ol class="arabic simple">
<li><p>Manually construct the design matrix <span class="math notranslate nohighlight">\(X\)</span>, ensuring it includes a column of ones for the intercept.</p></li>
<li><p>Compute the estimated coefficients <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> using NumPy’s matrix operations, specifically <code class="docutils literal notranslate"><span class="pre">np.linalg.inv(...)</span></code>.</p></li>
<li><p>Compare the computed estimates with the known or true values of the coefficients (<code class="docutils literal notranslate"><span class="pre">beta_true</span></code>) used to generate the data.</p></li>
<li><p>Validate our manual computation by comparing it to the output from the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> OLS implementation.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;X2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;X3&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="p">])</span> 

<span class="n">y_vec</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_mat shape:&quot;</span><span class="p">,</span> <span class="n">X_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_vec shape:&quot;</span><span class="p">,</span> <span class="n">y_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X_mat shape: (200, 4)
y_vec shape: (200, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">XTX</span> <span class="o">=</span> <span class="n">X_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_mat</span>
<span class="n">XTX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XTX</span><span class="p">)</span>
<span class="n">XTy</span> <span class="o">=</span> <span class="n">X_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_vec</span>
<span class="n">beta_hat_manual</span> <span class="o">=</span> <span class="n">XTX_inv</span> <span class="o">@</span> <span class="n">XTy</span>

<span class="n">beta_hat_manual</span> <span class="o">=</span> <span class="n">beta_hat_manual</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{Manual OLS estimates}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_manual</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{True coefficients}:&quot;</span><span class="p">,</span> <span class="n">beta_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{Manual OLS estimates}:\left[
\begin{array}{}
  5.2043 &amp;  1.4772 &amp; -1.9629 &amp;  0.5243
\end{array}
\right]\]</div>
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{True coefficients}:\left[
\begin{array}{}
  5.0000 &amp;  1.5000 &amp; -2.0000 &amp;  0.5000
\end{array}
\right]\]</div>
</div>
</div>
<p>We see the manually computed OLS estimates are close to the true ones.</p>
<section id="comparison-with-statsmodels">
<h3><span class="section-number">3.3.1. </span>Comparison with statsmodels<a class="headerlink" href="#comparison-with-statsmodels" title="Link to this heading">#</a></h3>
<p>We’ll let <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> compute OLS, then compare the results in detail.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note, concretely, you are expected to be able to compute all the algorithms <em>by hand</em>, meaning, <em>in plain <code class="docutils literal notranslate"><span class="pre">Python</span></code> code</em>. However, in real-life, you can of course use the existing libraries, and knowing the most common ones is part of the basic knowledge expected from a modern data scientist.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_design_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span><span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">]])</span>
<span class="n">model_ols</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">X_design_sm</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_ols</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="n">beta_hat_sm</span> <span class="o">=</span> <span class="n">model_ols</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;statsmodels OLS:&quot;</span><span class="p">,</span> <span class="n">beta_hat_sm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manual OLS:&quot;</span><span class="p">,</span> <span class="n">beta_hat_manual</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.976
Model:                            OLS   Adj. R-squared:                  0.976
Method:                 Least Squares   F-statistic:                     2657.
Date:                Wed, 16 Apr 2025   Prob (F-statistic):          2.01e-158
Time:                        11:06:50   Log-Likelihood:                -415.03
No. Observations:                 200   AIC:                             838.1
Df Residuals:                     196   BIC:                             851.2
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.2043      0.138     37.720      0.000       4.932       5.476
X1             1.4772      0.052     28.497      0.000       1.375       1.579
X2            -1.9629      0.023    -83.872      0.000      -2.009      -1.917
X3             0.5243      0.070      7.446      0.000       0.385       0.663
==============================================================================
Omnibus:                        1.025   Durbin-Watson:                   2.042
Prob(Omnibus):                  0.599   Jarque-Bera (JB):                1.117
Skew:                          -0.117   Prob(JB):                        0.572
Kurtosis:                       2.719   Cond. No.                         5.91
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
statsmodels OLS: [ 5.20431058  1.47721924 -1.96286883  0.52430039]
Manual OLS: [ 5.20431058  1.47721924 -1.96286883  0.52430039]
</pre></div>
</div>
</div>
</div>
</section>
<section id="observations">
<h3><span class="section-number">3.3.2. </span>Observations<a class="headerlink" href="#observations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The <em>plain <code class="docutils literal notranslate"><span class="pre">Python</span></code></em> results match <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> results (modulo tiny numerical differences).</p></li>
<li><p>Both are near the true coefficients <code class="docutils literal notranslate"><span class="pre">(5.0,</span> <span class="pre">1.5,</span> <span class="pre">-2.0,</span> <span class="pre">0.5)</span></code>.</p></li>
</ul>
</section>
</section>
<section id="residual-analysis-and-tests">
<h2><span class="section-number">3.4. </span>Residual Analysis and Tests<a class="headerlink" href="#residual-analysis-and-tests" title="Link to this heading">#</a></h2>
<p>Once we have the fitted model, let’s analyze <strong>residuals</strong>:</p>
<div class="math notranslate nohighlight">
\[
  e_i = y_i - \hat{y}_i.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">residuals</span> <span class="o">=</span> <span class="n">model_ols</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted_vals</span> <span class="o">=</span> <span class="n">model_ols</span><span class="o">.</span><span class="n">fittedvalues</span>
</pre></div>
</div>
</div>
</div>
<p>When evaluating the quality of a regression model, it is crucial to check whether the underlying assumptions hold. Two common residual diagnostic plots are:</p>
<ul class="simple">
<li><p>The residuals vs. fitted plot</p></li>
<li><p>The QQ-plot.</p></li>
</ul>
<p>We present both in details in the following sections.</p>
<section id="residuals-vs-fitted-plot">
<h3><span class="section-number">3.4.1. </span>Residuals vs. Fitted Plot<a class="headerlink" href="#residuals-vs-fitted-plot" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: This plot helps detect <strong>non-linearity</strong> and <strong>heteroscedasticity</strong> (unequal variance of residuals).</p></li>
<li><p><strong>How it works</strong>:</p>
<ul>
<li><p>The <strong>x-axis</strong> represents the fitted (predicted) values from the regression model.</p></li>
<li><p>The <strong>y-axis</strong> represents the residuals (errors).</p></li>
<li><p>Ideally, the residuals should be randomly scattered around zero, forming a <strong>homogeneous “cloud”</strong>.</p></li>
</ul>
</li>
<li><p><strong>Interpretation</strong>:</p>
<ul>
<li><p>If a clear pattern (e.g., a curve) appears, it suggests <strong>non-linearity</strong>, meaning the model might be missing key features or transformations.</p></li>
<li><p>If the spread of residuals <strong>increases or decreases systematically</strong>, it indicates <strong>heteroscedasticity</strong>, meaning the variance of errors is not constant, which violates regression assumptions.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_vals</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted Values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Residuals vs. Fitted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/066f4a984b216a772c93c97a7fb4b24a599360e8c3cf2587787cfa0a868eb154.png" src="_images/066f4a984b216a772c93c97a7fb4b24a599360e8c3cf2587787cfa0a868eb154.png" />
</div>
</div>
</section>
<section id="normal-q-q-plot-quantile-quantile-plot">
<h3><span class="section-number">3.4.2. </span>Normal Q-Q Plot (Quantile-Quantile Plot)<a class="headerlink" href="#normal-q-q-plot-quantile-quantile-plot" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: Checks whether the residuals follow a <strong>normal distribution</strong>, a key assumption in many regression analyses.</p></li>
<li><p><strong>How it works</strong>:</p>
<ul>
<li><p>The <strong>x-axis</strong> shows the theoretical quantiles (expected values under normality).</p></li>
<li><p>The <strong>y-axis</strong> shows the actual quantiles from the residuals.</p></li>
<li><p>If the residuals are normally distributed, they should closely follow a <strong>straight diagonal line</strong>.</p></li>
</ul>
</li>
<li><p><strong>Interpretation</strong>:</p>
<ul>
<li><p><strong>Straight-line alignment</strong>: Residuals are normally distributed.</p></li>
<li><p><strong>Deviations (e.g., S-shape, heavy tails, skewed distribution)</strong>: Suggests non-normal residuals, which might indicate missing variables, skewed data, or influential outliers.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">probplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Normal Q-Q Plot of Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/63e38859061d36547c61d80a29d3af5a427c8897751dc79a921b0c9732ba07d5.png" src="_images/63e38859061d36547c61d80a29d3af5a427c8897751dc79a921b0c9732ba07d5.png" />
</div>
</div>
<section id="comments">
<h4><span class="section-number">3.4.2.1. </span>Comments<a class="headerlink" href="#comments" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Ideally, residuals vs. fitted values have <strong>no clear pattern</strong> and appear <em>cloud-like</em>.</p></li>
<li><p>The Q-Q plot should be fairly linear if they are normally distributed.</p></li>
</ul>
<p>We can also check the standard deviation or run further tests (e.g., Shapiro-Wilk, Breusch-Pagan for heteroscedasticity) if desired.</p>
</section>
</section>
</section>
<section id="input-transformations-polynomial-features">
<h2><span class="section-number">3.5. </span>Input Transformations - Polynomial Features<a class="headerlink" href="#input-transformations-polynomial-features" title="Link to this heading">#</a></h2>
<p>When we talk about <strong>linear regression</strong>, we mean that the model is <strong>linear in parameters</strong>, not necessarily in the input variables <span class="math notranslate nohighlight">\(X\)</span>. That is, the target variable <span class="math notranslate nohighlight">\(y\)</span> is modeled as a <strong>linear combination of parameters</strong>:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 f_1(X) + \beta_2 f_2(X) + \dots + \beta_n f_n(X) + \epsilon
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta_0, \beta_1, \dots, \beta_n \)</span> are the regression coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( f_i(X) \)</span> are <strong>functions of the input features</strong> (which can be non-linear transformations like squares or interaction terms).</p></li>
<li><p><span class="math notranslate nohighlight">\( \epsilon \)</span> is the error term.</p></li>
</ul>
<section id="example-still-a-linear-model">
<h3><span class="section-number">3.5.1. </span>Example: Still a Linear Model<a class="headerlink" href="#example-still-a-linear-model" title="Link to this heading">#</a></h3>
<p>Even if we introduce <strong>non-linear transformations</strong> of <span class="math notranslate nohighlight">\(X\)</span>, as long as the model is <strong>linear in the parameters</strong> <span class="math notranslate nohighlight">\( \beta_i \)</span>, it is still considered a <strong>linear model</strong>. For example:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon
\]</div>
<p>This model includes a quadratic term (<span class="math notranslate nohighlight">\(X^2\)</span>), but is still a <strong>linear regression model</strong> because it remains a linear combination of the parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \beta_2\)</span>.</p>
</section>
<section id="what-would-not-be-a-linear-model">
<h3><span class="section-number">3.5.2. </span>What Would NOT Be a Linear Model?<a class="headerlink" href="#what-would-not-be-a-linear-model" title="Link to this heading">#</a></h3>
<p>A regression model becomes <strong>non-linear in parameters</strong> if the parameters appear in a non-linear way. For example:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + e^{\beta_1 X} + \epsilon
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
y = \frac{1}{\beta_0 + \beta_1 X} + \epsilon
\]</div>
<p>These models are <strong>non-linear in parameters</strong> because the relationship between <span class="math notranslate nohighlight">\(y\)</span> and the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients is no longer a simple linear combination.</p>
</section>
</section>
<section id="why-use-polynomial-features-in-linear-regression">
<h2><span class="section-number">3.6. </span>Why Use Polynomial Features in Linear Regression?<a class="headerlink" href="#why-use-polynomial-features-in-linear-regression" title="Link to this heading">#</a></h2>
<p>Since linear regression allows us to use <strong>non-linear transformations of <span class="math notranslate nohighlight">\(X\)</span></strong> while keeping the model linear in parameters, we can <strong>extend linear regression</strong> to capture more complex relationships.</p>
<section id="polynomial-feature-expansion">
<h3><span class="section-number">3.6.1. </span>Polynomial Feature Expansion<a class="headerlink" href="#polynomial-feature-expansion" title="Link to this heading">#</a></h3>
<p>Given original features <span class="math notranslate nohighlight">\( X_1, X_2, X_3 \)</span>, we can introduce:</p>
<ul class="simple">
<li><p><strong>Squared terms</strong>: <span class="math notranslate nohighlight">\( X_1^2, X_2^2, X_3^2 \)</span> → captures curvature.</p></li>
<li><p><strong>Interaction terms</strong>: <span class="math notranslate nohighlight">\( X_1X_2, X_1X_3, X_2X_3 \)</span> → captures relationships between variables.</p></li>
</ul>
<p>By expanding the features in this way, we allow a linear regression model to <strong>approximate non-linear patterns</strong> in the data, making it more flexible while maintaining its interpretability.</p>
</section>
<section id="example">
<h3><span class="section-number">3.6.2. </span>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>A second-degree polynomial regression model:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1^2 + \beta_5 X_2^2 + \beta_6 X_3^2 + \beta_7 X_1X_2 + \beta_8 X_1X_3 + \beta_9 X_2X_3 + \epsilon
\]</div>
<p>This is still <strong>a linear model in parameters</strong> but now captures non-linear relationships in <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
<section id="how-to-implement-polynomial-features-in-python">
<h3><span class="section-number">3.6.3. </span>How to Implement Polynomial Features in Python<a class="headerlink" href="#how-to-implement-polynomial-features-in-python" title="Link to this heading">#</a></h3>
<p>In Python, we can generate polynomial features using <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.PolynomialFeatures</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span><span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape:&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span><span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polynomial shape:&quot;</span><span class="p">,</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">X_poly_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">X_poly</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original shape: (200, 3)
Polynomial shape: (200, 9)
</pre></div>
</div>
</div>
</div>
<p>The OLS is obtained using the same closed-form formulation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">XTX_poly</span> <span class="o">=</span> <span class="n">X_poly_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_poly_mat</span>
<span class="n">XTX_poly_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XTX_poly</span><span class="p">)</span>
<span class="n">XTy_poly</span> <span class="o">=</span> <span class="n">X_poly_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_vec</span>
<span class="n">beta_hat_poly_manual</span> <span class="o">=</span> <span class="n">XTX_poly_inv</span> <span class="o">@</span> <span class="n">XTy_poly</span>
<span class="n">beta_hat_poly_manual</span> <span class="o">=</span> <span class="n">beta_hat_poly_manual</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manual OLS (polynomial) shape of beta:&quot;</span><span class="p">,</span> <span class="n">beta_hat_poly_manual</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{beta\_hat\_poly\_manual}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_poly_manual</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Manual OLS (polynomial) shape of beta: (10,)
</pre></div>
</div>
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{beta\_hat\_poly\_manual}:\left[
\begin{array}{}
  5.0526 &amp;  1.4773 &amp; -1.9646 &amp;  0.5315 &amp;  0.0036 &amp;  0.0054 &amp;  0.0249 &amp;  0.0037 &amp;  0.0133 &amp; -0.0026
\end{array}
\right]\]</div>
</div>
</div>
<p>We can compare the results offered by <code class="docutils literal notranslate"><span class="pre">statsmodel</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_poly_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_poly</span><span class="p">)</span>
<span class="n">model_poly_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">X_poly_sm</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">beta_hat_poly_sm</span> <span class="o">=</span> <span class="n">model_poly_sm</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span>

<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{Statsmodels polynomial OLS}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_poly_sm</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{Manual polynomial OLS}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_poly_manual</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Polynomial OLS summary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model_poly_sm</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{Statsmodels polynomial OLS}:\left[
\begin{array}{}
  5.0526 &amp;  1.4773 &amp; -1.9646 &amp;  0.5315 &amp;  0.0036 &amp;  0.0054 &amp;  0.0249 &amp;  0.0037 &amp;  0.0133 &amp; -0.0026
\end{array}
\right]\]</div>
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{Manual polynomial OLS}:\left[
\begin{array}{}
  5.0526 &amp;  1.4773 &amp; -1.9646 &amp;  0.5315 &amp;  0.0036 &amp;  0.0054 &amp;  0.0249 &amp;  0.0037 &amp;  0.0133 &amp; -0.0026
\end{array}
\right]\]</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Polynomial OLS summary:
                             OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.976
Model:                            OLS   Adj. R-squared:                  0.975
Method:                 Least Squares   F-statistic:                     872.4
Date:                Wed, 16 Apr 2025   Prob (F-statistic):          2.13e-149
Time:                        11:06:50   Log-Likelihood:                -413.46
No. Observations:                 200   AIC:                             846.9
Df Residuals:                     190   BIC:                             879.9
Df Model:                           9                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.0526      0.287     17.620      0.000       4.487       5.618
x1             1.4773      0.053     27.754      0.000       1.372       1.582
x2            -1.9646      0.024    -82.615      0.000      -2.012      -1.918
x3             0.5315      0.073      7.290      0.000       0.388       0.675
x4             0.0036      0.020      0.179      0.858      -0.036       0.043
x5             0.0054      0.009      0.571      0.569      -0.013       0.024
x6             0.0249      0.028      0.879      0.381      -0.031       0.081
x7             0.0037      0.005      0.803      0.423      -0.005       0.013
x8             0.0133      0.012      1.074      0.284      -0.011       0.038
x9            -0.0026      0.028     -0.095      0.924      -0.058       0.052
==============================================================================
Omnibus:                        0.925   Durbin-Watson:                   2.024
Prob(Omnibus):                  0.630   Jarque-Bera (JB):                1.006
Skew:                          -0.090   Prob(JB):                        0.605
Kurtosis:                       2.703   Cond. No.                         97.1
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<section id="residual-analysis-and-qq-plot">
<h4><span class="section-number">3.6.3.1. </span>Residual analysis and QQ-plot<a class="headerlink" href="#residual-analysis-and-qq-plot" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">residuals_poly</span> <span class="o">=</span> <span class="n">model_poly_sm</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted_poly</span> <span class="o">=</span> <span class="n">model_poly_sm</span><span class="o">.</span><span class="n">fittedvalues</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_poly</span><span class="p">,</span> <span class="n">residuals_poly</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted values (poly)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Residuals vs. Fitted (Polynomial Model)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">probplot</span><span class="p">(</span><span class="n">residuals_poly</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Normal Q-Q Plot (Polynomial Model Residuals)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9c8e9bae0f64916b0bd5d0326bb0ba5e3dbad4e7db63e67823f0e01a919c02f0.png" src="_images/9c8e9bae0f64916b0bd5d0326bb0ba5e3dbad4e7db63e67823f0e01a919c02f0.png" />
<img alt="_images/7b5f300c105616709dd2640d552d3973258d51b0a3c43ba708366e6ec8fc2b0b.png" src="_images/7b5f300c105616709dd2640d552d3973258d51b0a3c43ba708366e6ec8fc2b0b.png" />
</div>
</div>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p>If the polynomial model captures more structure, we may see less pattern in the residuals plot.</p></li>
<li><p>Alternatively, if the true relationship was already linear, polynomial terms might just overfit or remain insignificant.</p></li>
</ul>
</section>
</section>
</section>
<section id="rbf-expansion">
<h2><span class="section-number">3.7. </span>RBF Expansion<a class="headerlink" href="#rbf-expansion" title="Link to this heading">#</a></h2>
<p>Polynomial feature expansion is a powerful tool to introduce <strong>non-linearity</strong> into a linear regression model, but it has several limitations:</p>
<ul class="simple">
<li><p><strong>Feature explosion</strong>: As the polynomial degree increases, the number of new features grows <strong>exponentially</strong>, making models computationally expensive and prone to overfitting.</p></li>
<li><p><strong>Global transformations</strong>: Polynomial terms like <span class="math notranslate nohighlight">\(X^2, X^3\)</span> affect the entire input space, meaning that small changes in <span class="math notranslate nohighlight">\(X\)</span> have a large impact everywhere, which might not always be desirable.</p></li>
<li><p><strong>Poor handling of local variations</strong>: Polynomial functions are not ideal when the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span> has local variations or is non-smooth.</p></li>
</ul>
<p>This is where <strong>Radial Basis Function (RBF) expansion</strong> can be useful.</p>
<section id="what-is-rbf-expansion">
<h3><span class="section-number">3.7.1. </span>What is RBF Expansion?<a class="headerlink" href="#what-is-rbf-expansion" title="Link to this heading">#</a></h3>
<p>RBF expansion <strong>maps the original features into a higher-dimensional space</strong> using functions that respond <strong>locally</strong> to the input values.</p>
<p>Instead of using polynomial transformations like:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1^2 + \beta_5 X_2^2 + \beta_6 X_3^2 + \beta_7 X_1X_2 + ...
\]</div>
<p>We use <strong>random Fourier features</strong> to approximate an RBF kernel, generating transformed variables like:</p>
<div class="math notranslate nohighlight">
\[
\Phi(X) = \cos(WX) + \sin(WX)
\]</div>
<p>where <span class="math notranslate nohighlight">\( W \)</span> is a random weight matrix sampled from a Gaussian distribution.</p>
<section id="why-use-rbf-instead-of-polynomial-expansion">
<h4><span class="section-number">3.7.1.1. </span>Why Use RBF Instead of Polynomial Expansion?<a class="headerlink" href="#why-use-rbf-instead-of-polynomial-expansion" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Better for local variations</strong>: Unlike polynomials, RBFs can capture <strong>localized</strong> structures in data.</p></li>
<li><p><strong>Fixed feature size</strong>: The number of transformed features is independent of the input size, making it computationally efficient.</p></li>
<li><p><strong>Good for high-dimensional data</strong>: Works well when the input space is <strong>high-dimensional</strong> because it avoids feature explosion.</p></li>
</ul>
</section>
</section>
<section id="implementing-rbf-expansion-with-random-fourier-features">
<h3><span class="section-number">3.7.2. </span>Implementing RBF Expansion with Random Fourier Features<a class="headerlink" href="#implementing-rbf-expansion-with-random-fourier-features" title="Link to this heading">#</a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">sklearn.kernel_approximation.RBFSampler</span></code> to generate <strong>random Fourier features</strong> that approximate the RBF kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.kernel_approximation</span><span class="w"> </span><span class="kn">import</span> <span class="n">RBFSampler</span>

<span class="n">rbf</span> <span class="o">=</span> <span class="n">RBFSampler</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_rbf</span> <span class="o">=</span> <span class="n">rbf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span><span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RBF-transformed shape:&quot;</span><span class="p">,</span> <span class="n">X_rbf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># We&#39;ll do OLS with statsmodels for brevity:</span>
<span class="n">X_rbf_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_rbf</span><span class="p">)</span>
<span class="n">model_rbf_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">X_rbf_sm</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model_rbf_sm</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="n">resid_rbf</span> <span class="o">=</span> <span class="n">model_rbf_sm</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted_rbf</span> <span class="o">=</span> <span class="n">model_rbf_sm</span><span class="o">.</span><span class="n">fittedvalues</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_rbf</span><span class="p">,</span> <span class="n">resid_rbf</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted values (RBF)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Residuals vs. Fitted (RBF)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RBF-transformed shape: (200, 50)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.980
Model:                            OLS   Adj. R-squared:                  0.973
Method:                 Least Squares   F-statistic:                     146.9
Date:                Wed, 16 Apr 2025   Prob (F-statistic):          5.70e-105
Time:                        11:06:51   Log-Likelihood:                -396.22
No. Observations:                 200   AIC:                             894.4
Df Residuals:                     149   BIC:                             1063.
Df Model:                          50                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        -43.4898     19.729     -2.204      0.029     -82.475      -4.505
x1             7.1364     11.512      0.620      0.536     -15.612      29.885
x2             8.9773      3.334      2.692      0.008       2.388      15.566
x3             2.6549      7.719      0.344      0.731     -12.598      17.908
x4             1.1046      1.872      0.590      0.556      -2.595       4.804
x5           137.0639     46.449      2.951      0.004      45.280     228.848
x6            16.5251     11.039      1.497      0.137      -5.288      38.338
x7            -3.4503      4.473     -0.771      0.442     -12.288       5.388
x8             2.9028      3.919      0.741      0.460      -4.842      10.648
x9            -0.5036      6.801     -0.074      0.941     -13.942      12.934
x10           16.0558     15.230      1.054      0.294     -14.040      46.151
x11           24.7962     12.423      1.996      0.048       0.249      49.344
x12           59.4056      7.430      7.995      0.000      44.723      74.088
x13           -6.8051     11.750     -0.579      0.563     -30.024      16.414
x14           12.4240     24.130      0.515      0.607     -35.258      60.106
x15          -19.3450      7.617     -2.540      0.012     -34.397      -4.293
x16         -266.9437     65.123     -4.099      0.000    -395.627    -138.260
x17            5.0042      4.000      1.251      0.213      -2.900      12.908
x18           11.1227      5.828      1.909      0.058      -0.393      22.639
x19           24.4491     13.325      1.835      0.069      -1.882      50.780
x20           -6.4866      3.810     -1.702      0.091     -14.016       1.042
x21         -136.7679     40.658     -3.364      0.001    -217.108     -56.428
x22           -0.5810      7.714     -0.075      0.940     -15.825      14.663
x23           29.9036     16.223      1.843      0.067      -2.153      61.960
x24           43.4672     31.745      1.369      0.173     -19.261     106.195
x25           -2.5586      1.587     -1.612      0.109      -5.695       0.578
x26           24.3295     50.242      0.484      0.629     -74.950     123.609
x27           24.6458     14.259      1.728      0.086      -3.530      52.821
x28           -9.9883     14.837     -0.673      0.502     -39.306      19.329
x29           98.4893     31.798      3.097      0.002      35.657     161.322
x30           -5.2150     10.521     -0.496      0.621     -26.004      15.574
x31           -2.7180     16.134     -0.168      0.866     -34.600      29.164
x32            2.4668      1.976      1.248      0.214      -1.438       6.372
x33            2.5672      2.217      1.158      0.249      -1.814       6.948
x34          200.9372     54.077      3.716      0.000      94.081     307.793
x35           42.1785     14.095      2.992      0.003      14.327      70.031
x36          -24.5715    128.680     -0.191      0.849    -278.845     229.702
x37          156.2145     31.866      4.902      0.000      93.246     219.183
x38           -7.9412     16.226     -0.489      0.625     -40.004      24.122
x39           -1.1294      1.594     -0.709      0.480      -4.279       2.020
x40         -225.6936     88.747     -2.543      0.012    -401.059     -50.328
x41          -12.5555     36.848     -0.341      0.734     -85.367      60.256
x42          -12.9924      9.099     -1.428      0.155     -30.972       4.987
x43           25.7314     13.007      1.978      0.050       0.029      51.433
x44            7.2633     15.228      0.477      0.634     -22.827      37.353
x45          -70.8756     20.960     -3.381      0.001    -112.293     -29.458
x46          -46.6215    172.329     -0.271      0.787    -387.147     293.904
x47            0.5652      3.121      0.181      0.857      -5.602       6.732
x48           -6.3771     10.449     -0.610      0.543     -27.025      14.271
x49          153.3197     46.325      3.310      0.001      61.781     244.858
x50            9.1846      5.433      1.691      0.093      -1.551      19.920
==============================================================================
Omnibus:                        0.557   Durbin-Watson:                   2.049
Prob(Omnibus):                  0.757   Jarque-Bera (JB):                0.682
Skew:                          -0.041   Prob(JB):                        0.711
Kurtosis:                       2.726   Cond. No.                     1.75e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.75e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
<img alt="_images/4cfdcf2dfc685739f7f0b79f50c2d5690de3a55ca31ecf58cb9042b9c13157f9.png" src="_images/4cfdcf2dfc685739f7f0b79f50c2d5690de3a55ca31ecf58cb9042b9c13157f9.png" />
</div>
</div>
<section id="id1">
<h4><span class="section-number">3.7.2.1. </span>Comments<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul>
<li><p>Each RBF feature is basically a sinusoidal basis function in random projection space.</p></li>
<li><p>We could also attempt a manual approach:</p>
<div class="math notranslate nohighlight">
\[
    \hat{\beta} = (X_{\mathrm{rbf}}^\top X_{\mathrm{rbf}})^{-1} X_{\mathrm{rbf}}^\top y
  \]</div>
<p>if we want to replicate the normal equation style.</p>
</li>
<li><p>The dimensionality (50) can be large, so be mindful of potential numerical issues and overfitting.</p></li>
</ul>
</section>
</section>
</section>
<section id="ridge-regression">
<h2><span class="section-number">3.8. </span>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>For complex expansions (like polynomial of high degree or many RBF components), <strong>overfitting</strong> can happen.
Ridge regression adds an L2 penalty:</p>
<div class="math notranslate nohighlight">
\[
  \min_{\beta} \;\; \| Y - X \beta \|^2 + \alpha \| \beta \|^2.
\]</div>
<section id="ridge-closed-form">
<h3><span class="section-number">3.8.1. </span>Ridge Closed-form<a class="headerlink" href="#ridge-closed-form" title="Link to this heading">#</a></h3>
<p>Ridge also has a known closed-form:</p>
<div class="math notranslate nohighlight">
\[
  \hat{\beta}_{\mathrm{ridge}} = (X^\top X + \alpha I)^{-1} X^\top Y.
\]</div>
<p>We’ll show a manual solution, then compare to scikit-learn. We’ll do it on the <strong>polynomial features</strong> for illustration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_ridge</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">I_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X_poly_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># dimension p x p, where p = number of poly features + 1 (intercept)</span>

<span class="c1"># Usually we do not regularize the intercept, so we might set I_p[0,0] = 0.</span>
<span class="c1"># Let&#39;s do that to avoid shrinking the intercept:</span>
<span class="n">I_p</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="n">XTX_ridge</span> <span class="o">=</span> <span class="n">X_poly_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_poly_mat</span> <span class="o">+</span> <span class="n">alpha_ridge</span><span class="o">*</span><span class="n">I_p</span>
<span class="n">XTX_ridge_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XTX_ridge</span><span class="p">)</span>
<span class="n">XTy_ridge</span> <span class="o">=</span> <span class="n">X_poly_mat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_vec</span>
<span class="n">beta_hat_ridge_manual</span> <span class="o">=</span> <span class="n">XTX_ridge_inv</span> <span class="o">@</span> <span class="n">XTy_ridge</span>
<span class="n">beta_hat_ridge_manual</span> <span class="o">=</span> <span class="n">beta_hat_ridge_manual</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manual Ridge (polynomial) shape:&quot;</span><span class="p">,</span> <span class="n">beta_hat_ridge_manual</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{Manual Ridge Coeffs}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_ridge_manual</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Manual Ridge (polynomial) shape: (10,)
</pre></div>
</div>
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{Manual Ridge Coeffs}:\left[
\begin{array}{}
  5.0547 &amp;  1.4667 &amp; -1.9617 &amp;  0.5238 &amp;  0.0037 &amp;  0.0051 &amp;  0.0242 &amp;  0.0037 &amp;  0.0133 &amp; -0.0038
\end{array}
\right]\]</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge_sklearn</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha_ridge</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
<span class="c1"># &quot;fit_intercept=False&quot; because we already included the intercept column in X_poly_mat</span>
<span class="c1"># We will manually set the intercept to match the first column or we can remove intercept from X_poly_mat.</span>

<span class="n">ridge_sklearn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly_mat</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="n">beta_hat_ridge_sklearn</span> <span class="o">=</span> <span class="n">ridge_sklearn</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{scikit-learn Ridge Coeffs}:&quot;</span><span class="p">,</span> <span class="n">beta_hat_ridge_sklearn</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text{Difference (manual - sklearn)}:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">beta_hat_ridge_manual</span> <span class="o">-</span> <span class="n">beta_hat_ridge_sklearn</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{scikit-learn Ridge Coeffs}:\left[
\begin{array}{}
  4.1657 &amp;  1.4733 &amp; -1.9611 &amp;  0.5235 &amp;  0.0387 &amp;  0.0073 &amp;  0.0330 &amp;  0.0122 &amp;  0.0117 &amp;  0.0322
\end{array}
\right]\]</div>
<div class="output text_latex math notranslate nohighlight">
\[\displaystyle \text{Difference (manual - sklearn)}:\left[
\begin{array}{}
  0.8890 &amp; -0.0067 &amp; -0.0006 &amp;  0.0003 &amp; -0.0350 &amp; -0.0022 &amp; -0.0088 &amp; -0.0085 &amp;  0.0016 &amp; -0.0360
\end{array}
\right]\]</div>
</div>
</div>
<p>They should be nearly identical (tiny floating discrepancies may occur).</p>
</section>
<section id="residual-analysis-for-ridge">
<h3><span class="section-number">3.8.2. </span>Residual analysis for Ridge<a class="headerlink" href="#residual-analysis-for-ridge" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">X_poly_mat</span> <span class="o">@</span> <span class="n">beta_hat_ridge_manual</span>
<span class="n">resid_ridge</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_pred_ridge</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred_ridge</span><span class="p">,</span> <span class="n">resid_ridge</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted values (Ridge, polynomial)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Residuals vs. Fitted (Ridge)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ca2df777726d0e77669466f3360cc3192b38d8d60d06f924c16bb74b29eb126b.png" src="_images/ca2df777726d0e77669466f3360cc3192b38d8d60d06f924c16bb74b29eb126b.png" />
</div>
</div>
<p><strong>Remarks</strong>:</p>
<ul class="simple">
<li><p>As alpha_ridge increases, the polynomial coefficients shrink more, typically reducing variance (but possibly increasing bias).</p></li>
<li><p>We can further use cross-validation to select alpha automatically or do a hyperparameter grid.</p></li>
</ul>
</section>
</section>
<section id="conclusion">
<h2><span class="section-number">3.9. </span>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This chapter has provided a comprehensive, hands-on exploration of linear models—starting from the manual derivation of the Ordinary Least Squares (OLS) estimator to the application of regularization techniques and feature transformations. Through synthetic data, we validated the accuracy of our estimators, examined residuals for assumption-checking, and used diagnostic plots to evaluate model fit.</p>
<p>We have seen how linear regression, while conceptually straightforward, remains a powerful modeling tool when used thoughtfully. The incorporation of polynomial and radial basis function expansions extends the expressive capacity of linear models, enabling them to approximate complex, non-linear relationships without abandoning interpretability.</p>
<p>We also introduced Ridge regression, which provides a principled approach to combat overfitting and multicollinearity, especially in high-dimensional settings. This regularized extension highlights a key transition in statistical learning: balancing model flexibility with generalization performance.</p>
<p>Ultimately, this chapter emphasizes not only how to implement linear models, but how to critically assess their assumptions and limitations. These foundational skills will be essential as we move on to more sophisticated models in the chapters ahead.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Introduction to Probabilistic Methods and Monte Carlo Simulations</p>
      </div>
    </a>
    <a class="right-next"
       href="03.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Data Preprocessing, Local Models and Tree-Based Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-synthetic-multivariate-data">3.2. Generating Synthetic Multivariate Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">3.2.1. Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">3.2.1.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-data-inspection">3.2.2. Quick Data Inspection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols-manual-computation-using-matrix-algebra">3.3. Ordinary Least Squares (OLS) – Manual Computation Using Matrix Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-statsmodels">3.3.1. Comparison with statsmodels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observations">3.3.2. Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-and-tests">3.4. Residual Analysis and Tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residuals-vs-fitted-plot">3.4.1. Residuals vs. Fitted Plot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-q-q-plot-quantile-quantile-plot">3.4.2. Normal Q-Q Plot (Quantile-Quantile Plot)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">3.4.2.1. Comments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-transformations-polynomial-features">3.5. Input Transformations - Polynomial Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-still-a-linear-model">3.5.1. Example: Still a Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-would-not-be-a-linear-model">3.5.2. What Would NOT Be a Linear Model?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-polynomial-features-in-linear-regression">3.6. Why Use Polynomial Features in Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-feature-expansion">3.6.1. Polynomial Feature Expansion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">3.6.2. Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-implement-polynomial-features-in-python">3.6.3. How to Implement Polynomial Features in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-and-qq-plot">3.6.3.1. Residual analysis and QQ-plot</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rbf-expansion">3.7. RBF Expansion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-rbf-expansion">3.7.1. What is RBF Expansion?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-rbf-instead-of-polynomial-expansion">3.7.1.1. Why Use RBF Instead of Polynomial Expansion?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-rbf-expansion-with-random-fourier-features">3.7.2. Implementing RBF Expansion with Random Fourier Features</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.7.2.1. Comments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">3.8. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-closed-form">3.8.1. Ridge Closed-form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-for-ridge">3.8.2. Residual analysis for Ridge</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">3.9. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tribel Pascal, Simar Cédric, Bontempi Gianluca
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  This is the practicals handbook for the course INFO-F422 - Statistical Foundations of Machine Learning. This is intended to be used alongside the <a href='https://www.researchgate.net/publication/242692234_Statistical_foundations_of_machine_learning_the_handbook'> theoretical handbook</a>.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>