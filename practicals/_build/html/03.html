
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Data Preprocessing, Local Models and Tree-Based Models &#8212; Statistical Foundations of Machine Learning - Practicals handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="application/vnd.jupyter.widget-state+json">{"state": {"188ba3f4cd724b88a486f35adb7202bc": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dbdca1a4f4bd4b67b851da47b3835a93": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "d04ba29681344eccb70310e4e492d2a1": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_188ba3f4cd724b88a486f35adb7202bc", "max": 25.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_dbdca1a4f4bd4b67b851da47b3835a93", "tabbable": null, "tooltip": null, "value": 25.0}}, "3a9217e71e9842e2a6068da4b80b580a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f75ea247dbce4f958ba11bd492396585": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "afe4bb6ca5ee4c8d8a9dd2ead54bbdf5": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_3a9217e71e9842e2a6068da4b80b580a", "placeholder": "\u200b", "style": "IPY_MODEL_f75ea247dbce4f958ba11bd492396585", "tabbable": null, "tooltip": null, "value": "100%"}}, "9c2b5da2d28e4294ae934d6ee7a606a8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c3eaa817a3414603a48ef726b8d78e11": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "449cb05ac45d4a658a28f51715d34a6e": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_9c2b5da2d28e4294ae934d6ee7a606a8", "placeholder": "\u200b", "style": "IPY_MODEL_c3eaa817a3414603a48ef726b8d78e11", "tabbable": null, "tooltip": null, "value": "\u200725/25\u2007[00:20&lt;00:00,\u2007\u20071.23it/s]"}}, "1c20c67f489e426a94c8aa508893beaa": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "aac0281b24cd4924871a4e17d802c283": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_afe4bb6ca5ee4c8d8a9dd2ead54bbdf5", "IPY_MODEL_d04ba29681344eccb70310e4e492d2a1", "IPY_MODEL_449cb05ac45d4a658a28f51715d34a6e"], "layout": "IPY_MODEL_1c20c67f489e426a94c8aa508893beaa", "tabbable": null, "tooltip": null}}, "e875470df55c4b4f81210e34268dec02": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "305b246b5d604f5490b7736b24428fec": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "ccf34127b41c46ac84df5592e1908900": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_e875470df55c4b4f81210e34268dec02", "max": 39.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_305b246b5d604f5490b7736b24428fec", "tabbable": null, "tooltip": null, "value": 39.0}}, "8004216bbbba431d9f455e65ecc8e42d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fad0545338c34388a8d5d6cea2059bc6": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "06452cac71aa46b28cb2f70a315dbf0f": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_8004216bbbba431d9f455e65ecc8e42d", "placeholder": "\u200b", "style": "IPY_MODEL_fad0545338c34388a8d5d6cea2059bc6", "tabbable": null, "tooltip": null, "value": "100%"}}, "7755f3ef747348618f3e2678667ae420": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3f322212201344d2877284e2bee7197c": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "2407694e26d1467d8b5a4d7aed8f2619": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_7755f3ef747348618f3e2678667ae420", "placeholder": "\u200b", "style": "IPY_MODEL_3f322212201344d2877284e2bee7197c", "tabbable": null, "tooltip": null, "value": "\u200739/39\u2007[01:41&lt;00:00,\u2007\u20076.27s/it]"}}, "3357b527b4b541ec83889d2c8b0db398": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3454282e5f5344f6965db15bf0a3f175": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_06452cac71aa46b28cb2f70a315dbf0f", "IPY_MODEL_ccf34127b41c46ac84df5592e1908900", "IPY_MODEL_2407694e26d1467d8b5a4d7aed8f2619"], "layout": "IPY_MODEL_3357b527b4b541ec83889d2c8b0db398", "tabbable": null, "tooltip": null}}}, "version_major": 2, "version_minor": 0}</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '03';</script>
    <link rel="icon" href="_static/sfml.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Neural Networks for Regression" href="04.html" />
    <link rel="prev" title="3. Linear Models" href="02.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontpage.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/sfml.png" class="logo__image only-light" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>
    <script>document.write(`<img src="_static/sfml.png" class="logo__image only-dark" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontpage.html">
                    Home
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="01.html">2. Introduction to Probabilistic Methods and Monte Carlo Simulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.html">3. Linear Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Data Preprocessing, Local Models and Tree-Based Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.html">5. Neural Networks for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.html">6. Ensembles of models and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="06.html">7. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion.html">8. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">9. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/03.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data Preprocessing, Local Models and Tree-Based Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-precision-about-the-evaluation-metric-used-in-this-practical">4.1.1. Important precision about the evaluation metric used in this practical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-overview">4.1.2. Data preprocessing overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">4.2. K-Nearest Neighbors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">4.3. Regression Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">4.4. Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-reduce-variance">4.4.1. Why does this reduce variance?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">4.4.2. Main Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importances">4.5. Feature Importances</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">4.6. <code class="docutils literal notranslate"><span class="pre">Scikit-learn</span></code> Pipelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">4.7. Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#california-housing">4.7.1. California Housing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">4.7.2. Solution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-exploration">4.7.2.1. Dataset and exploration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values">4.7.2.2. Handling Missing Values</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#an-advanced-method-knnimputer">An advanced method: <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-categorical-values">4.7.2.3. Dealing with Categorical Values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-scaling">4.7.2.4. Normalization/Scaling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-knn-implementation">4.7.3. Vanilla KNN Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4.7.3.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees-and-random-forests">4.7.4. Regression Trees and Random Forests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4.7.5. Solution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-with-scikit-learn">4.7.5.1. Implementation with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-key-parameters">Exploring Key Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-max-depth">Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code></a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-n-estimators">Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-the-best-models">4.7.5.2. Feature importance with the best models</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-pipeline">4.7.6. Building a Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">4.7.6.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">4.7.7. Additional exercise: bias–variance trade-off in regression with decision trees</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">4.7.7.1. Solution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-preprocessing-local-models-and-tree-based-models">
<h1><span class="section-number">4. </span>Data Preprocessing, Local Models and Tree-Based Models<a class="headerlink" href="#data-preprocessing-local-models-and-tree-based-models" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">4.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Machine Learning, as you now know, is based on statistical analysis of data. Data comes in many kinds of formats, resolutions, annotations, … and therefore should be processed before performing analysis. This is the first content of this practical. We will focus on regression tasks, as already defined in the previous practicals, showing to commonly used strategies. The practical is structured around five key objectives:</p>
<ol class="arabic simple">
<li><p>Understanding and applying essential data preprocessing techniques<br />
Before training a machine learning model, the data must be properly prepared. We will cover the following common preprocessing steps:</p>
<ul class="simple">
<li><p>Handling missing values: strategies to impute or remove data with missing entries.</p></li>
<li><p>Feature scaling and normalization: ensuring features are on comparable scales to improve model performance.</p></li>
<li><p>Encoding categorical variables: transforming non-numeric data (e.g., categories) into numerical representations suitable for machine learning algorithms.</p></li>
</ul>
</li>
<li><p>Implementing the K-Nearest Neighbors (KNN) algorithm from scratch<br />
We will write a simple, “vanilla” version of the KNN algorithm for regression tasks. This implementation will help demystify how KNN works by reinforcing the idea of prediction based on similarity in feature space.</p></li>
<li><p>Training and evaluating tree-based regression models<br />
We will work with two popular types of decision-tree-based models for regression:</p>
<ul class="simple">
<li><p>Regression trees: models that recursively split the data space into regions to predict a continuous output.</p></li>
<li><p>Random Forests: an ensemble method that combines multiple decision trees to improve predictive accuracy and reduce overfitting.</p></li>
</ul>
</li>
<li><p>Interpreting feature importances in tree-based models<br />
One advantage of tree-based models is their ability to estimate the relative importance of each input feature. We will learn how to extract and interpret these feature importances to gain insights into which variables are most influential in the prediction.</p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Pipelines to streamline preprocessing and modeling<br />
To ensure clean and reproducible workflows, we will introduce <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> pipelines, which allow us to:</p>
<ul class="simple">
<li><p>Chain together preprocessing steps and model training into a single object.</p></li>
<li><p>Simplify cross-validation and hyperparameter tuning.</p></li>
<li><p>Prevent data leakage by ensuring that preprocessing is properly applied to both training and test data.</p></li>
</ul>
</li>
</ol>
<p>By the end of this practical, you should be able to preprocess real-world data appropriately, implement a basic regression model, use tree-based models effectively, and combine preprocessing and modeling steps into a coherent, automated pipeline using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<section id="important-precision-about-the-evaluation-metric-used-in-this-practical">
<h3><span class="section-number">4.1.1. </span>Important precision about the evaluation metric used in this practical<a class="headerlink" href="#important-precision-about-the-evaluation-metric-used-in-this-practical" title="Link to this heading">#</a></h3>
<p>In this practical, we will use the <em>Mean Squared Error</em> (MSE) as a convenient shorthand for the estimated <em>Mean Integrated Squared Error</em> <span class="math notranslate nohighlight">\((\widehat{\mathrm{MISE}})\)</span>. Indeed, in practice, we rarely know the true MISE and instead rely on estimates obtained via techniques like <span class="math notranslate nohighlight">\(K\)</span>-fold cross-validation or leave-one-out (LOO) cross-validation. So keep in mind that when we refer to the MSE in these contexts, what we are really computing is an estimation of <span class="math notranslate nohighlight">\(\widehat{\mathrm{MISE}}\)</span> from our data.</p>
</section>
<section id="data-preprocessing-overview">
<h3><span class="section-number">4.1.2. </span>Data preprocessing overview<a class="headerlink" href="#data-preprocessing-overview" title="Link to this heading">#</a></h3>
<p>Data preprocessing is the foundation of any successful machine learning project. Many Machine Learning models see their performances degraded when trained on poorly preprocessed data. Key steps include:</p>
<ul class="simple">
<li><p>Handling Missing Values: Missing data can bias results if not addressed. This problem affects many models—such as linear models, support vector machines (SVMs), and neural networks—but it is particularly critical for distance-based algorithms like KNN where missing feature values can disrupt the calculation of distances, leading to inaccurate outcomes. Common strategies for handling missing values include:</p>
<ul>
<li><p><em>Dropping</em> rows/columns with missing data.</p></li>
<li><p><em>Imputing</em> missing data using simple statistics (mean, median) or more sophisticated methods like <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code>.<br><br></p></li>
</ul>
</li>
<li><p>Normalization / Scaling: Many algorithms assume features are on similar scales. Without proper scaling, features with larger numerical ranges may dominate those with smaller ranges, potentially biasing the model’s results. Common scaling strategies include:</p>
<ul>
<li><p><em>Standardization</em>: Transform features to have zero mean and unit variance.</p></li>
<li><p><em>Min-Max Scaling</em>: Scale features to a fixed <span class="math notranslate nohighlight">\([0,1]\)</span> range.<br><br></p></li>
</ul>
</li>
<li><p>Dealing with Categorical Variables: Categorical variables must be converted to a numeric representation that preserves meaningful differences. Common strategies for dealing with categorical variables include:</p>
<ul>
<li><p><em>Label Encoding</em>: Assign an integer to each category (useful if there is an ordinal relationship).</p></li>
<li><p><em>One-Hot Encoding (Dummy Encoding)</em>: Create a new binary column for each category (common for nominal features). This approach can cause a significant increase in input dimensionality (see the curse of dimensionality).</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="k-nearest-neighbors">
<h2><span class="section-number">4.2. </span>K-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Link to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>💡Did you know ? The <span class="math notranslate nohighlight">\(K\)</span>-Neighrest Neighbors algorithm was first proposed in <span id="id1">Fix and Hodges [<a class="reference internal" href="bibliography.html#id2" title="E. Fix and J.L. Hodges. Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. USAF School of Aviation Medicine, 1951. URL: https://books.google.be/books?id=4XwytAEACAAJ.">FH51</a>]</span>.</p>
</aside>
<p>The <em><span class="math notranslate nohighlight">\(K\)</span>-Nearest Neighbors</em> (KNN) algorithm is a local modeling approach that infers its prediction from the data points closest (in some metric) to a query. Suppose we have a training set of inputs <span class="math notranslate nohighlight">\(\{x_i\}\)</span> with associated labels or targets <span class="math notranslate nohighlight">\(\{y_i\}\)</span>. For a new query point <span class="math notranslate nohighlight">\(q\)</span>:</p>
<ol class="arabic simple">
<li><p>Distance computation: Compute the distance (e.g., Euclidean) between <span class="math notranslate nohighlight">\(q\)</span> and each training example <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
<li><p>Ranking: Sort or rank the training points by increasing distance to <span class="math notranslate nohighlight">\(q\)</span>.</p></li>
<li><p>Neighborhood selection: Identify the subset of the <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors <span class="math notranslate nohighlight">\(\{x_{[1]}, \dots, x_{[K]}\}\)</span> where <span class="math notranslate nohighlight">\(y_{[i]}\)</span> denotes the label of neighbor <span class="math notranslate nohighlight">\(x_{[i]}\)</span>.</p></li>
<li><p>Regression or classification: KNN can be viewed as a local estimator of the conditional expectation <span class="math notranslate nohighlight">\(\mathbb{E}[y|x=q]\)</span>.</p></li>
</ol>
<p>For regression tasks, the simplest approach is to average the targets of the <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}(q) = \frac{1}{K}\sum_{i=1}^{K} y_{[i]}.
\]</div>
<p>Alternatively, a local linear model may be used:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}(q) = \hat{a}^T q + \hat{b},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{a}\)</span> and <span class="math notranslate nohighlight">\(\hat{b}\)</span> are estimated (locally) by least-squares on the neighbors.</p>
<p>For classification, one can estimate the conditional probability of belonging to a specific class (e.g., class “1”) by the proportion of neighbors labeled “1”:</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_1(q) = \frac{1}{K}\sum_{i=1}^{K} y_{[i]}.
\]</div>
<p>A threshold (e.g., 0.5) can then be used to decide the class label.</p>
<p>The key hyperparameter here is <span class="math notranslate nohighlight">\(K\)</span>. A <em>smaller</em> <span class="math notranslate nohighlight">\(K\)</span> reduces bias but increases variance, while a <em>larger</em> <span class="math notranslate nohighlight">\(K\)</span> produces a smoother model at the risk of higher bias. KNN’s performance also heavily depends on the distance metric, especially when features have different scales or include categorical variables (which may require specialized encodings or distance definitions).</p>
</section>
<section id="regression-trees">
<h2><span class="section-number">4.3. </span>Regression Trees<a class="headerlink" href="#regression-trees" title="Link to this heading">#</a></h2>
<p>Regression trees rely on a tree-based structure of <em>internal nodes</em> (where decisions are made) and <em>terminal nodes</em> (leaves), which partition the input space into mutually exclusive regions, each associated with a simple (often constant) local model. Its construction begins with a tree growing phase: starting from the root node that contains all data, we recursively choose a split that best reduces the empirical error. Specifically, for a node <span class="math notranslate nohighlight">\(t\)</span> containing <span class="math notranslate nohighlight">\(N(t)\)</span> samples, we define</p>
<div class="math notranslate nohighlight">
\[
R_{\mathrm{emp}}(t) \;=\; \min_{\alpha_t} \sum_{i=1}^{N(t)} L\bigl(y_i,\;h_t(x_i,\;\alpha_t)\bigr),
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is typically the squared error <span class="math notranslate nohighlight">\((y - \hat{y})^2\)</span> for regression. Given a potential split <span class="math notranslate nohighlight">\(s\)</span> dividing <span class="math notranslate nohighlight">\(t\)</span> into children <span class="math notranslate nohighlight">\(t_l\)</span> and <span class="math notranslate nohighlight">\(t_r\)</span>, we consider the decrease in empirical error:</p>
<div class="math notranslate nohighlight">
\[
\Delta E(s,\;t) \;=\; R_{\mathrm{emp}}(t)\;-\;\bigl(R_{\mathrm{emp}}(t_l)\;+\;R_{\mathrm{emp}}(t_r)\bigr).
\]</div>
<p>We choose the split <span class="math notranslate nohighlight">\(s^*\)</span> maximizing <span class="math notranslate nohighlight">\(\Delta E\)</span>, partition the dataset accordingly, and repeat the procedure recursively until no further improvement is found or a stopping criterion is met. This exhaustive splitting often yields a very large tree that overfits the data.</p>
<p>To address overfitting, a cost-complexity pruning procedure is commonly used. Introducing a complexity parameter <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span> that penalizes the number of leaf nodes, we define the cost-complexity measure:</p>
<div class="math notranslate nohighlight">
\[
R_{\lambda}(T) = R_{\mathrm{emp}}(T) + \lambda \, |T|,
\]</div>
<p>where <span class="math notranslate nohighlight">\(|T|\)</span> is the number of terminal nodes (leaves) in tree <span class="math notranslate nohighlight">\(T\)</span>.</p>
<ul class="simple">
<li><p>By gradually increasing <span class="math notranslate nohighlight">\(\lambda\)</span>, we obtain a <em>sequence</em> of subtrees:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
T_{\max} \supset T_{L-1} \supset \dots \supset T_2 \supset T_1.
\]</div>
<p>Each has fewer leaves. We consider all admissible subtrees <span class="math notranslate nohighlight">\(T_t \subset T\)</span> of the large tree and compute the smallest</p>
<div class="math notranslate nohighlight">
\[
\lambda_t = \frac{R_{\mathrm{emp}}(T) - R_{\mathrm{emp}}(T_t)}{|T| - |T_t|}
\]</div>
<p>that yields a lower cost. We select the subtree that minimizes <span class="math notranslate nohighlight">\(R_{\lambda}(T)\)</span> (the best balances between empirical error and model complexity).
The final tree structure is typically chosen via cross-validation or held-out validation to find the best <span class="math notranslate nohighlight">\(\lambda\)</span>.
A more visual representation of tree-based models, an animated version of decision trees (classification) can be found <a class="reference external" href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">here</a>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Random Forest have been first presented in <span id="id2">Ho [<a class="reference internal" href="bibliography.html#id3" title="Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(8):832-844, 1998. doi:10.1109/34.709601.">Ho98</a>]</span>.</p>
</aside>
</section>
<section id="random-forests">
<h2><span class="section-number">4.4. </span>Random Forests<a class="headerlink" href="#random-forests" title="Link to this heading">#</a></h2>
<p>A Random Forest (RF), is an ensemble learning technique designed to reduce the variance of decision trees by combining two main ideas:</p>
<ol class="arabic simple">
<li><p>Bootstrap Sampling (Bagging): We generate <span class="math notranslate nohighlight">\(B\)</span> <em>bootstrap</em> datasets, each by sampling (with replacement) from the original training data.</p></li>
<li><p>Random Feature Selection (Feature Bagging): At each split, a <em>random subset</em> of <span class="math notranslate nohighlight">\(n' &lt; n\)</span> features is chosen, and the best split is found <em>only among those <span class="math notranslate nohighlight">\(n'\)</span> features</em>.</p></li>
</ol>
<p>Hence, each tree <span class="math notranslate nohighlight">\(h_b(\mathbf{x}, \alpha_b)\)</span> is built from a <em>different</em> resampled dataset and uses only a random subset of features at each split. As a result, the trees are more decorrelated and often are <em>not pruned</em> heavily.</p>
<p>Formally, for a regression task:</p>
<div class="math notranslate nohighlight">
\[
  h_{\mathrm{rf}}(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} h_b(\mathbf{x}, \alpha_b),
\]</div>
<p>i.e., the average of all tree predictions. (In classification, we take the majority vote.)</p>
<section id="why-does-this-reduce-variance">
<h3><span class="section-number">4.4.1. </span>Why does this reduce variance?<a class="headerlink" href="#why-does-this-reduce-variance" title="Link to this heading">#</a></h3>
<p>Suppose each tree <span class="math notranslate nohighlight">\(h_b\)</span> has variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> and they have pairwise correlation <span class="math notranslate nohighlight">\(\rho\)</span>. The variance of the RF predictor <span class="math notranslate nohighlight">\(h_{\mathrm{rf}}\)</span> can be approximated as:</p>
<div class="math notranslate nohighlight">
\[
  \mathrm{Var}[h_{\mathrm{rf}}] \approx \frac{(1 - \rho)\,\sigma^2}{B} + \rho\,\sigma^2,
\]</div>
<p>which shows that increasing <span class="math notranslate nohighlight">\(B\)</span> (the number of trees) reduces the first term, while lowering the correlation <span class="math notranslate nohighlight">\(\rho\)</span> among trees reduces overall variance. Random feature selection helps reduce <span class="math notranslate nohighlight">\(\rho\)</span>, because each tree sees a different subset of features.</p>
</section>
<section id="main-hyperparameters">
<h3><span class="section-number">4.4.2. </span>Main Hyperparameters<a class="headerlink" href="#main-hyperparameters" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(B\)</span>: the number of trees in the forest.</p></li>
<li><p><span class="math notranslate nohighlight">\(n'\)</span>: the number of randomly selected features at each split (often <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> by default in classification).</p></li>
<li><p>Tree parameters: e.g., maximum depth, minimum samples per leaf, etc.</p></li>
</ol>
<p>In practice, random forests typically provide strong performance out of the box and are less sensitive to hyperparameter tuning compared to single trees or more complex models. They also allow computing a useful estimate of feature importance by measuring, for each variable, how much it contributes to the cost function improvement across all splits in all trees.</p>
</section>
</section>
<section id="feature-importances">
<h2><span class="section-number">4.5. </span>Feature Importances<a class="headerlink" href="#feature-importances" title="Link to this heading">#</a></h2>
<p>Most <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> models allows you to estimate how each feature contribues to the final prediction. For example, for tree-based models, both <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code> in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> store a <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> attribute after fitting. This gives an estimate of how much each feature contributed to reducing the split criterion (e.g., MSE).</p>
</section>
<section id="scikit-learn-pipelines">
<h2><span class="section-number">4.6. </span><code class="docutils literal notranslate"><span class="pre">Scikit-learn</span></code> Pipelines<a class="headerlink" href="#scikit-learn-pipelines" title="Link to this heading">#</a></h2>
<p>Pipelines in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> let us combine data preprocessing (e.g., imputation, scaling) and model training (e.g., a random forest) into a single workflow object. Below is a simple example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="n">example_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AddConstantTransformer</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom transformer that adds a constant value to all features.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">=</span> <span class="n">constant</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span>

<span class="n">custom_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;custom_add_constant&quot;</span><span class="p">,</span> <span class="n">AddConstantTransformer</span><span class="p">(</span><span class="n">constant</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercises">
<h2><span class="section-number">4.7. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="california-housing">
<h3><span class="section-number">4.7.1. </span>California Housing<a class="headerlink" href="#california-housing" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Load the well-known <em>California Housing</em> regression dataset using the following function:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Explore the dataset (distribution, summary statistics).</p></li>
<li><p>Check for missing values. If the dataset does not contain missing values, artificially introduce some for the purpose of this exercice. Then, handle missing values by:</p>
<ul class="simple">
<li><p>Dropping them (simple approach).</p></li>
<li><p>Imputing them (mean or median).</p></li>
<li><p>Using a more sophisticated method (e.g., <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code>).</p></li>
</ul>
</li>
<li><p>Normalize the features (using e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>). Remember that, as all processing steps, scaling is fit on the training set (not on the entire dataset) to avoid biasing the test set!</p></li>
<li><p>Preserve original vs. processed versions of the data for comparison.</p></li>
</ol>
</section>
<section id="solution">
<h3><span class="section-number">4.7.2. </span>Solution<a class="headerlink" href="#solution" title="Link to this heading">#</a></h3>
<section id="dataset-and-exploration">
<h4><span class="section-number">4.7.2.1. </span>Dataset and exploration<a class="headerlink" href="#dataset-and-exploration" title="Link to this heading">#</a></h4>
<p>For this exercise, we will use the California Housing dataset from <code class="docutils literal notranslate"><span class="pre">sklearn.datasets.fetch_california_housing</span></code>. It contains features related to California housing data, and the target is the average house value.</p>
<p>Alternatively, if you would like test your solution on other datasets, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> provides other built-in datasets, such as:</p>
<ul class="simple">
<li><p>Diabetes (a small dataset with 10 features for a regression task on disease progression).</p></li>
<li><p>Linnerud (exercise/physiological data, a small multi-output regression problem).
If you want a dataset with even more features or complexity, you could consider Ames Housing or Kaggle House Price, both of which have many categorical and numeric features but may require additional steps to download or preprocess. You can also explore the UCI Machine Learning Repository for a wide range of tabular datasets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">cal_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X_original</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y_original</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Target&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the feature matrix:&quot;</span><span class="p">,</span> <span class="n">X_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the target vector:&quot;</span><span class="p">,</span> <span class="n">y_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">X_original</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">X_original</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the feature matrix: (20640, 8)
Shape of the target vector: (20640,)
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 8 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   MedInc      20640 non-null  float64
 1   HouseAge    20640 non-null  float64
 2   AveRooms    20640 non-null  float64
 3   AveBedrms   20640 non-null  float64
 4   Population  20640 non-null  float64
 5   AveOccup    20640 non-null  float64
 6   Latitude    20640 non-null  float64
 7   Longitude   20640 non-null  float64
dtypes: float64(8)
memory usage: 1.3 MB
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.870671</td>
      <td>28.639486</td>
      <td>5.429000</td>
      <td>1.096675</td>
      <td>1425.476744</td>
      <td>3.070655</td>
      <td>35.631861</td>
      <td>-119.569704</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.899822</td>
      <td>12.585558</td>
      <td>2.474173</td>
      <td>0.473911</td>
      <td>1132.462122</td>
      <td>10.386050</td>
      <td>2.135952</td>
      <td>2.003532</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.499900</td>
      <td>1.000000</td>
      <td>0.846154</td>
      <td>0.333333</td>
      <td>3.000000</td>
      <td>0.692308</td>
      <td>32.540000</td>
      <td>-124.350000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.563400</td>
      <td>18.000000</td>
      <td>4.440716</td>
      <td>1.006079</td>
      <td>787.000000</td>
      <td>2.429741</td>
      <td>33.930000</td>
      <td>-121.800000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.534800</td>
      <td>29.000000</td>
      <td>5.229129</td>
      <td>1.048780</td>
      <td>1166.000000</td>
      <td>2.818116</td>
      <td>34.260000</td>
      <td>-118.490000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.743250</td>
      <td>37.000000</td>
      <td>6.052381</td>
      <td>1.099526</td>
      <td>1725.000000</td>
      <td>3.282261</td>
      <td>37.710000</td>
      <td>-118.010000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>15.000100</td>
      <td>52.000000</td>
      <td>141.909091</td>
      <td>34.066667</td>
      <td>35682.000000</td>
      <td>1243.333333</td>
      <td>41.950000</td>
      <td>-114.310000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="handling-missing-values">
<h4><span class="section-number">4.7.2.2. </span>Handling Missing Values<a class="headerlink" href="#handling-missing-values" title="Link to this heading">#</a></h4>
<p>Although the California Housing dataset typically does not contain missing values, let’s artificially introduce some missing values for demonstration. We will then show three approaches:</p>
<ol class="arabic simple">
<li><p>Dropping rows with missing values.</p></li>
<li><p>Imputing with mean or median.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> or another advanced method.</p></li>
</ol>
<section id="an-advanced-method-knnimputer">
<h5>An advanced method: <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code><a class="headerlink" href="#an-advanced-method-knnimputer" title="Link to this heading">#</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> uses a K-Nearest Neighbors approach to fill missing entries: for each row that has missing features, it finds the nearest <em>k</em> rows (in feature space) that do not have missing values in those columns, and uses their values (e.g., average) to impute the missing ones. This can often be more accurate than a simple mean or median if the data has local structure—rows that are similar in some features are likely also similar in the others.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNImputer</span>

<span class="n">missing_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">X_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.01</span>
<span class="n">X_missing</span> <span class="o">=</span> <span class="n">X_original</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X_missing</span><span class="p">[</span><span class="n">missing_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Number of missing values (artificial) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_missing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="n">X_train_full</span><span class="p">,</span> <span class="n">X_test_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">,</span> <span class="n">y_test_full</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_missing</span><span class="p">,</span> <span class="n">y_original</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">X_train_drop</span> <span class="o">=</span> <span class="n">X_train_full</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_train_drop</span> <span class="o">=</span> <span class="n">y_train_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X_train_drop</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
<span class="n">X_test_drop</span> <span class="o">=</span> <span class="n">X_test_full</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_test_drop</span> <span class="o">=</span> <span class="n">y_test_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X_test_drop</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- After Dropping Missing Values ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set size:&quot;</span><span class="p">,</span> <span class="n">X_train_drop</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set size:&quot;</span><span class="p">,</span> <span class="n">X_test_drop</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="n">mean_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_train_mean</span> <span class="o">=</span> <span class="n">mean_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_mean</span> <span class="o">=</span> <span class="n">mean_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>

<span class="n">median_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)</span>
<span class="n">X_train_median</span> <span class="o">=</span> <span class="n">median_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_median</span> <span class="o">=</span> <span class="n">median_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>

<span class="n">knn_imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_train_knn</span> <span class="o">=</span> <span class="n">knn_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_knn</span> <span class="o">=</span> <span class="n">knn_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Number of missing values (artificial) ---
MedInc        206
HouseAge      197
AveRooms      216
AveBedrms     232
Population    218
AveOccup      233
Latitude      207
Longitude     198
dtype: int64
--- After Dropping Missing Values ---
Training set size: (15202, 8)
Test set size: (3792, 8)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="dealing-with-categorical-values">
<h4><span class="section-number">4.7.2.3. </span>Dealing with Categorical Values<a class="headerlink" href="#dealing-with-categorical-values" title="Link to this heading">#</a></h4>
<p>In this dataset, we have only numerical features. However, real-world datasets often include categorical variables. Two popular encoding strategies are:</p>
<ol class="arabic simple">
<li><p>Label Encoding: Each category is assigned an integer.</p></li>
<li><p>One-Hot Encoding (Dummy Encoding): Create a new binary column for each category.</p></li>
</ol>
<p>Below is a small example using a synthetic dataframe containing a categorical column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">example_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Small&quot;</span><span class="p">,</span> <span class="s2">&quot;Medium&quot;</span><span class="p">,</span> <span class="s2">&quot;Large&quot;</span><span class="p">,</span> <span class="s2">&quot;Medium&quot;</span><span class="p">,</span> <span class="s2">&quot;Small&quot;</span><span class="p">],</span>
    <span class="s2">&quot;Weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">example_df</span><span class="p">[</span><span class="s2">&quot;Size_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">example_df</span><span class="p">[</span><span class="s2">&quot;Size&quot;</span><span class="p">])</span>

<span class="n">one_hot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">example_df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Size&quot;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;Size&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Original DataFrame with Label Encoding:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">example_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Size</th>
      <th>Weight</th>
      <th>Size_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Small</td>
      <td>1.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Medium</td>
      <td>2.3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Large</td>
      <td>5.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Medium</td>
      <td>2.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Small</td>
      <td>1.1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>DataFrame with One-Hot Encoding:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">one_hot_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight</th>
      <th>Size_label</th>
      <th>Size_Large</th>
      <th>Size_Medium</th>
      <th>Size_Small</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.3</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.5</td>
      <td>0</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.1</td>
      <td>2</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We’ll stick to numeric data for the main practical, but this is how you’d handle categorical features.</p>
</section>
<section id="normalization-scaling">
<h4><span class="section-number">4.7.2.4. </span>Normalization/Scaling<a class="headerlink" href="#normalization-scaling" title="Link to this heading">#</a></h4>
<p>Scaling is critical for distance-based methods. We will use <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>, which transforms each feature to have zero mean and unit variance, but note that <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> is also a standard.</p>
<blockquote>
<div><p><strong>Remember</strong>: <em>Always fit the scaler on the training set, then apply the same transformation to the test set. This prevents data leakage and preserves the integrity of the test data.</em></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_mean</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_mean</span><span class="p">)</span>

<span class="n">X_train_final</span> <span class="o">=</span> <span class="n">X_train_scaled</span>
<span class="n">X_test_final</span> <span class="o">=</span> <span class="n">X_test_scaled</span>
<span class="n">y_train_final</span> <span class="o">=</span> <span class="n">y_train_full</span>
<span class="n">y_test_final</span> <span class="o">=</span> <span class="n">y_test_full</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="vanilla-knn-implementation">
<h3><span class="section-number">4.7.3. </span>Vanilla KNN Implementation<a class="headerlink" href="#vanilla-knn-implementation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Split your data into training and test sets (we should have already done that in the previous exercise).</p></li>
<li><p>Implement a vanilla KNN regressor from scratch (no <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> for this part).</p></li>
<li><p>Choose a value of <span class="math notranslate nohighlight">\(k\)</span> (e.g., 3, 5, 7).</p></li>
<li><p>Compare predictions with the true values using Mean Squared Error (MSE).</p></li>
</ol>
<section id="id3">
<h4><span class="section-number">4.7.3.1. </span>Solution<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>On this graph, we clearly see that there is an optimal value for the number of neighbors to use: a value too small increases the variance of the predictions while a value too high increases the bias. The optimal value, of course, depends on the problem definition and on the training data.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn-v0_8-muted&#39;</span><span class="p">,</span> <span class="s1">&#39;practicals.mplstyle&#39;</span><span class="p">])</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.auto</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">def</span><span class="w"> </span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">knn_regressor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple KNN regressor (manual implementation).</span>
<span class="sd">    - X_train: training features (numpy array)</span>
<span class="sd">    - y_train: training targets (numpy array)</span>
<span class="sd">    - X_test: test features (numpy array)</span>
<span class="sd">    - k: number of neighbors to consider</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">test_point</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">X_test</span><span class="p">):</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">]</span>
        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="n">k</span><span class="p">]</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">k_indices</span><span class="p">])</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.spatial</span><span class="w"> </span><span class="kn">import</span> <span class="n">distance</span>

<span class="k">def</span><span class="w"> </span><span class="nf">knn_regressor_fast</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A faster KNN regressor using vectorized distance computation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
    <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">kth</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">k_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictions</span>

<span class="n">mses_knn</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
  <span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_regressor_fast</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">X_test_final</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
  <span class="n">mses_knn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">mses_knn</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE vs K (KNN)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;K&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "aac0281b24cd4924871a4e17d802c283"}</script><img alt="_images/23c6448c8c21055c42165ca9c32f68856bd36eab38bf282050df4754865f35d6.png" src="_images/23c6448c8c21055c42165ca9c32f68856bd36eab38bf282050df4754865f35d6.png" />
</div>
</div>
</section>
</section>
<section id="regression-trees-and-random-forests">
<h3><span class="section-number">4.7.4. </span>Regression Trees and Random Forests<a class="headerlink" href="#regression-trees-and-random-forests" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Train a regression tree on the training set.</p></li>
<li><p>Train a random forest on the training set.</p></li>
<li><p>Evaluate both models using MSE.</p></li>
<li><p>Compare their performance with your KNN model.</p></li>
<li><p>Determine how modifying the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> (for the regression trees) and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> (for the random forests) impacts the performances of the model. Plot these results graphically.</p></li>
<li><p>Plot the features importance of the best model with the optimal parameters</p></li>
</ol>
</section>
<section id="id4">
<h3><span class="section-number">4.7.5. </span>Solution<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<section id="implementation-with-scikit-learn">
<h4><span class="section-number">4.7.5.1. </span>Implementation with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code><a class="headerlink" href="#implementation-with-scikit-learn" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">tree_reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
<span class="n">mse_tree</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">)</span>

<span class="n">forest_reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">forest_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
<span class="n">y_pred_forest</span> <span class="o">=</span> <span class="n">forest_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
<span class="n">mse_forest</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_forest</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparison of MSE:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KNN (k=13): </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span><span class="w"> </span><span class="n">knn_regressor_fast</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span><span class="w"> </span><span class="n">y_train_final</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">X_test_final</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">13</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decision Tree: </span><span class="si">{</span><span class="n">mse_tree</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random Forest: </span><span class="si">{</span><span class="n">mse_forest</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Comparison of MSE:
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNN (k=13): 0.4382
Decision Tree: 0.4126
Random Forest: 0.2759
</pre></div>
</div>
</div>
</div>
<section id="exploring-key-parameters">
<h5>Exploring Key Parameters<a class="headerlink" href="#exploring-key-parameters" title="Link to this heading">#</a></h5>
<section id="decision-tree-max-depth">
<h6>Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code><a class="headerlink" href="#decision-tree-max-depth" title="Link to this heading">#</a></h6>
<p>We can examine how <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> (the maximum number of splits from the root to the leaf) affects performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">depths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">21</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<span class="n">mse_values_tree</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
    <span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
    <span class="n">mse_values_tree</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">],</span> <span class="n">mse_values_tree</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE vs max_depth (Decision Tree)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;max_depth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2135b67a75dac8a0e1a17c5ccaa0341e33485235d58fe27dea59dd829178d497.png" src="_images/2135b67a75dac8a0e1a17c5ccaa0341e33485235d58fe27dea59dd829178d497.png" />
</div>
</div>
</section>
</section>
<section id="random-forest-n-estimators">
<h5>Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code><a class="headerlink" href="#random-forest-n-estimators" title="Link to this heading">#</a></h5>
<p>Similarly, we can see how <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> (the number of trees in the forest) impacts the MSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimators_range</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">mse_values_rf</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">estimators_range</span><span class="p">):</span>
    <span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
    <span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
    <span class="n">mse_values_rf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">estimators_range</span><span class="p">,</span> <span class="n">mse_values_rf</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE vs n_estimators (Random Forest)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n_estimators&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "3454282e5f5344f6965db15bf0a3f175"}</script><img alt="_images/8eb1128b70d593f8d073eb43eb40d09bfd9e9552ac3d444d7a4bc6d382788e8e.png" src="_images/8eb1128b70d593f8d073eb43eb40d09bfd9e9552ac3d444d7a4bc6d382788e8e.png" />
</div>
</div>
</section>
</section>
<section id="feature-importance-with-the-best-models">
<h4><span class="section-number">4.7.5.2. </span>Feature importance with the best models<a class="headerlink" href="#feature-importance-with-the-best-models" title="Link to this heading">#</a></h4>
<p>Based on the experiments above, let’s define user-chosen parameters for the <em>best</em> (or at least improved) Decision Tree and Random Forest, then compute and display their feature importances.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Feel free to update <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> according to the results you obtained above. Later, we will see how this can be used as a <em>feature selection</em> method.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">60</span> 

<span class="n">best_tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">best_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>

<span class="n">best_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">best_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X_original</span><span class="o">.</span><span class="n">columns</span>

<span class="n">tree_importances</span> <span class="o">=</span> <span class="n">best_tree</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">forest_importances</span> <span class="o">=</span> <span class="n">best_forest</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="n">tree_sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">forest_sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Tree Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)),</span> <span class="n">tree_importances</span><span class="p">[</span><span class="n">tree_sorted_idx</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)),</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">tree_sorted_idx</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)),</span> <span class="n">forest_importances</span><span class="p">[</span><span class="n">forest_sorted_idx</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)),</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">forest_sorted_idx</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3f0bd4575a4e855b56315af4da15da2a019a60724a2e9fc162303258301eaf83.png" src="_images/3f0bd4575a4e855b56315af4da15da2a019a60724a2e9fc162303258301eaf83.png" />
</div>
</div>
</section>
</section>
<section id="building-a-pipeline">
<h3><span class="section-number">4.7.6. </span>Building a Pipeline<a class="headerlink" href="#building-a-pipeline" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Define a pipeline that applies the preprocessing steps (e.g., imputation, scaling) and the best-performing model (from Exercise 3).</p></li>
<li><p>Fit this pipeline on the training data and evaluate on the test data.</p></li>
<li><p>Observe how <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> handles transformations during <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p></li>
</ol>
<section id="id5">
<h4><span class="section-number">4.7.6.1. </span>Solution<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>Below, we define a pipeline that:</p>
<ol class="arabic simple">
<li><p>Imputes missing values with mean.</p></li>
<li><p>Scales the features.</p></li>
<li><p>Trains a random forest regressor.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">pipeline_rf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">pipeline_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">)</span>
<span class="n">y_pred_pipe</span> <span class="o">=</span> <span class="n">pipeline_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>
<span class="n">mse_pipeline_rf</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_full</span><span class="p">,</span> <span class="n">y_pred_pipe</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pipeline (Random Forest) MSE: </span><span class="si">{</span><span class="n">mse_pipeline_rf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline (Random Forest) MSE: 0.2759
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">
<h3><span class="section-number">4.7.7. </span>Additional exercise: bias–variance trade-off in regression with decision trees<a class="headerlink" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees" title="Link to this heading">#</a></h3>
<p>In this exercise, you will investigate the bias–variance trade-off for decision tree regressors of varying complexity. Specifically, you will perform simulations to estimate how the depth of a decision tree influences its performance.</p>
<ol class="arabic simple">
<li><p>Select a nonlinear, moderately complex target function e.g.:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[f(x) = \sin(4\pi x) + w\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[f(x) = \sin(2\pi x_1 x_2 x_3) + w\]</div>
<ol class="arabic simple" start="2">
<li><p>Evaluate three decision tree regressors, each with a different complexity level:</p>
<ul class="simple">
<li><p>Shallow tree: depth = 2</p></li>
<li><p>Medium-depth tree: depth = 6</p></li>
<li><p>Deep tree: depth = 20</p></li>
</ul>
</li>
<li><p>Generate training datasets composed of <span class="math notranslate nohighlight">\(n_{\text{train}} = 400\)</span> random samples:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
X_{\text{train}} \sim U(0,1), \quad y_{\text{train}} = f(X_{\text{train}}) + w
\]</div>
<p>Here, noise <span class="math notranslate nohighlight">\(w\)</span> follows a Gaussian distribution with mean 0 and standard deviation <span class="math notranslate nohighlight">\(0.3\)</span>:</p>
<div class="math notranslate nohighlight">
\[
w \sim \mathcal{N}(0, 0.3^2)
\]</div>
<ol class="arabic simple" start="5">
<li><p>Repeat the entire training and evaluation process <span class="math notranslate nohighlight">\(n_{\text{runs}} = 100\)</span> times. For each run, compute and store the mean squared error (MSE) using 10-fold cross-validation on the training set.</p></li>
<li><p>Evaluate your models on a test set <span class="math notranslate nohighlight">\(X_{\text{test}}\)</span> evenly spaced in the interval <span class="math notranslate nohighlight">\([0,1]\)</span> (e.g., 400 points).</p>
<ul class="simple">
<li><p>For each test point, compute predictions across all 100 simulation runs.</p></li>
<li><p>Using these predictions, compute</p>
<ul>
<li><p>MSE (CV): The average cross-validation mean squared error (which we will call <span class="math notranslate nohighlight">\(\widehat{\text{MISE}}_{\text{kfold}}\)</span>) obtained across the 100 runs.</p></li>
<li><p>Bias<span class="math notranslate nohighlight">\(^2\)</span>: Based on how far the <em>average prediction</em> per sample is from the sample’s true label.</p></li>
<li><p>Variance: Based on the variability of predictions per sample around that sample’s mean prediction.</p></li>
<li><p>MSE (true): the approximation <span class="math notranslate nohighlight">\(
\text{MSE} \approx \text{Bias}^2 + \text{Variance}.\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Discussion<br />
Analyze your results to illustrate how model complexity (tree depth) affects the balance between bias and variance:</p>
<ul class="simple">
<li><p>Identify which depth leads to underfitting (high bias, low variance).</p></li>
<li><p>Identify which depth leads to overfitting (low bias, high variance).</p></li>
</ul>
</li>
</ol>
<section id="id6">
<h4><span class="section-number">4.7.7.1. </span>Solution<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>

<span class="n">n_train</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">400</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">f_func</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span><span class="n">depth</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_runs</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">}</span>
<span class="n">cv_mse_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">depth</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">f_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_train</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        
        <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
            <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">),</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">cv_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">cv_mse_values</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_mse</span><span class="p">)</span>

<span class="n">noise_var</span> <span class="o">=</span> <span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bias_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mean_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">true_mse</span> <span class="o">=</span> <span class="n">bias_sq</span> <span class="o">+</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">noise_var</span>
    <span class="n">cv_mse_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_mse_values</span><span class="p">[</span><span class="n">depth</span><span class="p">])</span>

    <span class="n">metrics</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Bias^2&quot;</span><span class="p">:</span> <span class="n">bias_sq</span><span class="p">,</span>
        <span class="s2">&quot;Variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s2">&quot;True MSE&quot;</span><span class="p">:</span> <span class="n">true_mse</span><span class="p">,</span>
        <span class="s2">&quot;CV MSE&quot;</span><span class="p">:</span> <span class="n">cv_mse_mean</span>
    <span class="p">}</span>

<span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">metrics_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Tree Depth&quot;</span>
<span class="n">metrics_df</span> <span class="o">=</span> <span class="n">metrics_df</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            Bias^2  Variance  True MSE  CV MSE
Tree Depth                                    
2           0.1844    0.1033    0.3777  0.3724
6           0.0009    0.0324    0.1234  0.1255
20          0.0008    0.0889    0.1797  0.1802
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Linear Models</p>
      </div>
    </a>
    <a class="right-next"
       href="04.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Neural Networks for Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-precision-about-the-evaluation-metric-used-in-this-practical">4.1.1. Important precision about the evaluation metric used in this practical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-overview">4.1.2. Data preprocessing overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">4.2. K-Nearest Neighbors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">4.3. Regression Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">4.4. Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-reduce-variance">4.4.1. Why does this reduce variance?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">4.4.2. Main Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importances">4.5. Feature Importances</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">4.6. <code class="docutils literal notranslate"><span class="pre">Scikit-learn</span></code> Pipelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">4.7. Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#california-housing">4.7.1. California Housing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">4.7.2. Solution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-exploration">4.7.2.1. Dataset and exploration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values">4.7.2.2. Handling Missing Values</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#an-advanced-method-knnimputer">An advanced method: <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-categorical-values">4.7.2.3. Dealing with Categorical Values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-scaling">4.7.2.4. Normalization/Scaling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-knn-implementation">4.7.3. Vanilla KNN Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4.7.3.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees-and-random-forests">4.7.4. Regression Trees and Random Forests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4.7.5. Solution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-with-scikit-learn">4.7.5.1. Implementation with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-key-parameters">Exploring Key Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-max-depth">Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code></a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-n-estimators">Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-the-best-models">4.7.5.2. Feature importance with the best models</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-pipeline">4.7.6. Building a Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">4.7.6.1. Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">4.7.7. Additional exercise: bias–variance trade-off in regression with decision trees</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">4.7.7.1. Solution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tribel Pascal, Simar Cédric, Bontempi Gianluca
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  This is the practicals handbook for the course INFO-F422 - Statistical Foundations of Machine Learning. This is intended to be used alongside the <a href='https://www.researchgate.net/publication/242692234_Statistical_foundations_of_machine_learning_the_handbook'> theoretical handbook</a>.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>