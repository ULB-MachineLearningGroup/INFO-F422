
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Data preprocessing and tree-based models &#8212; Statistical Foundations of Machine Learning - Practicals handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="application/vnd.jupyter.widget-state+json">{"state": {"62c860bb46be44dd9d35ae582ad7e9f0": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8847ceca73614f6d97a347ab963eb8e5": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "0dfadd9640924564bf87672a45bee865": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_62c860bb46be44dd9d35ae582ad7e9f0", "max": 4128.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_8847ceca73614f6d97a347ab963eb8e5", "tabbable": null, "tooltip": null, "value": 4128.0}}, "0993897e877f4bbc975605472bcd9a95": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fb19e68a3d784a17b5e5812731e95103": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "6f57643a1790463d8bff4f43a76f2117": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_0993897e877f4bbc975605472bcd9a95", "placeholder": "\u200b", "style": "IPY_MODEL_fb19e68a3d784a17b5e5812731e95103", "tabbable": null, "tooltip": null, "value": "100%"}}, "9d3665fa11f84bb0b87363cdeb4ed1f1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "33cc1aa651284654a319e8ab235b1606": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "126dd356964d468a8a902c9a8ff242c6": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_9d3665fa11f84bb0b87363cdeb4ed1f1", "placeholder": "\u200b", "style": "IPY_MODEL_33cc1aa651284654a319e8ab235b1606", "tabbable": null, "tooltip": null, "value": "\u20074128/4128\u2007[05:02&lt;00:00,\u200713.64it/s]"}}, "55bc433f4fce4414a9c92dea869de7ab": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1fe0f79f3a03414c9702417a38c0655b": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_6f57643a1790463d8bff4f43a76f2117", "IPY_MODEL_0dfadd9640924564bf87672a45bee865", "IPY_MODEL_126dd356964d468a8a902c9a8ff242c6"], "layout": "IPY_MODEL_55bc433f4fce4414a9c92dea869de7ab", "tabbable": null, "tooltip": null}}, "f8b084935fa24fb2b0cad0fe7fa063c8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8bc36d7be360452d8b76837cbf5c2395": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "be8d320198b04016a5cd406a39046597": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_f8b084935fa24fb2b0cad0fe7fa063c8", "max": 4128.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_8bc36d7be360452d8b76837cbf5c2395", "tabbable": null, "tooltip": null, "value": 4128.0}}, "c2470974a4b04e79b57650f1b224e0d6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ac711806094847d18d46318d12ebb726": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "bf5aa7d09b6b4a2eaca11ac307bb04e1": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_c2470974a4b04e79b57650f1b224e0d6", "placeholder": "\u200b", "style": "IPY_MODEL_ac711806094847d18d46318d12ebb726", "tabbable": null, "tooltip": null, "value": "100%"}}, "0d6c33b0e6514eb585dc3e53221f0050": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bedbb5084ad14a2a9f3f683ad3fc7a15": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "634da0dec0324824a6b174a7e67d72cf": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_0d6c33b0e6514eb585dc3e53221f0050", "placeholder": "\u200b", "style": "IPY_MODEL_bedbb5084ad14a2a9f3f683ad3fc7a15", "tabbable": null, "tooltip": null, "value": "\u20074128/4128\u2007[05:20&lt;00:00,\u200712.62it/s]"}}, "c30566c68966487795a3bd068d108bba": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0b6537a3dd9247f693b2e62f94737d6f": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_bf5aa7d09b6b4a2eaca11ac307bb04e1", "IPY_MODEL_be8d320198b04016a5cd406a39046597", "IPY_MODEL_634da0dec0324824a6b174a7e67d72cf"], "layout": "IPY_MODEL_c30566c68966487795a3bd068d108bba", "tabbable": null, "tooltip": null}}, "8bde07d306324efc9362e5b9883005cc": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "66229530f7ca44daaef7a27d4d12ceba": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "f6263aaf7a6842f0b964806ddabd0074": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_8bde07d306324efc9362e5b9883005cc", "max": 4128.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_66229530f7ca44daaef7a27d4d12ceba", "tabbable": null, "tooltip": null, "value": 4128.0}}, "7d232d427f5c4de985039bfd6e11c1be": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a63284b20bc14820b82e5932d348eaf5": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "9412c721291e4cc9935244e94bfe980f": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_7d232d427f5c4de985039bfd6e11c1be", "placeholder": "\u200b", "style": "IPY_MODEL_a63284b20bc14820b82e5932d348eaf5", "tabbable": null, "tooltip": null, "value": "100%"}}, "c3bd02fdf5094acd8cc738b8a11aa367": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "79b6ca69a8a0469dbc0eb0d3198ecb44": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "7e5d6c3938e245e7b51ee6461e36776f": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_c3bd02fdf5094acd8cc738b8a11aa367", "placeholder": "\u200b", "style": "IPY_MODEL_79b6ca69a8a0469dbc0eb0d3198ecb44", "tabbable": null, "tooltip": null, "value": "\u20074128/4128\u2007[05:32&lt;00:00,\u200711.64it/s]"}}, "508eefb9cc034a75a97eff35f54b8045": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0b65596c3797484b9dbc7950fada8c00": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_9412c721291e4cc9935244e94bfe980f", "IPY_MODEL_f6263aaf7a6842f0b964806ddabd0074", "IPY_MODEL_7e5d6c3938e245e7b51ee6461e36776f"], "layout": "IPY_MODEL_508eefb9cc034a75a97eff35f54b8045", "tabbable": null, "tooltip": null}}}, "version_major": 2, "version_minor": 0}</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '03';</script>
    <link rel="icon" href="_static/sfml.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Neural Networks for Regression" href="04.html" />
    <link rel="prev" title="3. Linear Models" href="02.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontpage.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/sfml.png" class="logo__image only-light" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>
    <script>document.write(`<img src="_static/sfml.png" class="logo__image only-dark" alt="Statistical Foundations of Machine Learning - Practicals handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontpage.html">
                    Home
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="01.html">2. Introduction to probabilistic methods and Monte Carlo simulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.html">3. Linear Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Data preprocessing and tree-based models</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.html">5. Neural Networks for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.html">6. Ensembles of models and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="06.html">7. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion.html">8. Conclusion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/03.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data preprocessing and tree-based models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-overview">4.1.1. Data preprocessing overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">4.1.2. K-Nearest Neighbors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">4.1.3. Regression trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">4.1.4. Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-reduce-variance">4.1.4.1. Why does this reduce variance?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">4.1.4.2. Main Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importances">4.1.5. Feature importances</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">4.1.6. Scikit-learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-precision-about-the-evaluation-metric-used-in-this-practical">4.1.7. Important precision about the evaluation metric used in this practical:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-data-loading-exploration-and-preprocessing">4.2. Exercise 1: Data Loading, Exploration, and Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-exploration">4.2.1. Dataset and exploration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values">4.2.2. Handling Missing Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#knnimputer-explanation">4.2.2.1. <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-categorical-values">4.2.3. Dealing with Categorical Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-scaling">4.2.4. Normalization/Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-vanilla-knn-implementation">4.3. Exercise 2: Vanilla KNN Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">4.3.1. Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-regression-trees-and-random-forests">4.4. Exercise 3: Regression Trees and Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-with-scikit-learn">4.4.1. Implementation with Scikit-learn</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-key-parameters">4.4.2. Exploring Key Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-max-depth">4.4.2.1. Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-n-estimators">4.4.2.2. Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-the-best-models">4.4.3. Feature importance with the best models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-building-a-pipeline">4.5. Exercise 4: Building a Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-implementation">4.5.1. Pipeline Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">4.6. Additional exercise: bias–variance trade-off in regression with decision trees</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-preprocessing-and-tree-based-models">
<h1><span class="section-number">4. </span>Data preprocessing and tree-based models<a class="headerlink" href="#data-preprocessing-and-tree-based-models" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">4.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>This practical will address the following topics:</p>
<ol class="arabic simple">
<li><p>Typical <strong>data preprocessing</strong> steps (handling missing values, normalization, dealing with categorical variables).</p></li>
<li><p>A <strong>vanilla implementation</strong> of the <strong>K-Nearest Neighbors (KNN)</strong> algorithm for regression.</p></li>
<li><p>Regression with tree-based models: <strong>regression trees</strong> and <strong>Random forests</strong></p></li>
<li><p><strong>Feature importances</strong> in tree-based models.</p></li>
<li><p><strong>Scikit-learn pipelines</strong> to combine preprocessing and modeling into a single object.</p></li>
</ol>
<section id="data-preprocessing-overview">
<h3><span class="section-number">4.1.1. </span>Data preprocessing overview<a class="headerlink" href="#data-preprocessing-overview" title="Link to this heading">#</a></h3>
<p>Data preprocessing is the foundation of any successful machine learning project. Many Machine Learning models see their performances degraded when trained on poorly preprocessed data. Key steps include:</p>
<ul class="simple">
<li><p><strong>Handling Missing Values</strong>: Missing data can bias results if not addressed. This problem affects many models—such as linear models, support vector machines (SVMs), and neural networks—but it is particularly critical for distance-based algorithms like KNN where missing feature values can disrupt the calculation of distances, leading to inaccurate outcomes. Common strategies for handling missing values include:</p>
<ul>
<li><p><em>Dropping</em> rows/columns with missing data.</p></li>
<li><p><em>Imputing</em> missing data using simple statistics (mean, median) or more sophisticated methods like <strong><code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code></strong>.<br><br></p></li>
</ul>
</li>
<li><p><strong>Normalization / Scaling</strong>: Many algorithms assume features are on similar scales. Without proper scaling, features with larger numerical ranges may dominate those with smaller ranges, potentially biasing the model’s results. Common scaling strategies include:</p>
<ul>
<li><p><em>Standardization</em>: Transform features to have zero mean and unit variance.</p></li>
<li><p><em>Min-Max Scaling</em>: Scale features to a fixed <span class="math notranslate nohighlight">\([0,1]\)</span> range.<br><br></p></li>
</ul>
</li>
<li><p><strong>Dealing with Categorical Variables</strong>: Categorical variables must be converted to a numeric representation that preserves meaningful differences. Common strategies for dealing with categorical variables include:</p>
<ul>
<li><p><em>Label Encoding</em>: Assign an integer to each category (useful if there is an <strong>ordinal</strong> relationship).</p></li>
<li><p><em>One-Hot Encoding (Dummy Encoding)</em>: Create a new binary column for each category (common for <strong>nominal</strong> features). This approach can cause a significant increase in input dimensionality (see the curse of dimensionality).</p></li>
</ul>
</li>
</ul>
</section>
<section id="k-nearest-neighbors">
<h3><span class="section-number">4.1.2. </span>K-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Link to this heading">#</a></h3>
<p>The <em><span class="math notranslate nohighlight">\(K\)</span>-Nearest Neighbors</em> (KNN) algorithm is a <strong>local modeling</strong> approach that infers its prediction from the data points closest (in some metric) to a query. Suppose we have a training set of inputs <span class="math notranslate nohighlight">\(\{x_i\}\)</span> with associated labels or targets <span class="math notranslate nohighlight">\(\{y_i\}\)</span>. For a new query point <span class="math notranslate nohighlight">\(q\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Distance computation</strong>: Compute the distance (e.g., Euclidean) between <span class="math notranslate nohighlight">\(q\)</span> and each training example <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
<li><p><strong>Ranking</strong>: Sort or rank the training points by increasing distance to <span class="math notranslate nohighlight">\(q\)</span>.</p></li>
<li><p><strong>Neighborhood selection</strong>: Identify the subset of the <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors <span class="math notranslate nohighlight">\(\{x_{[1]}, \dots, x_{[K]}\}\)</span> where <span class="math notranslate nohighlight">\(y_{[i]}\)</span> denotes the label of neighbor <span class="math notranslate nohighlight">\(x_{[i]}\)</span>.</p></li>
<li><p><strong>Regression or classification</strong>: KNN can be viewed as a local estimator of the conditional expectation <span class="math notranslate nohighlight">\(\mathbb{E}[y|x=q]\)</span>.<br>For <strong>regression</strong> tasks, the simplest approach is to average the targets of the <span class="math notranslate nohighlight">\(K\)</span> nearest neighbors:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\hat{y}(q) = \frac{1}{K}\sum_{i=1}^{K} y_{[i]}.
\]</div>
<p>Alternatively, a local linear model may be used:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}(q) = \hat{a}^T q + \hat{b},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{a}\)</span> and <span class="math notranslate nohighlight">\(\hat{b}\)</span> are estimated (locally) by least-squares on the neighbors.
<br><br>For <strong>classification</strong>, one can estimate the conditional probability of belonging to a specific class (e.g., class “1”) by the proportion of neighbors labeled “1”:</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_1(q) = \frac{1}{K}\sum_{i=1}^{K} y_{[i]}.
\]</div>
<p>A threshold (e.g., 0.5) can then be used to decide the class label.</p>
<p>A key hyperparameter here is <span class="math notranslate nohighlight">\(K\)</span>. A <em>smaller</em> <span class="math notranslate nohighlight">\(K\)</span> reduces bias but increases variance, while a <em>larger</em> <span class="math notranslate nohighlight">\(K\)</span> produces a smoother model at the risk of higher bias. KNN’s performance also heavily depends on the distance metric, especially when features have different scales or include categorical variables (which may require specialized encodings or distance definitions).</p>
</section>
<section id="regression-trees">
<h3><span class="section-number">4.1.3. </span>Regression trees<a class="headerlink" href="#regression-trees" title="Link to this heading">#</a></h3>
<p>Regression trees rely on a tree-based structure of <em>internal nodes</em> (where decisions are made) and <em>terminal nodes</em> (leaves), which partition the input space into <strong>mutually exclusive</strong> regions, each associated with a simple (often constant) local model. Its construction begins with a <strong>tree growing</strong> phase: starting from the root node that contains all data, we recursively choose a split that best reduces the empirical error. Specifically, for a node <span class="math notranslate nohighlight">\(t\)</span> containing <span class="math notranslate nohighlight">\(N(t)\)</span> samples, we define</p>
<div class="math notranslate nohighlight">
\[
R_{\mathrm{emp}}(t) \;=\; \min_{\alpha_t} \sum_{i=1}^{N(t)} L\bigl(y_i,\;h_t(x_i,\;\alpha_t)\bigr),
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is typically the squared error <span class="math notranslate nohighlight">\((y - \hat{y})^2\)</span> for regression. Given a potential split <span class="math notranslate nohighlight">\(s\)</span> dividing <span class="math notranslate nohighlight">\(t\)</span> into children <span class="math notranslate nohighlight">\(t_l\)</span> and <span class="math notranslate nohighlight">\(t_r\)</span>, we consider the decrease in empirical error:</p>
<div class="math notranslate nohighlight">
\[
\Delta E(s,\;t) \;=\; R_{\mathrm{emp}}(t)\;-\;\bigl(R_{\mathrm{emp}}(t_l)\;+\;R_{\mathrm{emp}}(t_r)\bigr).
\]</div>
<p>We choose the split <span class="math notranslate nohighlight">\(s^*\)</span> maximizing <span class="math notranslate nohighlight">\(\Delta E\)</span>, partition the dataset accordingly, and repeat the procedure recursively until no further improvement is found or a stopping criterion is met. This exhaustive splitting often yields a <strong>very large</strong> tree that overfits the data.</p>
<p>To address overfitting, a <strong>cost-complexity pruning</strong> procedure is commonly used. Introducing a complexity parameter <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span> that penalizes the number of leaf nodes, we define the cost-complexity measure:</p>
<div class="math notranslate nohighlight">
\[
R_{\lambda}(T) = R_{\mathrm{emp}}(T) + \lambda \, |T|,
\]</div>
<p>where <span class="math notranslate nohighlight">\(|T|\)</span> is the number of terminal nodes (leaves) in tree <span class="math notranslate nohighlight">\(T\)</span>.</p>
<ul class="simple">
<li><p>By gradually increasing <span class="math notranslate nohighlight">\(\lambda\)</span>, we obtain a <em>sequence</em> of subtrees:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
T_{\max} \supset T_{L-1} \supset \dots \supset T_2 \supset T_1.
\]</div>
<p>Each has fewer leaves. We consider all admissible subtrees <span class="math notranslate nohighlight">\(T_t \subset T\)</span> of the large tree and compute the smallest</p>
<div class="math notranslate nohighlight">
\[
\lambda_t = \frac{R_{\mathrm{emp}}(T) - R_{\mathrm{emp}}(T_t)}{|T| - |T_t|}
\]</div>
<p>that yields a lower cost. We select the subtree that <strong>minimizes</strong> <span class="math notranslate nohighlight">\(R_{\lambda}(T)\)</span> (the best balances between empirical error and model complexity).</p>
<ul class="simple">
<li><p>The final tree structure is typically chosen via cross-validation or held-out validation to find the best <span class="math notranslate nohighlight">\(\lambda\)</span>.</p></li>
</ul>
<p>Below is a short snippet demonstrating how to initialize a <strong>DecisionTreeRegressor</strong> in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> with default parameters (such as <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>, etc.):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">tree_reg_example</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
    <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>             
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>              
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>                  
<span class="p">)</span>
</pre></div>
</div>
<p>This model is then grown with the standard CART algorithm described above.</p>
<p>For a more visual representation of tree-based models, an animated version of decision trees (classification) can be found <a class="reference external" href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">here</a>.</p>
</section>
<section id="random-forests">
<h3><span class="section-number">4.1.4. </span>Random Forests<a class="headerlink" href="#random-forests" title="Link to this heading">#</a></h3>
<p>A <strong>Random Forest</strong> (RF) is an ensemble learning technique designed to reduce the variance of decision trees by combining two main ideas:</p>
<ol class="arabic simple">
<li><p><strong>Bootstrap Sampling</strong> (Bagging): We generate <span class="math notranslate nohighlight">\(B\)</span> <em>bootstrap</em> datasets, each by sampling (with replacement) from the original training data.</p></li>
<li><p><strong>Random Feature Selection</strong> (Feature Bagging): At each split, a <em>random subset</em> of <span class="math notranslate nohighlight">\(n' &lt; n\)</span> features is chosen, and the best split is found <em>only among those <span class="math notranslate nohighlight">\(n'\)</span> features</em>.</p></li>
</ol>
<p>Hence, each tree <span class="math notranslate nohighlight">\(h_b(\mathbf{x}, \alpha_b)\)</span> is built from a <em>different</em> resampled dataset <strong>and</strong> uses only a random subset of features at each split. As a result, the trees are more <strong>decorrelated</strong> and often are <em>not pruned</em> heavily.</p>
<p>Formally, for a regression task:</p>
<div class="math notranslate nohighlight">
\[
  h_{\mathrm{rf}}(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} h_b(\mathbf{x}, \alpha_b),
\]</div>
<p>i.e., the <strong>average</strong> of all tree predictions. (In classification, we take the majority vote.)</p>
<section id="why-does-this-reduce-variance">
<h4><span class="section-number">4.1.4.1. </span>Why does this reduce variance?<a class="headerlink" href="#why-does-this-reduce-variance" title="Link to this heading">#</a></h4>
<p>Suppose each tree <span class="math notranslate nohighlight">\(h_b\)</span> has variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> and they have pairwise correlation <span class="math notranslate nohighlight">\(\rho\)</span>. The variance of the RF predictor <span class="math notranslate nohighlight">\(h_{\mathrm{rf}}\)</span> can be approximated as:</p>
<div class="math notranslate nohighlight">
\[
  \mathrm{Var}[h_{\mathrm{rf}}] \approx \frac{(1 - \rho)\,\sigma^2}{B} + \rho\,\sigma^2,
\]</div>
<p>which shows that <strong>increasing</strong> <span class="math notranslate nohighlight">\(B\)</span> (the number of trees) reduces the first term, while <strong>lowering</strong> the correlation <span class="math notranslate nohighlight">\(\rho\)</span> among trees reduces overall variance. Random feature selection helps reduce <span class="math notranslate nohighlight">\(\rho\)</span>, because each tree sees a different subset of features.</p>
</section>
<section id="main-hyperparameters">
<h4><span class="section-number">4.1.4.2. </span>Main Hyperparameters<a class="headerlink" href="#main-hyperparameters" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(B\)</span>: the number of trees in the forest.</p></li>
<li><p><span class="math notranslate nohighlight">\(n'\)</span>: the number of randomly selected features at each split (often <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> by default in classification).</p></li>
<li><p><strong>Tree parameters</strong>: e.g., maximum depth, minimum samples per leaf, etc.</p></li>
</ol>
<p>In practice, random forests typically provide strong performance out of the box and are less sensitive to hyperparameter tuning compared to single trees or more complex models. They also allow computing a useful estimate of <strong>feature importance</strong> by measuring, for each variable, how much it contributes to the cost function improvement across all splits in all trees.</p>
<p>Below is a short snippet demonstrating how to <strong>initialize a RandomForestRegressor</strong> in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> with default values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">forest_reg_example</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>    <span class="c1"># number of trees</span>
    <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</pre></div>
</div>
<p>All of these parameters correspond to the concepts introduced in the sections above.</p>
</section>
</section>
<section id="feature-importances">
<h3><span class="section-number">4.1.5. </span>Feature importances<a class="headerlink" href="#feature-importances" title="Link to this heading">#</a></h3>
<p>Most scikit-learn models allows you to estimate how each feature contribues to the final prediction. For example, for tree-based models, both <strong>DecisionTreeRegressor</strong> and <strong>RandomForestRegressor</strong> in scikit-learn store a <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> attribute after fitting. This gives an estimate of how much each feature contributed to reducing the split criterion (e.g., MSE). Here’s a quick example using a fitted model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Suppose we have a fitted random forest model: forest_reg_example.fit(X, y)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Feature1&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature2&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature3&quot;</span><span class="p">]</span>  <span class="c1"># adapt to your data</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">forest_reg_example</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">importances</span><span class="p">)),</span> <span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">importances</span><span class="p">)),</span> <span class="p">[</span><span class="n">feature_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The resulting bar plot helps visualize which features were most important for the splits (on average) across the forest. A single decision tree also offers a similar <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> vector but typically is less robust than the ensemble measure.</p>
</section>
<section id="scikit-learn-pipelines">
<h3><span class="section-number">4.1.6. </span>Scikit-learn Pipelines<a class="headerlink" href="#scikit-learn-pipelines" title="Link to this heading">#</a></h3>
<p>Pipelines in scikit-learn let us combine <strong>data preprocessing</strong> (e.g., imputation, scaling) and <strong>model training</strong> (e.g., a random forest) into a single workflow object. Below is a simple example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># Example pipeline: impute missing values with mean, then scale, then fit a linear model</span>
<span class="n">example_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Demonstration of how a pipeline is defined. No fitting done here.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Demonstration of how a pipeline is defined. No fitting done here.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AddConstantTransformer</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom transformer that adds a constant value to all features.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constant</span> <span class="o">=</span> <span class="n">constant</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant</span>

<span class="n">custom_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;custom_add_constant&quot;</span><span class="p">,</span> <span class="n">AddConstantTransformer</span><span class="p">(</span><span class="n">constant</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># custom_pipeline.fit(X_train, y_train)</span>
<span class="c1"># y_pred = custom_pipeline.predict(X_test)</span>
<span class="c1"># print(&quot;MSE with custom transformer in pipeline:&quot;, mean_squared_error(y_test, y_pred))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="important-precision-about-the-evaluation-metric-used-in-this-practical">
<h3><span class="section-number">4.1.7. </span>Important precision about the evaluation metric used in this practical:<a class="headerlink" href="#important-precision-about-the-evaluation-metric-used-in-this-practical" title="Link to this heading">#</a></h3>
<p>In this practical, we will use the <em>Mean Squared Error</em> (MSE) as a convenient shorthand for the estimated <em>Mean Integrated Squared Error</em> <span class="math notranslate nohighlight">\((\widehat{\mathrm{MISE}})\)</span>. Indeed, in practice, we rarely know the true MISE and instead rely on estimates obtained via techniques like <span class="math notranslate nohighlight">\(K\)</span>-fold cross-validation or leave-one-out (LOO) cross-validation. So keep in mind that when we refer to the MSE in these contexts, what we are really computing is an estimation of <span class="math notranslate nohighlight">\(\widehat{\mathrm{MISE}}\)</span> from our data.</p>
</section>
</section>
<section id="exercise-1-data-loading-exploration-and-preprocessing">
<h2><span class="section-number">4.2. </span>Exercise 1: Data Loading, Exploration, and Preprocessing<a class="headerlink" href="#exercise-1-data-loading-exploration-and-preprocessing" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Load the well-known <strong>California Housing</strong> regression dataset using the following function:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Explore the dataset (distribution, summary statistics).</p></li>
<li><p>Check for missing values. If the dataset does not contain missing values, artificially introduce some for the purpose of this exercice. Then, handle missing values by:</p>
<ul class="simple">
<li><p>Dropping them (simple approach).</p></li>
<li><p>Imputing them (mean or median).</p></li>
<li><p>Using a more sophisticated method (e.g., <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code>).</p></li>
</ul>
</li>
<li><p>Normalize the features (using e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>). Remember that, as all processing steps, <strong>scaling is fit on the training set</strong> (not on the entire dataset) to avoid biasing the test set!</p></li>
<li><p>Preserve original vs. processed versions of the data for comparison.</p></li>
</ol>
<section id="dataset-and-exploration">
<h3><span class="section-number">4.2.1. </span>Dataset and exploration<a class="headerlink" href="#dataset-and-exploration" title="Link to this heading">#</a></h3>
<p>For this exercise, we will use the <strong>California Housing</strong> dataset from <code class="docutils literal notranslate"><span class="pre">sklearn.datasets.fetch_california_housing</span></code>. It contains features related to California housing data, and the target is the average house value.</p>
<p>Alternatively, if you would like test your solution on other datasets, scikit-learn provides other built-in datasets, such as:</p>
<ul class="simple">
<li><p><strong>Diabetes</strong> (a small dataset with 10 features for a regression task on disease progression).</p></li>
<li><p><strong>Linnerud</strong> (exercise/physiological data, a small multi-output regression problem).
If you want a dataset with even more features or complexity, you could consider <strong>Ames Housing</strong> or <strong>Kaggle House Price</strong>, both of which have many categorical and numeric features but may require additional steps to download or preprocess. You can also explore the <strong>UCI Machine Learning Repository</strong> for a wide range of tabular datasets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="c1"># Load the dataset</span>
<span class="n">cal_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X_original</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y_original</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Target&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the feature matrix:&quot;</span><span class="p">,</span> <span class="n">X_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the target vector:&quot;</span><span class="p">,</span> <span class="n">y_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Basic Info ---&quot;</span><span class="p">)</span>
<span class="n">X_original</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Data Statistics (Original) ---&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">X_original</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data exploration done. Now let&#39;s handle missing values.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the feature matrix: (20640, 8)
Shape of the target vector: (20640,)

--- Basic Info ---
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 8 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   MedInc      20640 non-null  float64
 1   HouseAge    20640 non-null  float64
 2   AveRooms    20640 non-null  float64
 3   AveBedrms   20640 non-null  float64
 4   Population  20640 non-null  float64
 5   AveOccup    20640 non-null  float64
 6   Latitude    20640 non-null  float64
 7   Longitude   20640 non-null  float64
dtypes: float64(8)
memory usage: 1.3 MB

--- Data Statistics (Original) ---
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.870671</td>
      <td>28.639486</td>
      <td>5.429000</td>
      <td>1.096675</td>
      <td>1425.476744</td>
      <td>3.070655</td>
      <td>35.631861</td>
      <td>-119.569704</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.899822</td>
      <td>12.585558</td>
      <td>2.474173</td>
      <td>0.473911</td>
      <td>1132.462122</td>
      <td>10.386050</td>
      <td>2.135952</td>
      <td>2.003532</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.499900</td>
      <td>1.000000</td>
      <td>0.846154</td>
      <td>0.333333</td>
      <td>3.000000</td>
      <td>0.692308</td>
      <td>32.540000</td>
      <td>-124.350000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.563400</td>
      <td>18.000000</td>
      <td>4.440716</td>
      <td>1.006079</td>
      <td>787.000000</td>
      <td>2.429741</td>
      <td>33.930000</td>
      <td>-121.800000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.534800</td>
      <td>29.000000</td>
      <td>5.229129</td>
      <td>1.048780</td>
      <td>1166.000000</td>
      <td>2.818116</td>
      <td>34.260000</td>
      <td>-118.490000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.743250</td>
      <td>37.000000</td>
      <td>6.052381</td>
      <td>1.099526</td>
      <td>1725.000000</td>
      <td>3.282261</td>
      <td>37.710000</td>
      <td>-118.010000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>15.000100</td>
      <td>52.000000</td>
      <td>141.909091</td>
      <td>34.066667</td>
      <td>35682.000000</td>
      <td>1243.333333</td>
      <td>41.950000</td>
      <td>-114.310000</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data exploration done. Now let&#39;s handle missing values.
</pre></div>
</div>
</div>
</div>
</section>
<section id="handling-missing-values">
<h3><span class="section-number">4.2.2. </span>Handling Missing Values<a class="headerlink" href="#handling-missing-values" title="Link to this heading">#</a></h3>
<p>Although the California Housing dataset typically does not contain missing values, let’s <strong>artificially introduce some</strong> missing values for demonstration. We will then show three approaches:</p>
<ol class="arabic simple">
<li><p><strong>Dropping</strong> rows with missing values.</p></li>
<li><p><strong>Imputing</strong> with mean or median.</p></li>
<li><p><strong>KNNImputer</strong> or another advanced method.</p></li>
</ol>
<section id="knnimputer-explanation">
<h4><span class="section-number">4.2.2.1. </span><code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> Explanation<a class="headerlink" href="#knnimputer-explanation" title="Link to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> uses a K-Nearest Neighbors approach to fill missing entries: for each row that has missing features, it finds the nearest <em>k</em> rows (in feature space) that do not have missing values in those columns, and uses their values (e.g., average) to impute the missing ones. This can often be more accurate than a simple mean or median if the data has local structure—rows that are similar in some features are likely also similar in the others.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Artificially introduce missing values</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">missing_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">X_original</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.01</span>  <span class="c1"># ~1% missing</span>
<span class="n">X_missing</span> <span class="o">=</span> <span class="n">X_original</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X_missing</span><span class="p">[</span><span class="n">missing_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Number of missing values (artificial) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_missing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="c1"># Let&#39;s first do a simple train-test split</span>
<span class="n">X_train_full</span><span class="p">,</span> <span class="n">X_test_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">,</span> <span class="n">y_test_full</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_missing</span><span class="p">,</span> <span class="n">y_original</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># 2.3.1 Dropping rows with missing values</span>
<span class="n">X_train_drop</span> <span class="o">=</span> <span class="n">X_train_full</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_train_drop</span> <span class="o">=</span> <span class="n">y_train_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X_train_drop</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
<span class="n">X_test_drop</span> <span class="o">=</span> <span class="n">X_test_full</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_test_drop</span> <span class="o">=</span> <span class="n">y_test_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X_test_drop</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- After Dropping Missing Values ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set size:&quot;</span><span class="p">,</span> <span class="n">X_train_drop</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set size:&quot;</span><span class="p">,</span> <span class="n">X_test_drop</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 2.3.2 Imputing with mean or median</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="n">mean_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_train_mean</span> <span class="o">=</span> <span class="n">mean_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_mean</span> <span class="o">=</span> <span class="n">mean_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>

<span class="n">median_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)</span>
<span class="n">X_train_median</span> <span class="o">=</span> <span class="n">median_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_median</span> <span class="o">=</span> <span class="n">median_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>

<span class="c1"># 2.3.3 A more sophisticated method: KNNImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNImputer</span>

<span class="n">knn_imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_train_knn</span> <span class="o">=</span> <span class="n">knn_imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>
<span class="n">X_test_knn</span> <span class="o">=</span> <span class="n">knn_imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Imputation done. Now we have multiple versions of the dataset.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Number of missing values (artificial) ---
MedInc        212
HouseAge      190
AveRooms      200
AveBedrms     203
Population    230
AveOccup      167
Latitude      193
Longitude     221
dtype: int64

--- After Dropping Missing Values ---
Training set size: (15246, 8)
Test set size: (3833, 8)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Imputation done. Now we have multiple versions of the dataset.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="dealing-with-categorical-values">
<h3><span class="section-number">4.2.3. </span>Dealing with Categorical Values<a class="headerlink" href="#dealing-with-categorical-values" title="Link to this heading">#</a></h3>
<p>In this dataset, we have only numerical features. However, real-world datasets often include <strong>categorical</strong> variables. Two popular encoding strategies are:</p>
<ol class="arabic simple">
<li><p><strong>Label Encoding</strong>: Each category is assigned an integer.</p></li>
<li><p><strong>One-Hot Encoding (Dummy Encoding)</strong>: Create a new binary column for each category.</p></li>
</ol>
<p>Below is a small example using a synthetic dataframe containing a categorical column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Small&quot;</span><span class="p">,</span> <span class="s2">&quot;Medium&quot;</span><span class="p">,</span> <span class="s2">&quot;Large&quot;</span><span class="p">,</span> <span class="s2">&quot;Medium&quot;</span><span class="p">,</span> <span class="s2">&quot;Small&quot;</span><span class="p">],</span>
    <span class="s2">&quot;Weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># 2.4.1 Label Encoding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">example_df</span><span class="p">[</span><span class="s2">&quot;Size_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">example_df</span><span class="p">[</span><span class="s2">&quot;Size&quot;</span><span class="p">])</span>

<span class="c1"># 2.4.2 One-Hot Encoding</span>
<span class="n">one_hot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">example_df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Size&quot;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;Size&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original DataFrame with Label Encoding:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">example_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DataFrame with One-Hot Encoding:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">one_hot_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original DataFrame with Label Encoding:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Size</th>
      <th>Weight</th>
      <th>Size_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Small</td>
      <td>1.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Medium</td>
      <td>2.3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Large</td>
      <td>5.5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Medium</td>
      <td>2.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Small</td>
      <td>1.1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DataFrame with One-Hot Encoding:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight</th>
      <th>Size_label</th>
      <th>Size_Large</th>
      <th>Size_Medium</th>
      <th>Size_Small</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.3</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.5</td>
      <td>0</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.1</td>
      <td>2</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We’ll stick to numeric data for the main practical, but this is how you’d handle categorical features.</p>
</section>
<section id="normalization-scaling">
<h3><span class="section-number">4.2.4. </span>Normalization/Scaling<a class="headerlink" href="#normalization-scaling" title="Link to this heading">#</a></h3>
<p>Scaling is critical for distance-based methods. We will use <strong><code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code></strong>, which transforms each feature to have zero mean and unit variance.</p>
<blockquote>
<div><p><strong>Remember</strong>: <em>Always fit the scaler on the <strong>training set</strong>, then apply the same transformation to the test set. This prevents data leakage and preserves the integrity of the test data.</em></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># We&#39;ll choose the mean-imputed dataset for demonstration</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_mean</span><span class="p">)</span>  <span class="c1"># fit on training</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_mean</span><span class="p">)</span>       <span class="c1"># transform test with the same scaler</span>

<span class="c1"># Another popular method: Min-Max Scaling (commented out)</span>
<span class="c1"># from sklearn.preprocessing import MinMaxScaler</span>
<span class="c1"># minmax_scaler = MinMaxScaler()</span>
<span class="c1"># X_train_scaled_minmax = minmax_scaler.fit_transform(X_train_mean)</span>
<span class="c1"># X_test_scaled_minmax = minmax_scaler.transform(X_test_mean)</span>

<span class="c1"># Renaming final sets</span>
<span class="n">X_train_final</span> <span class="o">=</span> <span class="n">X_train_scaled</span>
<span class="n">X_test_final</span> <span class="o">=</span> <span class="n">X_test_scaled</span>
<span class="n">y_train_final</span> <span class="o">=</span> <span class="n">y_train_full</span>
<span class="n">y_test_final</span> <span class="o">=</span> <span class="n">y_test_full</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data is now scaled (fit on training set) using mean-imputed values.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data is now scaled (fit on training set) using mean-imputed values.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2-vanilla-knn-implementation">
<h2><span class="section-number">4.3. </span>Exercise 2: Vanilla KNN Implementation<a class="headerlink" href="#exercise-2-vanilla-knn-implementation" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Split your data into training and test sets (we should have already done that in the previous exercise).</p></li>
<li><p>Implement a vanilla KNN regressor from scratch (no scikit-learn for this part).</p></li>
<li><p>Choose a value of <span class="math notranslate nohighlight">\(k\)</span> (e.g., 3, 5, 7).</p></li>
<li><p>Compare predictions with the true values using Mean Squared Error (MSE).</p></li>
</ol>
<section id="implementation">
<h3><span class="section-number">4.3.1. </span>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.auto</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">def</span><span class="w"> </span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">knn_regressor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple KNN regressor (manual implementation).</span>
<span class="sd">    - X_train: training features (numpy array)</span>
<span class="sd">    - y_train: training targets (numpy array)</span>
<span class="sd">    - X_test: test features (numpy array)</span>
<span class="sd">    - k: number of neighbors to consider</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">test_point</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">X_test</span><span class="p">):</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">]</span>
        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="n">k</span><span class="p">]</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">k_indices</span><span class="p">])</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]:</span>
  <span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_regressor</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">X_test_final</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
  <span class="n">mse_knn</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KNN Regressor (k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">) MSE: </span><span class="si">{</span><span class="n">mse_knn</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "1fe0f79f3a03414c9702417a38c0655b"}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNN Regressor (k=3) MSE: 0.4954
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "0b6537a3dd9247f693b2e62f94737d6f"}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNN Regressor (k=5) MSE: 0.4588
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "0b65596c3797484b9dbc7950fada8c00"}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNN Regressor (k=7) MSE: 0.4495
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3-regression-trees-and-random-forests">
<h2><span class="section-number">4.4. </span>Exercise 3: Regression Trees and Random Forests<a class="headerlink" href="#exercise-3-regression-trees-and-random-forests" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Train a regression tree on the training set.</p></li>
<li><p>Train a random forest on the training set.</p></li>
<li><p>Evaluate both models using MSE.</p></li>
<li><p>Compare their performance with your KNN model.</p></li>
<li><p>Determine how modifying the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> (for the regression trees) and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> (for the random forests) impacts the performances of the model. Plot these results graphically.</p></li>
<li><p>Plot the features importance of the best model with the optimal parameters</p></li>
</ol>
<section id="implementation-with-scikit-learn">
<h3><span class="section-number">4.4.1. </span>Implementation with Scikit-learn<a class="headerlink" href="#implementation-with-scikit-learn" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="c1"># 4.2.1 Regression Tree</span>
<span class="n">tree_reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
<span class="n">mse_tree</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regression Tree MSE (default parameters): </span><span class="si">{</span><span class="n">mse_tree</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 4.2.2 Random Forest</span>
<span class="n">forest_reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">forest_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
<span class="n">y_pred_forest</span> <span class="o">=</span> <span class="n">forest_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
<span class="n">mse_forest</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_forest</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random Forest MSE (default parameters): </span><span class="si">{</span><span class="n">mse_forest</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Comparison of MSE:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KNN (k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">mse_knn</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decision Tree: </span><span class="si">{</span><span class="n">mse_tree</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random Forest: </span><span class="si">{</span><span class="n">mse_forest</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Regression Tree MSE (default parameters): 0.5443
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random Forest MSE (default parameters): 0.2672

Comparison of MSE:
KNN (k=7): 0.4495
Decision Tree: 0.5443
Random Forest: 0.2672
</pre></div>
</div>
</div>
</div>
</section>
<section id="exploring-key-parameters">
<h3><span class="section-number">4.4.2. </span>Exploring Key Parameters<a class="headerlink" href="#exploring-key-parameters" title="Link to this heading">#</a></h3>
<section id="decision-tree-max-depth">
<h4><span class="section-number">4.4.2.1. </span>Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code><a class="headerlink" href="#decision-tree-max-depth" title="Link to this heading">#</a></h4>
<p>We can examine how <strong>max_depth</strong> (the maximum number of splits from the root to the leaf) affects performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn-v0_8-muted&#39;</span><span class="p">,</span> <span class="s1">&#39;practicals.mplstyle&#39;</span><span class="p">])</span>

<span class="n">depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">mse_values_tree</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
    <span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
    <span class="n">mse_values_tree</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">],</span> <span class="n">mse_values_tree</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE vs max_depth (Decision Tree)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;max_depth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/278d17d865a53769e2e545f2b45fb9b65c06eb669ba00812e298b03d5b606d61.png" src="_images/278d17d865a53769e2e545f2b45fb9b65c06eb669ba00812e298b03d5b606d61.png" />
</div>
</div>
</section>
<section id="random-forest-n-estimators">
<h4><span class="section-number">4.4.2.2. </span>Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code><a class="headerlink" href="#random-forest-n-estimators" title="Link to this heading">#</a></h4>
<p>Similarly, we can see how <strong>n_estimators</strong> (the number of trees in the forest) impacts the MSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimators_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>
<span class="n">mse_values_rf</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">estimators_range</span><span class="p">:</span>
    <span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>
    <span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
    <span class="n">mse_values_rf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">estimators_range</span><span class="p">,</span> <span class="n">mse_values_rf</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;MSE vs n_estimators (Random Forest)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n_estimators&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cfc76430b857e29c0933574e682284084f3f2611c2c20d48a489cb71f93d469f.png" src="_images/cfc76430b857e29c0933574e682284084f3f2611c2c20d48a489cb71f93d469f.png" />
</div>
</div>
</section>
</section>
<section id="feature-importance-with-the-best-models">
<h3><span class="section-number">4.4.3. </span>Feature importance with the best models<a class="headerlink" href="#feature-importance-with-the-best-models" title="Link to this heading">#</a></h3>
<p>Based on the experiments above, let’s define <strong>user-chosen</strong> parameters for the <em>best</em> (or at least improved) <strong>Decision Tree</strong> and <strong>Random Forest</strong>, then <strong>compute and display</strong> their feature importances.</p>
<p>Feel free to update <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> according to the results you obtained above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># User-chosen parameters (edit based on the results of section 4.3)</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="mi">5</span>     <span class="c1"># for the Decision Tree</span>
<span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># for the Random Forest</span>

<span class="c1"># Instantiate and fit the best Decision Tree</span>
<span class="n">best_tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">best_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>

<span class="c1"># Instantiate and fit the best Random Forest</span>
<span class="n">best_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">best_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">)</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X_original</span><span class="o">.</span><span class="n">columns</span>  <span class="c1"># Using original column names</span>

<span class="n">tree_importances</span> <span class="o">=</span> <span class="n">best_tree</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">forest_importances</span> <span class="o">=</span> <span class="n">best_forest</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="c1"># Sort features by importance for each model</span>
<span class="n">tree_sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">forest_sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Tree Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)),</span> <span class="n">tree_importances</span><span class="p">[</span><span class="n">tree_sorted_idx</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tree_importances</span><span class="p">)),</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">tree_sorted_idx</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)),</span> <span class="n">forest_importances</span><span class="p">[</span><span class="n">forest_sorted_idx</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">forest_importances</span><span class="p">)),</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">forest_sorted_idx</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9cfed6f80dcd0906cceb6543f15cb1e50c6c9314c038588adf479961d02adcb6.png" src="_images/9cfed6f80dcd0906cceb6543f15cb1e50c6c9314c038588adf479961d02adcb6.png" />
</div>
</div>
</section>
</section>
<section id="exercise-4-building-a-pipeline">
<h2><span class="section-number">4.5. </span>Exercise 4: Building a Pipeline<a class="headerlink" href="#exercise-4-building-a-pipeline" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Define</strong> a pipeline that applies the preprocessing steps (e.g., imputation, scaling) and the best-performing model (from Exercise 3).</p></li>
<li><p><strong>Fit</strong> this pipeline on the training data and evaluate on the test data.</p></li>
<li><p><strong>Observe</strong> how scikit-learn handles transformations during <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p></li>
</ol>
<section id="pipeline-implementation">
<h3><span class="section-number">4.5.1. </span>Pipeline Implementation<a class="headerlink" href="#pipeline-implementation" title="Link to this heading">#</a></h3>
<p>Below, we define a pipeline that:</p>
<ol class="arabic simple">
<li><p>Imputes missing values with mean.</p></li>
<li><p>Scales the features.</p></li>
<li><p>Trains a random forest regressor.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">pipeline_rf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Fit the pipeline on the entire training set with missing values (X_train_full)</span>
<span class="n">pipeline_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">)</span>
<span class="n">y_pred_pipe</span> <span class="o">=</span> <span class="n">pipeline_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_full</span><span class="p">)</span>
<span class="n">mse_pipeline_rf</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test_full</span><span class="p">,</span> <span class="n">y_pred_pipe</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pipeline (Random Forest) MSE: </span><span class="si">{</span><span class="n">mse_pipeline_rf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline (Random Forest) MSE: 0.2672
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">
<h2><span class="section-number">4.6. </span>Additional exercise: bias–variance trade-off in regression with decision trees<a class="headerlink" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees" title="Link to this heading">#</a></h2>
<p>In this exercise, you will investigate the bias–variance trade-off for decision tree regressors of varying complexity. Specifically, you will perform simulations to estimate how the depth of a decision tree influences its performance.</p>
<p><strong>Your tasks are as follows:</strong></p>
<ol class="arabic">
<li><p>Select a nonlinear, moderately complex target function e.g.:
<span class="math notranslate nohighlight">\(f(x) = \sin(4\pi x) + w\)</span> or <span class="math notranslate nohighlight">\(f(x) = \sin(2\pi x_1 x_2 x_3) + w\)</span></p></li>
<li><p>Evaluate three decision tree regressors, each with a different complexity level:</p>
<ul class="simple">
<li><p><strong>Shallow tree</strong>: depth = 2</p></li>
<li><p><strong>Medium-depth tree</strong>: depth = 6</p></li>
<li><p><strong>Deep tree</strong>: depth = 20</p></li>
</ul>
</li>
<li><p>Generate training datasets composed of <span class="math notranslate nohighlight">\(n_{\text{train}} = 400\)</span> random samples:</p>
<div class="math notranslate nohighlight">
\[
   X_{\text{train}} \sim U(0,1), \quad y_{\text{train}} = f(X_{\text{train}}) + w
   \]</div>
<p>Here, noise <span class="math notranslate nohighlight">\(w\)</span> follows a Gaussian distribution with mean 0 and standard deviation <span class="math notranslate nohighlight">\(0.3\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   w \sim \mathcal{N}(0, 0.3^2)
   \]</div>
</li>
<li><p>Repeat the entire training and evaluation process <span class="math notranslate nohighlight">\(n_{\text{runs}} = 100\)</span> times. For each run, compute and store the mean squared error (MSE) using 10-fold cross-validation on the training set.</p></li>
<li><p>Evaluate your models on a test set <span class="math notranslate nohighlight">\(X_{\text{test}}\)</span> evenly spaced in the interval <span class="math notranslate nohighlight">\([0,1]\)</span> (e.g., 400 points).</p>
<ul class="simple">
<li><p>For each test point, compute predictions across all 100 simulation runs.</p></li>
<li><p>Using these predictions, compute</p>
<ul>
<li><p><strong>MSE (CV)</strong>: The average cross-validation mean squared error (which we will call <span class="math notranslate nohighlight">\(\widehat{\text{MISE}}_{\text{kfold}}\)</span>) obtained across the 100 runs.</p></li>
<li><p><strong>Bias(^2)</strong>: Based on how far the <em>average prediction</em> per sample is from the sample’s true label.</p></li>
<li><p><strong>Variance</strong>: Based on the variability of predictions per sample around that sample’s mean prediction.</p></li>
<li><p><strong>MSE (true)</strong>: the approximation <span class="math notranslate nohighlight">\(
\text{MSE} \approx \text{Bias}^2 + \text{Variance}.\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Discussion</strong><br />
Analyze your results to illustrate how model complexity (tree depth) affects the balance between bias and variance:</p>
<ul class="simple">
<li><p>Identify which depth leads to underfitting (high bias, low variance).</p></li>
<li><p>Identify which depth leads to overfitting (low bias, high variance).</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Simulation parameters</span>
<span class="n">n_train</span> <span class="o">=</span> <span class="mi">400</span>        <span class="c1"># number of training samples per run</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1"># number of simulation runs for averaging</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.3</span>     <span class="c1"># standard deviation of noise</span>
<span class="n">depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span> <span class="c1"># tree depths to evaluate</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>               <span class="c1"># number of folds for K-fold cross-validation</span>

<span class="c1"># Define the target function f(x) = sin(4πx)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Prepare a grid of test points to evaluate predictions (for bias/variance calculations)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">400</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">f_func</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Storage for predictions and CV results</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span><span class="n">depth</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_runs</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">}</span>
<span class="n">cv_mse_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">depth</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">}</span>

<span class="c1"># Simulation over multiple runs</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>

    <span class="c1"># Generate random training data for this run</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">f_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_train</span><span class="p">)</span>

    <span class="c1"># Train and evaluate each tree depth</span>
    <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>

        <span class="c1"># Train decision tree regressor</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Store predictions</span>
        <span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

        <span class="c1"># Compute K-fold CV MSE on the training set (negative MSE is returned by cross_val_score)</span>
        <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
            <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">),</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">cv_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">cv_mse_values</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_mse</span><span class="p">)</span>


<span class="c1"># Known noise variance</span>
<span class="n">noise_var</span> <span class="o">=</span> <span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Initialize a dictionary to store the results</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
    <span class="c1"># Mean prediction over all runs at each test point</span>
    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Bias^2 (averaged over test points)</span>
    <span class="n">bias_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mean_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Variance (averaged over test points)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">depth</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="c1"># True MSE = Bias^2 + Variance + noise variance</span>
    <span class="n">true_mse</span> <span class="o">=</span> <span class="n">bias_sq</span> <span class="o">+</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">noise_var</span>

    <span class="c1"># Average CV MSE across runs</span>
    <span class="n">cv_mse_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_mse_values</span><span class="p">[</span><span class="n">depth</span><span class="p">])</span>

    <span class="n">metrics</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Bias^2&quot;</span><span class="p">:</span> <span class="n">bias_sq</span><span class="p">,</span>
        <span class="s2">&quot;Variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s2">&quot;True MSE&quot;</span><span class="p">:</span> <span class="n">true_mse</span><span class="p">,</span>
        <span class="s2">&quot;CV MSE&quot;</span><span class="p">:</span> <span class="n">cv_mse_mean</span>
    <span class="p">}</span>

<span class="c1"># Display the metrics in a table for each depth</span>
<span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># transpose so that depth is the index</span>
<span class="n">metrics_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Tree Depth&quot;</span>
<span class="c1"># Round the values for cleaner display</span>
<span class="n">metrics_df</span> <span class="o">=</span> <span class="n">metrics_df</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            Bias^2  Variance  True MSE  CV MSE
Tree Depth                                    
2           0.1875    0.0996    0.3770  0.3779
6           0.0011    0.0316    0.1227  0.1243
20          0.0009    0.0886    0.1795  0.1784
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Linear Models</p>
      </div>
    </a>
    <a class="right-next"
       href="04.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Neural Networks for Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-overview">4.1.1. Data preprocessing overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">4.1.2. K-Nearest Neighbors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">4.1.3. Regression trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">4.1.4. Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-this-reduce-variance">4.1.4.1. Why does this reduce variance?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">4.1.4.2. Main Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importances">4.1.5. Feature importances</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">4.1.6. Scikit-learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-precision-about-the-evaluation-metric-used-in-this-practical">4.1.7. Important precision about the evaluation metric used in this practical:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-data-loading-exploration-and-preprocessing">4.2. Exercise 1: Data Loading, Exploration, and Preprocessing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-exploration">4.2.1. Dataset and exploration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values">4.2.2. Handling Missing Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#knnimputer-explanation">4.2.2.1. <code class="docutils literal notranslate"><span class="pre">KNNImputer</span></code> Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-categorical-values">4.2.3. Dealing with Categorical Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-scaling">4.2.4. Normalization/Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-vanilla-knn-implementation">4.3. Exercise 2: Vanilla KNN Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">4.3.1. Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-regression-trees-and-random-forests">4.4. Exercise 3: Regression Trees and Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-with-scikit-learn">4.4.1. Implementation with Scikit-learn</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-key-parameters">4.4.2. Exploring Key Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-max-depth">4.4.2.1. Decision Tree: <code class="docutils literal notranslate"><span class="pre">max_depth</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-n-estimators">4.4.2.2. Random Forest: <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-the-best-models">4.4.3. Feature importance with the best models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-building-a-pipeline">4.5. Exercise 4: Building a Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-implementation">4.5.1. Pipeline Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-exercise-biasvariance-trade-off-in-regression-with-decision-trees">4.6. Additional exercise: bias–variance trade-off in regression with decision trees</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tribel Pascal, Simar Cédric, Bontempi Gianluca
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  This is the practicals handbook for the course INFO-F422 - Statistical Foundations of Machine Learning. This is intended to be used alongside the <a href='https://www.researchgate.net/publication/242692234_Statistical_foundations_of_machine_learning_the_handbook'> theoretical handbook</a>.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>